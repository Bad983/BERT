{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FoxtgJ8dP-0C","executionInfo":{"status":"ok","timestamp":1669759045901,"user_tz":-60,"elapsed":81313,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"555a9ce0-afb8-4852-8085-1551dfbde78c"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 7.5 MB/s \n","\u001b[K     |████████████████████████████████| 497.9 MB 4.4 kB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 28.3 MB/s \n","\u001b[K     |████████████████████████████████| 1.4 MB 53.4 MB/s \n","\u001b[K     |████████████████████████████████| 462 kB 57.2 MB/s \n","\u001b[?25h"]}],"source":["!pip install -q -U \"tensorflow-text==2.8.*\""]},{"cell_type":"code","source":["!pip install -q tensorflow_datasets"],"metadata":{"id":"tPyeQ8XVQB8M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PVt5I5ujRI9G","executionInfo":{"status":"ok","timestamp":1669759078195,"user_tz":-60,"elapsed":27016,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"c3efdb4d-254d-4e98-e9a6-f9f859741205"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import collections\n","import os\n","import pathlib\n","import re\n","import string\n","import sys\n","import tempfile\n","import time\n","import pandas as pd\n","import unicodedata\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow_datasets as tfds\n","import tensorflow_text as text\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"],"metadata":{"id":"6zJdhoDiQCdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PARAMETRI GLOBALI\n","root_folder = 'drive/MyDrive/BERT/'\n","\n","# DATI\n","data_folder_name = 'data'\n","train_filename = 'ita.txt'\n","\n","DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n","train_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n","\n","# VOCABOLARIO\n","vocab_folder = 'vocab'\n","en_vocab_finalname = 'en_vocab.txt'\n","it_vocab_finalname = 'it_vocab.txt'\n","\n","VOCAB_PATH = os.path.abspath(os.path.join(root_folder, vocab_folder))\n","en_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, en_vocab_finalname))\n","it_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, it_vocab_finalname))\n","\n","# MODELLO TOKENIZER\n","model_name = 'tokenizer_en_it_model'\n","tokenizer_folder_name = 'tokenizer'\n","\n","TOKEN_PATH = os.path.abspath(os.path.join(root_folder, tokenizer_folder_name))\n","tokenizer_filenamepath = os.path.abspath(os.path.join(TOKEN_PATH, model_name))"],"metadata":{"id":"NKHJuB_cQMme"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# parametri per il modello\n","INPUT_COLUMN = 'input'\n","TARGET_COLUMN = 'target'\n","# TARGET_FOR_INPUT = 'target_for_input'\n","NUM_SAMPLES = 1000000\n","MAX_VOCAB_SIZE = 20000\n","BATCH_SIZE = 32\n","MAX_SEQ_LENGTH = 16"],"metadata":{"id":"_hfF2lPLQthq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n","\n","def preprocess_sentence(w):\n","    '''\n","    Preprocessing dei testi di input, impostando tutti i caratteri\n","    minuscoli, aggiungendo uno spazio prima di ogni punto e sostituendo\n","    qualsiasi carattere con uno spazio se non è compreso nel seguente elenco:\n","    (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","    '''\n","    w = unicode_to_ascii(w.lower().strip())\n","\n","    # inserimento di uno spazio tra ogni parola e il successivo punto,\n","    # punto esclamativo, punto interrogativo e virgola\n","    # esempio: \"ciao, come và?\" => \"ciao , come và ?\"\n","    w = re.sub(r\"([?.!,])\", r\" \\1 \", w) # inserimento di uno spazio\n","\n","    # sostituzione dei caratteri non desiderati con uno spazio\n","    w = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", w)\n","\n","    w = re.sub(r'[\" \"]+', \" \", w) # rimozione di più spazi consecutivi\n","    return w"],"metadata":{"id":"Qx4q0p44QRjO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Caricamento dataset: frasi in inglese, frasi in italiano\n","df = pd.read_csv(\n","    train_filenamepath,\n","    sep=\"\\t\",\n","    header=None,\n","    names=[INPUT_COLUMN, TARGET_COLUMN],\n","    usecols=[0,1],\n","    nrows=NUM_SAMPLES\n",")\n","\n","print(df.iloc[42:52], '\\n')\n","\n","# Preprocessing dei dati di Input\n","input_data = df[INPUT_COLUMN].tolist()\n","\n","# Preprocessing dei dati Target con aggiunta del token di fine frase\n","target_data = df[TARGET_COLUMN].tolist()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qYowimQQdXQ","executionInfo":{"status":"ok","timestamp":1669759085135,"user_tz":-60,"elapsed":2997,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"89086087-02b7-450f-a0e9-67df67917fad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["     input          target\n","42  Do it.      Lo faccia.\n","43  Do it.      La faccia.\n","44  Do it.         Fatelo.\n","45  Do it.         Fatela.\n","46  Go on.     Vai avanti.\n","47  Go on.       Continua.\n","48  Go on.       Continui.\n","49  Go on.     Continuate.\n","50  Go on.    Vada avanti.\n","51  Go on.  Andate avanti. \n","\n"]}]},{"cell_type":"code","source":["# Definizione del dataset\n","# [from_tensor_slices] permette di recuperare batch\n","# di esempi dai dataset di riferimento\n","dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n","\n","# impostazione del recupero di esempi presi in maniera\n","# casuale in gruppi di [BATCH_SIZE] tra quelli disponibili\n","dataset = dataset.shuffle(len(input_data)).batch(BATCH_SIZE, drop_remainder=True)"],"metadata":{"id":"z4-tODgWQi5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","bert_vocab_args = dict(\n","    # The target vocabulary size\n","    vocab_size = MAX_VOCAB_SIZE,\n","    # Reserved tokens that must be included in the vocabulary\n","    reserved_tokens=reserved_tokens,\n","    # Arguments for `text.BertTokenizer`\n","    bert_tokenizer_params=bert_tokenizer_params,\n","    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","    learn_params={},\n",")"],"metadata":{"id":"aL8ipmOARxtP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_en = dataset.map(lambda en, it: en)\n","train_it = dataset.map(lambda en, it: it)"],"metadata":{"id":"k7Ki0_hnVs_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","en_vocab = bert_vocab.bert_vocab_from_dataset(\n","    train_en.batch(10000).prefetch(2),\n","    **bert_vocab_args\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5jU9gt1IR009","executionInfo":{"status":"ok","timestamp":1669759154888,"user_tz":-60,"elapsed":65129,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"1162cf10-9406-4fe2-8d11-32cc88551082"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 57.1 s, sys: 494 ms, total: 57.6 s\n","Wall time: 1min 5s\n"]}]},{"cell_type":"code","source":["%%time\n","it_vocab = bert_vocab.bert_vocab_from_dataset(\n","    train_it.batch(10000).prefetch(2),\n","    **bert_vocab_args\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Q2Au6xkXByS","executionInfo":{"status":"ok","timestamp":1669759271712,"user_tz":-60,"elapsed":116828,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"6c38a7dc-1b47-4ee8-e1f2-962607f67fdf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 1min 55s, sys: 570 ms, total: 1min 55s\n","Wall time: 1min 56s\n"]}]},{"cell_type":"code","source":["print('VOCABOLARIO INGLESE')\n","print(en_vocab[:10])\n","print(en_vocab[100:110])\n","print(en_vocab[150:160])\n","print(en_vocab[-10:])\n","print('----------------------------------------------')\n","print('VOCABOLARIO ITALIANO')\n","print(it_vocab[:10])\n","print(it_vocab[100:110])\n","print(it_vocab[150:160])\n","print(it_vocab[-10:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q4kc0sliSK5e","executionInfo":{"status":"ok","timestamp":1669759271713,"user_tz":-60,"elapsed":18,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"359e7c86-4a6e-4ebd-b974-ca60f66a0b80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["VOCABOLARIO INGLESE\n","['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', '%', \"'\", ',']\n","['and', 'how', 'will', 'there', 'has', 'about', 'now', 'isn', 'all', 'going']\n","['out', 'by', 'when', 'said', 'lot', 'work', 'let', 'told', 'something', 'car']\n","['##-', '##.', '##/', '##:', '##;', '##?', '##j', '##°', '##’', '##€']\n","----------------------------------------------\n","VOCABOLARIO ITALIANO\n","['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', '%', \"'\", ',']\n","['ancora', 'sia', 'cosi', 'del', 'penso', 'casa', 'hai', 'questa', 'detto', 'siete']\n","['sempre', 'oggi', 'dove', 'puo', 'parlare', 'tempo', 'adesso', 'ne', 'bene', 'delle']\n","['##/', '##:', '##;', '##?', '##b', '##j', '##q', '##°', '##’', '##€']\n"]}]},{"cell_type":"code","source":["def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)"],"metadata":{"id":"CdPtwIfnWIwC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["write_vocab_file(en_vocab_filenamepath, en_vocab)\n","write_vocab_file(it_vocab_filenamepath, it_vocab)"],"metadata":{"id":"j0erEcs0WM8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["en_tokenizer = text.BertTokenizer(en_vocab_filenamepath, **bert_tokenizer_params)\n","it_tokenizer = text.BertTokenizer(it_vocab_filenamepath, **bert_tokenizer_params)"],"metadata":{"id":"Qgfu54bXWQgp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for en_examples, it_examples in dataset.batch(1).take(1):\n","  for ex in en_examples:\n","    print(ex[:5].numpy())\n","  for ex in it_examples:\n","    print(ex[:5].numpy())  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4UK3yyh-WXrI","executionInfo":{"status":"ok","timestamp":1669759273124,"user_tz":-60,"elapsed":431,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"b1359408-33cd-4bee-8510-39a23bc41ba1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[b'She will return within an hour.' b'I am decorating the classroom.'\n"," b'Your question is illogical.' b\"Tom isn't going to hurt you.\"\n"," b'You said it would never happen.']\n","[b\"Lei torner\\xc3\\xa0 tra un'ora.\" b\"Io sto decorando l'aula.\"\n"," b'La vostra domanda \\xc3\\xa8 illogica.'\n"," b'Tom non le far\\xc3\\xa0 del male.'\n"," b'Ha detto che non sarebbe mai successo.']\n"]}]},{"cell_type":"code","source":["# Tokenize the examples -> (batch, word, word-piece)\n","en_token_batch = en_tokenizer.tokenize(en_examples)\n","# Merge the word and word-piece axes -> (batch, tokens)\n","en_token_batch = en_token_batch.merge_dims(-2,-1)\n","\n","for ex in en_token_batch.to_list():\n","  print(ex[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"piSBBbJxWssa","executionInfo":{"status":"ok","timestamp":1669759273124,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"c0ef30f3-d4d1-46a1-b992-1951adab64c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[88, 102, 862, 1615, 119, 654, 11], [34, 174, 5632, 58, 1571, 11], [79, 370, 59, 1092, 5010, 11], [56, 107, 8, 45, 109, 57, 371, 55, 11], [55, 153, 61, 132, 129, 354, 11]]\n"]}]},{"cell_type":"code","source":["# Tokenize the examples -> (batch, word, word-piece)\n","it_token_batch = it_tokenizer.tokenize(it_examples)\n","# Merge the word and word-piece axes -> (batch, tokens)\n","it_token_batch = it_token_batch.merge_dims(-2,-1)\n","\n","for ex in it_token_batch.to_list():\n","  print(ex[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jXDSVWcIW24a","executionInfo":{"status":"ok","timestamp":1669759273125,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"751a60c0-65fb-468b-f43a-6844a7d3897e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[71, 1535, 493, 62, 8, 121, 11], [60, 141, 7834, 1166, 3968, 37, 8, 2804, 11], [59, 221, 349, 30, 61, 520, 5654, 11], [55, 56, 74, 459, 103, 306, 11], [63, 108, 58, 56, 326, 94, 263, 11]]\n"]}]},{"cell_type":"code","source":["en_words = en_tokenizer.detokenize(en_token_batch)\n","en_words = tf.strings.reduce_join(en_words, separator=' ', axis=-1)\n","print(en_words[0][:5].numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQoAXBX9XqsS","executionInfo":{"status":"ok","timestamp":1669759273600,"user_tz":-60,"elapsed":482,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"cfaba7f9-cb64-4934-cf1d-de22aa21a558"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[b'she will return within an hour .' b'i am decorating the classroom .'\n"," b'your question is illogical .' b\"tom isn ' t going to hurt you .\"\n"," b'you said it would never happen .']\n"]}]},{"cell_type":"code","source":["it_words = it_tokenizer.detokenize(it_token_batch)\n","it_words = tf.strings.reduce_join(it_words, separator=' ', axis=-1)\n","print(it_words[0][:5].numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxCN8ml3VFix","executionInfo":{"status":"ok","timestamp":1669759273601,"user_tz":-60,"elapsed":40,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"370d0d90-4235-4fa1-bd82-57d36d306bb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[b\"lei tornera tra un ' ora .\" b\"io sto decorando l ' aula .\"\n"," b'la vostra domanda e illogica .' b'tom non le fara del male .'\n"," b'ha detto che non sarebbe mai successo .']\n"]}]},{"cell_type":"code","source":["START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape()[0]\n","  starts = tf.fill([count,1], START)\n","  ends = tf.fill([count,1], END)\n","  x = tf.concat([starts, ragged, ends], axis=1)\n","  # x = keras.preprocessing.sequence.pad_sequences(x.numpy(), maxlen=MAX_SEQ_LENGTH, padding='post')\n","  return x"],"metadata":{"id":"8zkX-NKnVwCp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["en_words = add_start_end(en_token_batch[0][:5])\n","print(en_words[1])\n","\n","en_words = en_tokenizer.detokenize(en_words)\n","en_words = tf.strings.reduce_join(en_words, separator=' ', axis=-1)\n","\n","print(en_words[1].numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-vR3pd8ZWovg","executionInfo":{"status":"ok","timestamp":1669759273604,"user_tz":-60,"elapsed":37,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"3fb115a4-f147-4652-d45d-64355ece9a23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([   2   34  174 5632   58 1571   11    3], shape=(8,), dtype=int64)\n","b'[START] i am decorating the classroom . [END]'\n"]}]},{"cell_type":"code","source":["it_words = add_start_end(it_token_batch[0][:5])\n","print(it_words[1])\n","\n","it_words = it_tokenizer.detokenize(it_words)\n","it_words = tf.strings.reduce_join(it_words, separator=' ', axis=-1)\n","\n","print(it_words[1].numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDG1ZRXcYIDm","executionInfo":{"status":"ok","timestamp":1669759273605,"user_tz":-60,"elapsed":30,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"c84066d5-9251-4e38-c583-9eb3eb22c5be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([   2   60  141 7834 1166 3968   37    8 2804   11    3], shape=(11,), dtype=int64)\n","b\"[START] io sto decorando l ' aula . [END]\"\n"]}]},{"cell_type":"code","source":["def cleanup_text(reserved_tokens, token_txt):\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result"],"metadata":{"id":"HBjAVbAuaiva"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["words = en_tokenizer.detokenize(en_token_batch)\n","words[0][:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cvVxMgK21Mgc","executionInfo":{"status":"ok","timestamp":1669759273606,"user_tz":-60,"elapsed":26,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"778d369f-6f19-4c08-a8ef-b03a98ca1e3e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.RaggedTensor [[b'she', b'will', b'return', b'within', b'an', b'hour', b'.'],\n"," [b'i', b'am', b'decorating', b'the', b'classroom', b'.'],\n"," [b'your', b'question', b'is', b'illogical', b'.'],\n"," [b'tom', b'isn', b\"'\", b't', b'going', b'to', b'hurt', b'you', b'.'],\n"," [b'you', b'said', b'it', b'would', b'never', b'happen', b'.']]>"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["words = cleanup_text(reserved_tokens, words).numpy()\n","words[0][:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_GAkIDZy1PXx","executionInfo":{"status":"ok","timestamp":1669759273606,"user_tz":-60,"elapsed":22,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"9b8d0c7f-85f3-48fb-bf05-12f51d96a5e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([b'she will return within an hour .',\n","       b'i am decorating the classroom .',\n","       b'your question is illogical .',\n","       b\"tom isn ' t going to hurt you .\",\n","       b'you said it would never happen .'], dtype=object)"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["en_input = add_start_end(en_token_batch[0][:5])\n","en_input = en_input.to_tensor()\n","print(f'Shape en_input  : {en_input.shape}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5LuELIWyeeO","executionInfo":{"status":"ok","timestamp":1669759273607,"user_tz":-60,"elapsed":19,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"d479b8a3-d7d2-4fd8-d231-c36d0c43af30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape en_input  : (5, 11)\n"]}]},{"cell_type":"markdown","source":["### Classe Tokenizer Custom"],"metadata":{"id":"JCy5XCYL1yhB"}},{"cell_type":"code","source":["class CustomTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n","    self._reserved_tokens = reserved_tokens\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","    \n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","    \n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens)        "],"metadata":{"id":"LsmkD3Kb1k65"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizers = tf.Module()\n","tokenizers.en = CustomTokenizer(reserved_tokens, en_vocab_filenamepath)\n","tokenizers.it = CustomTokenizer(reserved_tokens, it_vocab_filenamepath)\n","\n","tf.saved_model.save(tokenizers, tokenizer_filenamepath)"],"metadata":{"id":"65DpY7QUj1jz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VeyDzXMQzP_o"},"execution_count":null,"outputs":[]}]}