{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FoxtgJ8dP-0C","executionInfo":{"status":"ok","timestamp":1669759084869,"user_tz":-60,"elapsed":82119,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"1f3af022-ee1d-4e39-f345-4615ce248f31"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 6.8 MB/s \n","\u001b[K     |████████████████████████████████| 497.9 MB 4.2 kB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 41.6 MB/s \n","\u001b[K     |████████████████████████████████| 462 kB 70.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.4 MB 45.9 MB/s \n","\u001b[?25h"]}],"source":["!pip install -q -U \"tensorflow-text==2.8.*\""]},{"cell_type":"code","source":["!pip install -q tensorflow_datasets"],"metadata":{"id":"tPyeQ8XVQB8M","executionInfo":{"status":"ok","timestamp":1669759088834,"user_tz":-60,"elapsed":3982,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PVt5I5ujRI9G","executionInfo":{"status":"ok","timestamp":1669759112266,"user_tz":-60,"elapsed":23444,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"249d258d-e013-41be-eb2e-0aa57d83073b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import collections\n","import os\n","import pathlib\n","import re\n","import string\n","import sys\n","import tempfile\n","import time\n","import pandas as pd\n","import unicodedata\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import tensorflow_datasets as tfds\n","import tensorflow_text as text\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"],"metadata":{"id":"6zJdhoDiQCdM","executionInfo":{"status":"ok","timestamp":1669759116576,"user_tz":-60,"elapsed":4315,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# PARAMETRI GLOBALI\n","root_folder = 'drive/MyDrive/BERT/'\n","\n","# DATI\n","data_folder_name = 'data'\n","train_filename = 'ita.txt'\n","\n","DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n","train_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n","\n","# VOCABOLARIO\n","vocab_folder = 'vocab'\n","it_vocab_finalname = 'it_vocab_transformer.txt'\n","\n","VOCAB_PATH = os.path.abspath(os.path.join(root_folder, vocab_folder))\n","it_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, it_vocab_finalname))\n","\n","# MODELLO TOKENIZER\n","model_name = 'tokenizer_it_transformer_model'\n","tokenizer_folder_name = 'tokenizer'\n","\n","TOKEN_PATH = os.path.abspath(os.path.join(root_folder, tokenizer_folder_name))\n","tokenizer_filenamepath = os.path.abspath(os.path.join(TOKEN_PATH, model_name))"],"metadata":{"id":"NKHJuB_cQMme","executionInfo":{"status":"ok","timestamp":1669759116577,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# parametri per il modello\n","INPUT_COLUMN = 'input'\n","TARGET_COLUMN = 'target'\n","NUM_SAMPLES = 1000000\n","MAX_VOCAB_SIZE = 20000\n","BATCH_SIZE = 32\n","MAX_SEQ_LENGTH = 16"],"metadata":{"id":"_hfF2lPLQthq","executionInfo":{"status":"ok","timestamp":1669759116578,"user_tz":-60,"elapsed":12,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Caricamento dataset: frasi in inglese, frasi in italiano\n","df = pd.read_csv(\n","    train_filenamepath,\n","    sep=\"\\t\",\n","    header=None,\n","    names=[INPUT_COLUMN, TARGET_COLUMN],\n","    usecols=[0,1],\n","    nrows=NUM_SAMPLES\n",")\n","\n","print(df.iloc[42:52], '\\n')\n","\n","# Preprocessing dei dati di Input\n","input_data = df[INPUT_COLUMN].tolist()\n","\n","# Preprocessing dei dati Target con aggiunta del token di fine frase\n","target_data = df[TARGET_COLUMN].tolist()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qYowimQQdXQ","executionInfo":{"status":"ok","timestamp":1669759118559,"user_tz":-60,"elapsed":1992,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"36cc0b5f-1b98-467a-f4b3-18bdbe9b1280"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["     input          target\n","42  Do it.      Lo faccia.\n","43  Do it.      La faccia.\n","44  Do it.         Fatelo.\n","45  Do it.         Fatela.\n","46  Go on.     Vai avanti.\n","47  Go on.       Continua.\n","48  Go on.       Continui.\n","49  Go on.     Continuate.\n","50  Go on.    Vada avanti.\n","51  Go on.  Andate avanti. \n","\n"]}]},{"cell_type":"code","source":["# Definizione del dataset\n","# [from_tensor_slices] permette di recuperare batch\n","# di esempi dai dataset di riferimento\n","dataset = tf.data.Dataset.from_tensor_slices(target_data)\n","\n","# impostazione del recupero di esempi presi in maniera\n","# casuale in gruppi di [BATCH_SIZE] tra quelli disponibili\n","dataset = dataset.shuffle(len(target_data)).batch(BATCH_SIZE, drop_remainder=True)"],"metadata":{"id":"z4-tODgWQi5P","executionInfo":{"status":"ok","timestamp":1669759120900,"user_tz":-60,"elapsed":2348,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","bert_vocab_args = dict(\n","    # The target vocabulary size\n","    vocab_size = MAX_VOCAB_SIZE,\n","    # Reserved tokens that must be included in the vocabulary\n","    reserved_tokens=reserved_tokens,\n","    # Arguments for `text.BertTokenizer`\n","    bert_tokenizer_params=bert_tokenizer_params,\n","    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","    learn_params={},\n",")"],"metadata":{"id":"aL8ipmOARxtP","executionInfo":{"status":"ok","timestamp":1669759120901,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# train_en = dataset.map(lambda en, it: en)\n","# train_it = dataset.map(lambda en, it: it)"],"metadata":{"id":"k7Ki0_hnVs_W","executionInfo":{"status":"ok","timestamp":1669759121525,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["%%time\n","it_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset.batch(10000).prefetch(2),\n","    **bert_vocab_args\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_Q2Au6xkXByS","executionInfo":{"status":"ok","timestamp":1669759248519,"user_tz":-60,"elapsed":127002,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"458b6ff0-6166-487c-8697-1cec037ed06c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 1min 59s, sys: 518 ms, total: 2min\n","Wall time: 2min 7s\n"]}]},{"cell_type":"code","source":["print('VOCABOLARIO ITALIANO')\n","print(it_vocab[:10])\n","print(it_vocab[100:110])\n","print(it_vocab[150:160])\n","print(it_vocab[-10:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q4kc0sliSK5e","executionInfo":{"status":"ok","timestamp":1669759248520,"user_tz":-60,"elapsed":19,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"1a6fc74b-4fcb-4a56-a775-5aa1c12fe9b4"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["VOCABOLARIO ITALIANO\n","['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', '%', \"'\", ',']\n","['ancora', 'sia', 'cosi', 'del', 'penso', 'casa', 'hai', 'questa', 'detto', 'siete']\n","['sempre', 'oggi', 'dove', 'puo', 'parlare', 'tempo', 'adesso', 'ne', 'bene', 'delle']\n","['##/', '##:', '##;', '##?', '##b', '##j', '##q', '##°', '##’', '##€']\n"]}]},{"cell_type":"code","source":["def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)"],"metadata":{"id":"CdPtwIfnWIwC","executionInfo":{"status":"ok","timestamp":1669759248521,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["write_vocab_file(it_vocab_filenamepath, it_vocab)"],"metadata":{"id":"j0erEcs0WM8-","executionInfo":{"status":"ok","timestamp":1669759248940,"user_tz":-60,"elapsed":430,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["it_tokenizer = text.BertTokenizer(it_vocab_filenamepath, **bert_tokenizer_params)"],"metadata":{"id":"Qgfu54bXWQgp","executionInfo":{"status":"ok","timestamp":1669759248941,"user_tz":-60,"elapsed":11,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["for it_examples in dataset.batch(1).take(1):\n","  for ex in it_examples:\n","    print(ex[:5].numpy())  "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4UK3yyh-WXrI","executionInfo":{"status":"ok","timestamp":1669759249330,"user_tz":-60,"elapsed":396,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"180e4187-aa8f-49a7-e804-c10aa7923433"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["[b'Sarei potuto morire.' b'Non hai davvero bisogno di farlo, vero?'\n"," b'Io ho superato ogni singolo esame.' b'Hai subito un discreto trauma.'\n"," b'Io sapevo che Tom era un insegnante di francese a Boston, quindi non credetti a Mary quando mi disse che era un tassista a Chicago.']\n"]}]},{"cell_type":"code","source":["# Tokenize the examples -> (batch, word, word-piece)\n","it_token_batch = it_tokenizer.tokenize(it_examples)\n","# Merge the word and word-piece axes -> (batch, tokens)\n","it_token_batch = it_token_batch.merge_dims(-2,-1)\n","\n","for ex in it_token_batch.to_list():\n","  print(ex[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jXDSVWcIW24a","executionInfo":{"status":"ok","timestamp":1669759249333,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"74fe7ba6-8318-45d9-ae18-8f2a7852c1ae"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[[556, 887, 778, 11], [56, 106, 160, 147, 57, 149, 9, 95, 25], [60, 66, 2046, 197, 6544, 563, 11], [106, 1342, 62, 2407, 7226, 11], [60, 347, 58, 55, 80, 62, 316, 57, 131, 26, 117, 9, 598, 56, 5929, 26, 72, 138, 68, 240, 58, 80, 62, 2932, 26, 1464, 11]]\n"]}]},{"cell_type":"code","source":["it_words = it_tokenizer.detokenize(it_token_batch)\n","it_words = tf.strings.reduce_join(it_words, separator=' ', axis=-1)\n","print(it_words[0][:5].numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FxCN8ml3VFix","executionInfo":{"status":"ok","timestamp":1669759249333,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"c505e32a-00be-48d0-96d8-73801574cadb"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["[b'sarei potuto morire .' b'non hai davvero bisogno di farlo , vero ?'\n"," b'io ho superato ogni singolo esame .' b'hai subito un discreto trauma .'\n"," b'io sapevo che tom era un insegnante di francese a boston , quindi non credetti a mary quando mi disse che era un tassista a chicago .']\n"]}]},{"cell_type":"code","source":["START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape()[0]\n","  starts = tf.fill([count,1], START)\n","  ends = tf.fill([count,1], END)\n","  x = tf.concat([starts, ragged, ends], axis=1)\n","  # x = keras.preprocessing.sequence.pad_sequences(x.numpy(), maxlen=MAX_SEQ_LENGTH, padding='post')\n","  return x"],"metadata":{"id":"8zkX-NKnVwCp","executionInfo":{"status":"ok","timestamp":1669759249689,"user_tz":-60,"elapsed":363,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["it_words = add_start_end(it_token_batch[0][:5])\n","print(it_words[1])\n","\n","it_words = it_tokenizer.detokenize(it_words)\n","it_words = tf.strings.reduce_join(it_words, separator=' ', axis=-1)\n","\n","print(it_words[1].numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDG1ZRXcYIDm","executionInfo":{"status":"ok","timestamp":1669759249690,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"f425c3d0-0d2b-4162-89ca-c687b3ea37bb"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([  2  56 106 160 147  57 149   9  95  25   3], shape=(11,), dtype=int64)\n","b'[START] non hai davvero bisogno di farlo , vero ? [END]'\n"]}]},{"cell_type":"code","source":["def cleanup_text(reserved_tokens, token_txt):\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result"],"metadata":{"id":"HBjAVbAuaiva","executionInfo":{"status":"ok","timestamp":1669759249691,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["### Classe Tokenizer Custom"],"metadata":{"id":"JCy5XCYL1yhB"}},{"cell_type":"code","source":["class CustomTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n","    self._reserved_tokens = reserved_tokens\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","    \n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","    \n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens)        "],"metadata":{"id":"LsmkD3Kb1k65","executionInfo":{"status":"ok","timestamp":1669759249692,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["tokenizers = tf.Module()\n","tokenizers.it = CustomTokenizer(reserved_tokens, it_vocab_filenamepath)\n","\n","tf.saved_model.save(tokenizers, tokenizer_filenamepath)"],"metadata":{"id":"65DpY7QUj1jz","executionInfo":{"status":"ok","timestamp":1669759253457,"user_tz":-60,"elapsed":3773,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VeyDzXMQzP_o","executionInfo":{"status":"ok","timestamp":1669759253458,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":23,"outputs":[]}]}