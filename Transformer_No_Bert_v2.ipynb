{"cells":[{"cell_type":"code","source":["!pip install -q -U 'tensorflow-text==2.8.*'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NE4enZGpvMRX","executionInfo":{"status":"ok","timestamp":1669887552436,"user_tz":-60,"elapsed":73045,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"outputId":"235c6ca0-01e0-4f9f-b893-9d1c32f789b9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 12.6 MB/s \n","\u001b[K     |████████████████████████████████| 498.0 MB 12 kB/s \n","\u001b[K     |████████████████████████████████| 462 kB 62.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.4 MB 62.6 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 63.7 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -q tf-models-official"],"metadata":{"id":"FPtWz_qHuofc","executionInfo":{"status":"ok","timestamp":1669887634173,"user_tz":-60,"elapsed":81747,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"290d3853-081a-4dd3-ecd0-c0bacc3d17a7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 2.3 MB 14.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.3 MB 75.4 MB/s \n","\u001b[K     |████████████████████████████████| 38.2 MB 154 kB/s \n","\u001b[K     |████████████████████████████████| 662 kB 68.3 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 70.0 MB/s \n","\u001b[K     |████████████████████████████████| 238 kB 60.5 MB/s \n","\u001b[K     |████████████████████████████████| 352 kB 29.0 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 58.7 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 1.8 MB/s \n","\u001b[K     |████████████████████████████████| 588.3 MB 22 kB/s \n","\u001b[K     |████████████████████████████████| 118 kB 76.0 MB/s \n","\u001b[K     |████████████████████████████████| 6.0 MB 71.7 MB/s \n","\u001b[K     |████████████████████████████████| 439 kB 60.0 MB/s \n","\u001b[K     |████████████████████████████████| 1.7 MB 65.0 MB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"hJy-juNOpUOY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669887665390,"user_tz":-60,"elapsed":31235,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"outputId":"a9a19d19-8224-463a-b494-a964669099ed"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"id":"UaAiWsEuC_4K","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669887672336,"user_tz":-60,"elapsed":6952,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"outputId":"61cb4235-3d13-4720-f6fc-bbf064e97f5d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.8.0 and strictly below 2.11.0 (nightly versions are not supported). \n"," The versions of TensorFlow you are currently using is 2.11.0 and is not supported. \n","Some things might work, some things might not.\n","If you were to encounter a bug, do not file an issue.\n","If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n","You can find the compatibility matrix in TensorFlow Addon's readme:\n","https://github.com/tensorflow/addons\n","  warnings.warn(\n"]}],"source":["import os\n","import re\n","import time\n","import unicodedata\n","import datetime\n","import pathlib\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import backend as K\n","from keras import layers\n","\n","import tensorflow_hub as hub\n","import tensorflow_models as tfm\n","\n","import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"]},{"cell_type":"code","source":["tf.get_logger().setLevel('ERROR')\n","tf.config.run_functions_eagerly(True)"],"metadata":{"id":"uKEqRlKowOQS","executionInfo":{"status":"ok","timestamp":1669887672337,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Variabili Globali"],"metadata":{"id":"HRe16D-rUBLA"}},{"cell_type":"code","source":["# PARAMETRI GLOBALI\n","root_folder = 'drive/MyDrive/BERT/'\n","\n","# DATI\n","data_folder_name = 'data'\n","train_filename = 'ita.txt'\n","\n","DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n","train_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n","\n","# PATH LOG Tensorboard\n","PATH_LOG = 'logs/fit/transformer_no_bert_v2'\n","PATH_LOG = os.path.abspath(os.path.join(root_folder, PATH_LOG))\n","log_dir =  os.path.abspath(os.path.join(PATH_LOG, datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))) \n","\n","# PATH WEIGHTS Tensorboard\n","PATH_WEIGHTS = 'weights/transformer_nobert_v2'\n","PATH_WEIGHTS = os.path.abspath(os.path.join(root_folder, PATH_WEIGHTS))\n","checkpoint_path = os.path.abspath(os.path.join(PATH_WEIGHTS, 'cp.ckpt'))\n","log_weights = os.path.abspath(os.path.join(PATH_WEIGHTS, 'weights_30_epochs.h5'))\n","\n","# MODELLO TOKENIZER\n","model_name = 'tokenizer_en_it_model'\n","tokenizer_folder_name = 'tokenizer'\n","\n","TOKEN_PATH = os.path.abspath(os.path.join(root_folder, tokenizer_folder_name))\n","tokenizer_filenamepath = os.path.abspath(os.path.join(TOKEN_PATH, model_name))"],"metadata":{"id":"ewLgCIuEpczO","executionInfo":{"status":"ok","timestamp":1669887731678,"user_tz":-60,"elapsed":471,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# parametri per il modello\n","INPUT_COLUMN = 'input'\n","TARGET_COLUMN = 'target'\n","TARGET_FOR_INPUT = 'target_for_input'\n","NUM_SAMPLES = 27200 # portato da 10.000 a 100.000\n","TRAIN = 24000\n","\n","MAX_VOCAB_SIZE = 20000 # portato da 20.0000 a 200.000\n","EMBEDDING_DIM = 64  # --> 256  Densa non lineare relu --> 64  Densa non lineare relu (oppure Conv1D kernel=1)\n","HIDDEN_DIM = 1024 # numero di celle nei layer ricorrenti nascosti\n","\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 2000\n","EPOCHS = 10\n","MAX_SEQ_LENGTH = 32 # --> portare a 10 con layer 'lambda'[:,:10]\n","\n","NUM_LAYERS = 4 # Numero di layer di Encoder e Decoder del Transformer\n","NUM_HEADS = 8 # Numero di meccanismi di multi-head attention\n","FF_DIM = 16 # Numero di celle dei Layer Feed Forward\n","DROPUOT = 0.1\n","\n","LEARNING_RATE=0.0003\n","\n","# IMPOSTO IL DEBUG A TRUE \n","debug = True\n","training = True"],"metadata":{"id":"8CN-4Uzoqbjl","executionInfo":{"status":"ok","timestamp":1669887734327,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Caricamento Dati"],"metadata":{"id":"LU7AorKXT8K7"}},{"cell_type":"code","source":["# Caricamento dataset: frasi in inglese, frasi in italiano\n","df = pd.read_csv(\n","    train_filenamepath,\n","    sep=\"\\t\",\n","    header=None,\n","    names=[INPUT_COLUMN, TARGET_COLUMN],\n","    usecols=[0,1],\n","    nrows=NUM_SAMPLES\n",")\n","\n","print(df.iloc[10012:10016], '\\n')\n","\n","# Preprocessing dei dati di Input\n","input_data = df[INPUT_COLUMN].tolist()\n","\n","# Preprocessing dei dati Target con aggiunta del token di fine frase\n","target_data = df[TARGET_COLUMN].tolist()\n","\n","train_input_data = input_data[:TRAIN]\n","train_target_data = target_data[:TRAIN]\n","\n","test_input_data = input_data[TRAIN:]\n","test_target_data = target_data[TRAIN:]\n","\n","print('-----------TRAIN SET--------------')\n","print(train_input_data[-4:])\n","print(train_target_data[-4:])\n","print('-----------TEST SET---------------')\n","print(test_input_data[-4:])\n","print(test_target_data[-4:])\n"],"metadata":{"id":"-K_qU8ouq5lO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669887738400,"user_tz":-60,"elapsed":2287,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"outputId":"73373429-5900-4ed6-8375-645af2cba86b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["               input             target\n","10012  They're free.       Sono libere.\n","10013  They're free.  Loro sono libere.\n","10014  They're gone.       Sono andati.\n","10015  They're gone.  Loro sono andati. \n","\n","-----------TRAIN SET--------------\n","[\"You're a cutie.\", \"You're a cutie.\", \"You're a cutie.\", \"You're a cutie.\"]\n","['Sei una ragazza carina.', 'Sei una bella ragazza.', 'Tu sei una ragazza carina.', 'Tu sei una bella ragazza.']\n","-----------TEST SET---------------\n","['I sent Tom home.', 'I sent Tom home.', 'I sent her home.', 'I sent her home.']\n","['Mandai Tom a casa.', 'Io mandai Tom a casa.', \"L'ho mandata a casa.\", \"Io l'ho mandata a casa.\"]\n"]}]},{"cell_type":"markdown","source":["### Tokenizer\n","\n","Carico il modello di tokenizer creato utilizzzando il set di dati a disposizione"],"metadata":{"id":"njyY9RWlFMWu"}},{"cell_type":"code","source":["tokenizers = tf.saved_model.load(tokenizer_filenamepath)"],"metadata":{"id":"-4B-HWWcmsmz","executionInfo":{"status":"ok","timestamp":1669887744288,"user_tz":-60,"elapsed":5893,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(input_data[-2:])\n","print(tokenizers.en.tokenize(input_data[-2:]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize(input_data[-2:]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.en.detokenize(tokenizers.en.tokenize(input_data[-2:])))\n","print('------------------------------------------------------------------')\n","print(target_data[-2:])\n","print(tokenizers.it.tokenize(target_data[-2:]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.it.tokenize(target_data[-2:]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.it.detokenize(tokenizers.it.tokenize(target_data[-2:])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rKEFeDdGFIGS","executionInfo":{"status":"ok","timestamp":1669887746871,"user_tz":-60,"elapsed":833,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"outputId":"ded11aed-6fda-4362-ef4b-27cafa78985a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["['I sent her home.', 'I sent her home.']\n","<tf.RaggedTensor [[2, 34, 757, 139, 147, 11, 3],\n"," [2, 34, 757, 139, 147, 11, 3]]>\n","[[  2  34 757 139 147  11   3   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"," [  2  34 757 139 147  11   3   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n","tf.Tensor([b'i sent her home .' b'i sent her home .'], shape=(2,), dtype=string)\n","------------------------------------------------------------------\n","[\"L'ho mandata a casa.\", \"Io l'ho mandata a casa.\"]\n","<tf.RaggedTensor [[2, 37, 8, 66, 7931, 26, 105, 11, 3],\n"," [2, 60, 37, 8, 66, 7931, 26, 105, 11, 3]]>\n","[[   2   37    8   66 7931   26  105   11    3    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0]\n"," [   2   60   37    8   66 7931   26  105   11    3    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0]]\n","tf.Tensor([b\"l ' ho mandata a casa .\" b\"io l ' ho mandata a casa .\"], shape=(2,), dtype=string)\n"]}]},{"cell_type":"markdown","source":["### Creazione dataset\n","Utilizzo della libreria tf.data per la gestione del dataset da utilizzare.\n","Verranno creati batch di esempi che verranno utilizzati durante l'addestramento."],"metadata":{"id":"5QIDajkEsVU1"}},{"cell_type":"code","source":["def prepare_batch(en, it):\n","  zero = tf.zeros([BATCH_SIZE, MAX_SEQ_LENGTH], tf.int64)\n","  en = tokenizers.en.tokenize(en) # Output is ragged.\n","  en = tf.concat([en, zero], 1)\n","  en = en[:, :MAX_SEQ_LENGTH]     # Trim to MAX_TOKENS.\n","  en = en.to_tensor()             # Convert to 0-padded dense Tensor\n","\n","  it = tokenizers.it.tokenize(it)\n","  it_inputs = it[:, :-1].to_tensor()  # Drop the [END] tokens\n","  it_labels = it[:, 1:].to_tensor()   # Drop the [START] tokens\n","  \n","  it_inputs = tf.concat([it_inputs, zero], 1)\n","  it_inputs = it_inputs[:, :(MAX_SEQ_LENGTH)]\n","\n","  it_labels = tf.concat([it_labels, zero], 1)\n","  it_labels = it_labels[:, :(MAX_SEQ_LENGTH)]\n","\n","  return (en, it_inputs), it_labels"],"metadata":{"id":"ccH3jHoABPzV","executionInfo":{"status":"ok","timestamp":1669887750383,"user_tz":-60,"elapsed":2,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def make_batches(ds):\n","  return (\n","      ds\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .map(prepare_batch, tf.data.AUTOTUNE)\n","      .prefetch(buffer_size=tf.data.AUTOTUNE))"],"metadata":{"id":"l_dswlCiBTdR","executionInfo":{"status":"ok","timestamp":1669887750771,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Definizione del dataset\n","# [from_tensor_slices] permette di recuperare batch\n","# di esempi dai dataset di riferimento\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_input_data, train_target_data))\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_input_data, test_target_data))\n","\n","# impostazione del recupero di esempi presi in maniera\n","# casuale in gruppi di [BATCH_SIZE] tra quelli disponibili\n","train_dataset = make_batches(train_dataset)\n","test_dataset = make_batches(test_dataset)"],"metadata":{"id":"tktJ5YuIsYe3","executionInfo":{"status":"ok","timestamp":1669887751588,"user_tz":-60,"elapsed":820,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8c44ca9-a595-479a-ae08-a00de41b40c5"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Recupero un batch di esempi per la verifica delle classi custom che andrò a creare\n","for (en_input, it_input), it_target in train_dataset.take(1):\n","  print(f'Shape en input           : {en_input.shape}')\n","  print(f'Example en input         : {en_input[0]}')  \n","  print('-------------------------------------------------------')\n","  print(f'Shape it input           : {it_input.shape}')\n","  print(f'Example it input         : {it_input[0]}')  \n","  print(f'Shape it input           : {it_target.shape}')\n","  print(f'Example it target        : {it_target[0]}')  "],"metadata":{"id":"VH_aKPlV_AWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669887752125,"user_tz":-60,"elapsed":539,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"outputId":"4f452350-7854-44e4-dcf9-f816be753b98"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape en input           : (32, 32)\n","Example en input         : [   2   34   44 5809 4988   11    3    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0]\n","-------------------------------------------------------\n","Shape it input           : (32, 32)\n","Example it input         : [   2   66  132 6360   11    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0]\n","Shape it input           : (32, 32)\n","Example it target        : [  66  132 6360   11    3    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0]\n"]}]},{"cell_type":"markdown","source":["### Token and Position Embedding\n","\n","Implementazione del blocco Embedding per l'utilizzo di vettori posizionali insieme ai vettori di token di parole tramite estensione della classe Layer di Keras"],"metadata":{"id":"gAu1IXlRZzlq"}},{"cell_type":"code","source":["class TokenAndPositionEmbedding(layers.Layer):\n","  def __init__(self, maxlen, vocab_size, embed_dim):\n","    super(TokenAndPositionEmbedding, self).__init__()\n","    self.maxlen = maxlen\n","    self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","    self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","  def call(self, x, debug=False):\n","    x = keras.preprocessing.sequence.pad_sequences(x, maxlen=self.maxlen, padding='post')\n","    maxlen = tf.shape(x)[-1]\n","\n","    if debug:\n","      print('********** DEBUG TOKEN AND POSITION EMBEDDING ***********')\n","      print(f'Sequence Max len                          : {maxlen}')\n","      print(f'Sequence Shape                            : {tf.shape(x)}')\n","\n","    positions = tf.range(start=0, limit=maxlen, delta=1)\n","    positions = self.pos_emb(positions)\n","    x = self.token_emb(x)\n","    output = x + positions\n","\n","    if debug:\n","      print(f'Shape TokenAndPositionEmbedding           : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"o9-RSKTqsmUC","executionInfo":{"status":"ok","timestamp":1669887752126,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["token_position_en = TokenAndPositionEmbedding(MAX_SEQ_LENGTH, tokenizers.en.get_vocab_size(), EMBEDDING_DIM)\n","token_position_it = TokenAndPositionEmbedding(MAX_SEQ_LENGTH, tokenizers.it.get_vocab_size(), EMBEDDING_DIM)\n","\n","inputs_encoder = token_position_en(en_input, debug)\n","inputs_decoder = token_position_it(it_input, debug)"],"metadata":{"id":"rr_EWQUX8EWP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669887752563,"user_tz":-60,"elapsed":445,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"outputId":"fd6fa4bc-3da8-4fa3-e714-f08d2542422e"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 32\n","Sequence Shape                            : [32 32]\n","Shape TokenAndPositionEmbedding           : (32, 32, 64)\n","*********************************************************\n","********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 32\n","Sequence Shape                            : [32 32]\n","Shape TokenAndPositionEmbedding           : (32, 32, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["### Encoder\n","\n","Implmentazione di un blocco di EncoderTransformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"_iq7Y-d4eRd8"}},{"cell_type":"code","source":["class Encoder(layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='ENC'):\n","    super(Encoder, self).__init__()\n","    self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = keras.Sequential(\n","      [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = layers.LayerNormalization()\n","    self.layernorm2 = layers.LayerNormalization()\n","    self.dropout1 = layers.Dropout(rate)\n","    self.dropout2 = layers.Dropout(rate)\n","    self._name = name\n","\n","  def call(self, inputs, training=False, debug=False):\n","    attn_output = self.att(query=inputs,\n","                           value=inputs, \n","                           key=inputs)\n","    \n","    attn_output = self.dropout1(attn_output)\n","    out1 = self.layernorm1(inputs + attn_output)\n","\n","    ffn_output = self.ffn(out1)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","\n","    output = self.layernorm2(out1 + ffn_output)\n","\n","    if debug:\n","      print('********************* DEBUG ENCODER *********************')\n","      print(f'Shape Input Layer Encoder       : {inputs.shape}')\n","      print(f'Shape Output Layer Encoder      : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"joTBTlWF8ETD","executionInfo":{"status":"ok","timestamp":1669887752563,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["encoder = Encoder(MAX_SEQ_LENGTH, \n","                  EMBEDDING_DIM, \n","                  NUM_HEADS, \n","                  FF_DIM, \n","                  DROPUOT)\n","\n","outputs_encoder = encoder(inputs=inputs_encoder,\n","                          training=training, \n","                          debug=debug)"],"metadata":{"id":"JaIzBxFCfKe9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669887756695,"user_tz":-60,"elapsed":4136,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"outputId":"8e4e8da1-d40c-499e-84ba-af359f522383"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["********************* DEBUG ENCODER *********************\n","Shape Input Layer Encoder       : (32, 32, 64)\n","Shape Output Layer Encoder      : (32, 32, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["### Decoder\n","\n","Implementazione di un blocco di DecoderTransformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"grNE3Ww9e6Av"}},{"cell_type":"code","source":["class Decoder(layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='DEC'):\n","    super(Decoder, self).__init__()\n","    self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.att2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = keras.Sequential(\n","      [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = layers.LayerNormalization()\n","    self.layernorm2 = layers.LayerNormalization()\n","    self.layernorm3 = layers.LayerNormalization()\n","    self.dropout1 = layers.Dropout(rate)\n","    self.dropout2 = layers.Dropout(rate)\n","    self.dropout3 = layers.Dropout(rate)\n","    self._name = name\n","\n","  def call(self, inputs, encoder_output, training=False, debug=False):\n","    attn_output1 = self.att1(query=inputs,\n","                             value=inputs, \n","                             key=inputs, \n","                             use_causal_mask=True)\n","    \n","    attn_output1 = self.dropout1(attn_output1)\n","    out1 = self.layernorm1(inputs + attn_output1)\n","\n","    attn_output2 = self.att2(key=encoder_output, \n","                             value=encoder_output, \n","                             query=out1)\n","    \n","    attn_output2 = self.dropout2(attn_output2, training=training)\n","    out2 = self.layernorm2(out1 + attn_output2)\n","\n","    ffn_output = self.ffn(out2)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","\n","    output = self.layernorm3(out2 + ffn_output)\n","\n","    if debug:\n","      print('******************* DEBUG DECODER ***********************')\n","      print(f'Input Shape                       : {inputs.shape}')\n","      print(f'Shape Outputs Decoder             : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"SO5rYsFpfFS_","executionInfo":{"status":"ok","timestamp":1669887756696,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(MAX_SEQ_LENGTH, \n","                  EMBEDDING_DIM, \n","                  NUM_HEADS, \n","                  FF_DIM, \n","                  DROPUOT)\n","\n","outputs_decoder = decoder(inputs=inputs_decoder, \n","                          encoder_output=outputs_encoder,  \n","                          training=training,\n","                          debug=debug)"],"metadata":{"id":"yysVdkHH8EPH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669887756696,"user_tz":-60,"elapsed":13,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"outputId":"f9e6f499-6657-451f-bfd4-9c91a1a94e68"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 32, 64)\n","Shape Outputs Decoder             : (32, 32, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["### Transformer\n","\n","Implementazione del blocco Transformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"ne4zTOG_NKfV"}},{"cell_type":"code","execution_count":22,"metadata":{"pycharm":{"name":"#%%\n"},"id":"lw2xMCAMC_4M","executionInfo":{"status":"ok","timestamp":1669887756697,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"outputs":[],"source":["class TransformerBlock(keras.Model):\n","  def __init__(self, \n","               num_layers, \n","               embed_dim, \n","               num_heads, \n","               ff_dim, \n","               max_len,\n","               input_vocab_size,\n","               target_vocab_size,\n","               rate=0.5):\n","    \n","    super(TransformerBlock, self).__init__()\n","\n","    self.num_layers = num_layers\n","\n","    self.token_pos_enc = TokenAndPositionEmbedding(max_len, input_vocab_size, embed_dim)\n","    self.token_pos_dec = TokenAndPositionEmbedding(max_len, target_vocab_size, embed_dim)\n","\n","    self.encoder = [Encoder(max_len, embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n","    self.decoder = [Decoder(max_len, embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n","\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    # self.soft = tf.keras.layers.Dense(target_vocab_size, activation='softmax')\n","\n","  def call(self, inputs, training=False, debug=False):\n","    inputs_encoder, inputs_decoder  = inputs\n","\n","    inputs_encoder = self.token_pos_enc(inputs_encoder, debug)\n","    inputs_decoder = self.token_pos_dec(inputs_decoder, debug)\n","\n","    if debug:\n","      print(f'---------------- DEBUG TRANSFORMER BLOCK ----------------')\n","      print(f'inputs_encoder       : {inputs_encoder.shape}')\n","      print(f'inputs_decoder       : {inputs_decoder.shape}')      \n","\n","    encoder_output = inputs_encoder\n","    transformer_output = inputs_decoder\n","\n","    for i in range(self.num_layers):\n","      encoder_output = self.encoder[i](inputs=encoder_output, \n","                                       training=training, \n","                                       debug=debug) \n","      \n","    for i in range(self.num_layers):\n","      transformer_output = self.decoder[i](inputs=transformer_output, \n","                                           encoder_output=encoder_output, \n","                                           training=training,\n","                                           debug=debug)\n","\n","    logits = self.final_layer(transformer_output)\n","    # logits = self.soft(logits)\n","\n","    if debug:\n","      print(f'Output Shape       : {logits.shape}')\n","      print(f'Output Transformer : {logits[0, :1, :12]}')    \n","      print(f'---------------------------------------------------------')\n","\n","    return logits"]},{"cell_type":"code","source":["transformer = TransformerBlock(NUM_LAYERS, \n","                               EMBEDDING_DIM, \n","                               NUM_HEADS, \n","                               FF_DIM,\n","                               MAX_SEQ_LENGTH,\n","                               tokenizers.en.get_vocab_size(),\n","                               tokenizers.it.get_vocab_size(),\n","                               DROPUOT)\n","\n","transformer_output = transformer((en_input, it_input), \n","                                 training=training,\n","                                 debug=debug)"],"metadata":{"id":"pr--G0ZZVAMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669887757536,"user_tz":-60,"elapsed":847,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"outputId":"9e095f4d-8dfe-4e98-e7a7-459f7a0b88fd"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 32\n","Sequence Shape                            : [32 32]\n","Shape TokenAndPositionEmbedding           : (32, 32, 64)\n","*********************************************************\n","********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 32\n","Sequence Shape                            : [32 32]\n","Shape TokenAndPositionEmbedding           : (32, 32, 64)\n","*********************************************************\n","---------------- DEBUG TRANSFORMER BLOCK ----------------\n","inputs_encoder       : (32, 32, 64)\n","inputs_decoder       : (32, 32, 64)\n","********************* DEBUG ENCODER *********************\n","Shape Input Layer Encoder       : (32, 32, 64)\n","Shape Output Layer Encoder      : (32, 32, 64)\n","*********************************************************\n","********************* DEBUG ENCODER *********************\n","Shape Input Layer Encoder       : (32, 32, 64)\n","Shape Output Layer Encoder      : (32, 32, 64)\n","*********************************************************\n","********************* DEBUG ENCODER *********************\n","Shape Input Layer Encoder       : (32, 32, 64)\n","Shape Output Layer Encoder      : (32, 32, 64)\n","*********************************************************\n","********************* DEBUG ENCODER *********************\n","Shape Input Layer Encoder       : (32, 32, 64)\n","Shape Output Layer Encoder      : (32, 32, 64)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 32, 64)\n","Shape Outputs Decoder             : (32, 32, 64)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 32, 64)\n","Shape Outputs Decoder             : (32, 32, 64)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 32, 64)\n","Shape Outputs Decoder             : (32, 32, 64)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 32, 64)\n","Shape Outputs Decoder             : (32, 32, 64)\n","*********************************************************\n","Output Shape       : (32, 32, 9278)\n","Output Transformer : [[-0.02879459  0.00229713  0.0521832   0.00966768  0.11805477  0.05649727\n","  -0.13492782  0.01763337 -0.06616645  0.0565329   0.04929698 -0.00790004]]\n","---------------------------------------------------------\n"]}]},{"cell_type":"code","source":["transformer.summary()"],"metadata":{"id":"_kwqvJSu8liP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669887757537,"user_tz":-60,"elapsed":39,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}},"outputId":"8886a902-2006-4a61-a0c3-97c888952296"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer_block\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," token_and_position_embeddin  multiple                 395712    \n"," g_2 (TokenAndPositionEmbedd                                     \n"," ing)                                                            \n","                                                                 \n"," token_and_position_embeddin  multiple                 595840    \n"," g_3 (TokenAndPositionEmbedd                                     \n"," ing)                                                            \n","                                                                 \n"," ENC (Encoder)               multiple                  135056    \n","                                                                 \n"," ENC (Encoder)               multiple                  135056    \n","                                                                 \n"," ENC (Encoder)               multiple                  135056    \n","                                                                 \n"," ENC (Encoder)               multiple                  135056    \n","                                                                 \n"," DEC (Decoder)               multiple                  267856    \n","                                                                 \n"," DEC (Decoder)               multiple                  267856    \n","                                                                 \n"," DEC (Decoder)               multiple                  267856    \n","                                                                 \n"," DEC (Decoder)               multiple                  267856    \n","                                                                 \n"," dense_20 (Dense)            multiple                  603070    \n","                                                                 \n","=================================================================\n","Total params: 3,206,270\n","Trainable params: 3,206,270\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["### Addestramento"],"metadata":{"id":"IFmcHTSDTvYk"}},{"cell_type":"code","source":["transformer.compile(\n","    loss= tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=tf.keras.optimizers.Adam(epsilon=1e-9, learning_rate=LEARNING_RATE),\n","    metrics=['accuracy'])"],"metadata":{"id":"bOyqCyjIr-L2","executionInfo":{"status":"ok","timestamp":1669887757538,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Create a callback that saves the model's weights\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 save_best_only=True)\n","\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"],"metadata":{"id":"3hurmpSjJ_dT","executionInfo":{"status":"ok","timestamp":1669887757538,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniele Badiali","userId":"07880650122235290376"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","history = transformer.fit(train_dataset,\n","                          epochs=EPOCHS,\n","                          validation_data=test_dataset,\n","                          callbacks=[tensorboard_callback, \n","                                     cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"etOGtBcer9yi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ad0e6c7e-da3e-42c7-deb6-69d463634665"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","750/750 [==============================] - 616s 820ms/step - loss: 1.9820 - accuracy: 0.8646 - val_loss: 0.8355 - val_accuracy: 0.8773\n","Epoch 2/10\n","750/750 [==============================] - 620s 826ms/step - loss: 0.5978 - accuracy: 0.9137 - val_loss: 0.6644 - val_accuracy: 0.9037\n","Epoch 3/10\n","750/750 [==============================] - 612s 816ms/step - loss: 0.4830 - accuracy: 0.9244 - val_loss: 0.5902 - val_accuracy: 0.9105\n","Epoch 4/10\n","750/750 [==============================] - 628s 838ms/step - loss: 0.4110 - accuracy: 0.9310 - val_loss: 0.5450 - val_accuracy: 0.9144\n","Epoch 5/10\n","750/750 [==============================] - 605s 807ms/step - loss: 0.3550 - accuracy: 0.9366 - val_loss: 0.5162 - val_accuracy: 0.9183\n","Epoch 6/10\n","750/750 [==============================] - 616s 822ms/step - loss: 0.3068 - accuracy: 0.9421 - val_loss: 0.4887 - val_accuracy: 0.9224\n","Epoch 7/10\n","750/750 [==============================] - 595s 793ms/step - loss: 0.2640 - accuracy: 0.9474 - val_loss: 0.4656 - val_accuracy: 0.9241\n","Epoch 8/10\n","200/750 [=======>......................] - ETA: 6:58 - loss: 0.2325 - accuracy: 0.9517"]}]},{"cell_type":"markdown","source":["### Valutazione dell'addestramento\n","Avendo in output il log ed i risultati dell'addestramento, possiamo visualizzare\n","queste informazioni relativamente alle metriche di interesse."],"metadata":{"id":"L0w4wF79UhAp"}},{"cell_type":"code","source":["# visualizzazione andamento addestramento\n","# su un grafico composto da due sub-plot\n","# uno per il loss, l'altro per l'accuracy\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n","\n","# Errore durante l'addestramento\n","ax1.plot(history.history['loss'], label='Loss')\n","ax1.plot(history.history['val_loss'], label='Validation Loss')\n","ax1.set_title('Training Loss')\n","ax1.legend()\n","\n","# Accuratezza durante l'addestramento\n","ax2.plot(history.history['accuracy'], label='Accuracy')\n","ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","ax2.set_title('Training Accuracy')\n","ax2.legend()\n","\n","plt.show()"],"metadata":{"id":"RpXR2p5VAdoG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test del modello\n","La seguente cella permette di caricare l'ultimo checkpoint dell'addestramento\n","precedentemente salvato."],"metadata":{"id":"ReOkcBp2WHWW"}},{"cell_type":"code","source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"],"metadata":{"id":"5PIf_6-RSBb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Translate:\n","  def __init__(self, transformer_block, tokenizers):\n","    self.transformer = transformer_block\n","    self.tokenizers = tokenizers\n","\n","  def predict(self, input_text, max_length):\n","    if input_text is None:\n","      input_text = input_data[np.random.choice(len(input_data))]\n","      print(input_text)\n","\n","    # print(input_text)\n","    inputs_encoder = self.tokenizers.en.tokenize(input_text).to_tensor()\n","    inputs_encoder = keras.preprocessing.sequence.pad_sequences(inputs_encoder, maxlen=max_length, padding='post')\n","\n","    # print(inputs_encoder)\n","    \n","    start_end = self.tokenizers.it.tokenize([''])[0]\n","    start = start_end[0][tf.newaxis]\n","    end = (start_end[1][tf.newaxis]).numpy()[0]\n","\n","    output_array = tf.TensorArray(dtype=tf.int64, size=max_length, dynamic_size=True)\n","    output_array = output_array.write(0, start)     \n","\n","    out_words = []\n","\n","    for i in tf.range(max_length):\n","      # decodifica e recupero probabilità di output\n","      output = tf.transpose(output_array.stack())\n","      # print('Output', output)\n","      transformer_output = transformer([inputs_encoder, output], \n","                                        training=False,\n","                                        debug=False)\n","\n","      predictions = transformer_output[:, -1:, :]\n","\n","      # selezione della parola più probabile\n","      predict = tf.argmax(predictions, -1)\n","      pred_values = (K.argmax(transformer_output, axis=-1)).numpy()\n","      # preds = pred_values[0][predict.numpy()[0][0]]\n","      \n","      # print('predict',  predict)\n","      # print('predictions', transformer_output)\n","      # print('pred_values', pred_values[0][0])\n","      # print('predict[0]', pred_values[0][i])\n","      \n","  \n","      # print('Preds', preds)\n","      # print('Detokenize', (tokenizers.it.detokenize(pred_values)))\n","      # print('##########################')\n","    \n","      # inserimento della parola nella sequenza di output\n","      output_array = output_array.write(i+1, [pred_values[0][i]])\n","      # output_array = output_array.write(i+1, predict[0])\n","\n","      # termine del ciclo quando si incontra il token <end-of-sentence>\n","      # oppure la lunghezza massima prevista della sequenza\n","      # print(end)\n","      if pred_values[0][i] == end:\n","        break\n","\n","    output = tf.transpose(output_array.stack())\n","    text = tokenizers.it.detokenize(output)[0]  \n","\n","    tokens = tokenizers.it.lookup(output)[0]\n","\n","    return text, tokens"],"metadata":{"id":"L2PEoJVb1V8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_sequences = [test_input_data[41], test_input_data[102], test_input_data[612], test_input_data[432], test_input_data[222]]\n","\n","translate = Translate(transformer_block=transformer,\n","                      tokenizers=tokenizers)\n","\n","for test_sequence in test_sequences:\n","  text, token = translate.predict(tf.constant([test_sequence]), MAX_SEQ_LENGTH)\n","\n","  print(f'{\"Input:\":15s}: {test_sequence}')\n","  print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')  \n","  # print(f'tokens : {token}')\n","  # print(target_data[41])\n","  # break\n","  print('---------------------------------------------')\n","\n","print(test_target_data[41])\n","print(test_target_data[102])\n","print(test_target_data[612])\n","print(test_target_data[432])\n","print(test_target_data[222])"],"metadata":{"id":"udIjI2jZWR6g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tensorboard"],"metadata":{"id":"YJf4hjv4PMAJ"}},{"cell_type":"code","source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"],"metadata":{"id":"vcwHe7VJWt-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["log_dir"],"metadata":{"id":"7AB28JmGPQgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir drive/MyDrive/BERT/logs/fit/20221026-134720"],"metadata":{"id":"2ZkkDKVwPT2O"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"collapsed_sections":["gAu1IXlRZzlq","_iq7Y-d4eRd8","grNE3Ww9e6Av","ne4zTOG_NKfV","IFmcHTSDTvYk","L0w4wF79UhAp","ReOkcBp2WHWW","YJf4hjv4PMAJ"]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}