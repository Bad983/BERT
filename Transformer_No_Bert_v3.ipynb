{"cells":[{"cell_type":"code","source":["!pip install -q -U 'tensorflow-text==2.8.*'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NE4enZGpvMRX","executionInfo":{"status":"ok","timestamp":1670882497373,"user_tz":-60,"elapsed":72016,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"dd8ae333-2300-4441-a120-a5ffba1917ff"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 20.7 MB/s \n","\u001b[K     |████████████████████████████████| 498.0 MB 12 kB/s \n","\u001b[K     |████████████████████████████████| 462 kB 57.0 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 59.5 MB/s \n","\u001b[K     |████████████████████████████████| 1.4 MB 61.8 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -q tf-models-official"],"metadata":{"id":"FPtWz_qHuofc","executionInfo":{"status":"ok","timestamp":1670882574002,"user_tz":-60,"elapsed":76636,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9abba2e2-4344-4612-d56b-d36a35407e9d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 2.4 MB 30.6 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 51.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.3 MB 72.5 MB/s \n","\u001b[K     |████████████████████████████████| 238 kB 81.9 MB/s \n","\u001b[K     |████████████████████████████████| 118 kB 78.7 MB/s \n","\u001b[K     |████████████████████████████████| 352 kB 67.6 MB/s \n","\u001b[K     |████████████████████████████████| 2.3 MB 67.4 MB/s \n","\u001b[K     |████████████████████████████████| 38.2 MB 216 kB/s \n","\u001b[K     |████████████████████████████████| 588.3 MB 20 kB/s \n","\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n","\u001b[K     |████████████████████████████████| 662 kB 69.9 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 67.9 MB/s \n","\u001b[K     |████████████████████████████████| 6.0 MB 52.8 MB/s \n","\u001b[K     |████████████████████████████████| 1.7 MB 73.6 MB/s \n","\u001b[K     |████████████████████████████████| 439 kB 81.3 MB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"hJy-juNOpUOY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670882603471,"user_tz":-60,"elapsed":29491,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"804546f5-fa18-43a2-98bb-441654c96512"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"id":"UaAiWsEuC_4K","executionInfo":{"status":"ok","timestamp":1670882609374,"user_tz":-60,"elapsed":5910,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"outputs":[],"source":["import os\n","import re\n","import time\n","import unicodedata\n","import datetime\n","import pathlib\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import backend as K\n","from keras import layers\n","\n","import tensorflow_hub as hub\n","import tensorflow_models as tfm\n","\n","import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"]},{"cell_type":"code","source":["tf.get_logger().setLevel('ERROR')\n","tf.config.run_functions_eagerly(True)"],"metadata":{"id":"uKEqRlKowOQS","executionInfo":{"status":"ok","timestamp":1670882609379,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### Variabili Globali"],"metadata":{"id":"HRe16D-rUBLA"}},{"cell_type":"code","source":["# PARAMETRI GLOBALI\n","root_folder = 'drive/MyDrive/BERT/'\n","\n","# DATI\n","data_folder_name = 'data'\n","train_filename = 'ita.txt'\n","\n","DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n","train_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n","\n","# PATH LOG Tensorboard\n","PATH_LOG = 'logs/fit/transformer_no_bert_v3'\n","PATH_LOG = os.path.abspath(os.path.join(root_folder, PATH_LOG))\n","log_dir =  os.path.abspath(os.path.join(PATH_LOG, datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))) \n","\n","# PATH WEIGHTS Tensorboard\n","PATH_WEIGHTS = 'weights/transformer_nobert_v3'\n","PATH_WEIGHTS = os.path.abspath(os.path.join(root_folder, PATH_WEIGHTS))\n","checkpoint_path = os.path.abspath(os.path.join(PATH_WEIGHTS, 'cp.ckpt'))\n","\n","# MODELLO TOKENIZER\n","model_name = 'tokenizer_en_it_model'\n","tokenizer_folder_name = 'tokenizer'\n","\n","TOKEN_PATH = os.path.abspath(os.path.join(root_folder, tokenizer_folder_name))\n","tokenizer_filenamepath = os.path.abspath(os.path.join(TOKEN_PATH, model_name))\n","\n","# VOCABOLARIO\n","vocab_folder = 'vocab'\n","en_vocab_finalname = 'en_vocab_custom.txt'\n","it_vocab_finalname = 'it_vocab_custom.txt'\n","\n","VOCAB_PATH = os.path.abspath(os.path.join(root_folder, vocab_folder))\n","en_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, en_vocab_finalname))\n","it_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, it_vocab_finalname))"],"metadata":{"id":"ewLgCIuEpczO","executionInfo":{"status":"ok","timestamp":1670882609380,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Parametri Modello"],"metadata":{"id":"nW6B5GqUIoPl"}},{"cell_type":"code","source":["# parametri per il modello\n","INPUT_COLUMN = 'input'\n","TARGET_COLUMN = 'target'\n","\n","# parametri per il modello\n","NUM_SAMPLES = 300000 \n","TRAIN = 18016\n","VALIDATION = 6016\n","TEST = 100\n","\n","MAX_VOCAB_SIZE = 20000 # portato da 20.0000 a 200.000\n","EMBEDDING_DIM = 64  # --> 256  Densa non lineare relu --> 64  Densa non lineare relu (oppure Conv1D kernel=1)\n","HIDDEN_DIM = 1024 # numero di celle nei layer ricorrenti nascosti\n","\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 2000\n","MAX_SEQ_LENGTH = 64\n","\n","NUM_LAYERS = 1 # Numero di layer di Encoder e Decoder del Transformer\n","NUM_HEADS = 8 # Numero di meccanismi di multi-head attention\n","FF_DIM = 16 # Numero di celle dei Layer Feed Forward\n","DROPUOT = 0.5\n","\n","# Ottimizzatore Adam\n","LEARNING_RATE_ADAM = 1e-4\n","BETA_1 = 0.66\n","BETA_2 = 0.999\n","EPOCHS_ADAM = 50\n","\n","# IMPOSTO IL DEBUG A TRUE \n","debug = True\n","training = True"],"metadata":{"id":"8CN-4Uzoqbjl","executionInfo":{"status":"ok","timestamp":1670882609381,"user_tz":-60,"elapsed":15,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"LU7AorKXT8K7"}},{"cell_type":"code","source":["# Caricamento dataset: frasi in inglese, frasi in italiano\n","df = pd.read_csv(\n","    train_filenamepath,\n","    sep=\"\\t\",\n","    header=None,\n","    names=[INPUT_COLUMN, TARGET_COLUMN],\n","    usecols=[0,1],\n","    nrows=NUM_SAMPLES\n",")\n","\n","df = df[-(TRAIN+VALIDATION+TEST):].reset_index(drop=True)\n","\n","# Mischio il dataset in modo che sia più uniforme tra train e test\n","df = df.iloc[np.random.permutation(df.index)].reset_index(drop=True)\n","\n","print(df.iloc[-4:], '\\n')\n","\n","# Preprocessing dei dati di Input\n","input_data = df[INPUT_COLUMN].tolist()\n","\n","# Preprocessing dei dati Target con aggiunta del token di fine frase\n","target_data = df[TARGET_COLUMN].tolist()\n","\n","train_input_data = input_data[:TRAIN]\n","train_target_data = target_data[:TRAIN]\n","\n","validation_input_data = input_data[TRAIN:TRAIN+VALIDATION]\n","validation_target_data = target_data[TRAIN:TRAIN+VALIDATION]\n","\n","test_input_data = input_data[TRAIN+VALIDATION:]\n","test_target_data = target_data[TRAIN+VALIDATION:]\n","\n","print('-----------TRAIN SET--------------')\n","print(train_input_data[-4:])\n","print(train_target_data[-4:])\n","print('-----------VALIDATION SET---------------')\n","print(validation_input_data[-4:])\n","print(validation_target_data[-4:])\n","print('-----------TEST SET---------------')\n","print(test_input_data[-4:])\n","print(test_target_data[-4:])"],"metadata":{"id":"-K_qU8ouq5lO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670883364942,"user_tz":-60,"elapsed":1694,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"5e644161-2e9f-4942-9186-6f66ab0942f5"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["                                      input  \\\n","24128    Why would Tom want to change that?   \n","24129    He is the shorter of the two boys.   \n","24130  Do you know Tom's girlfriend's name?   \n","24131   Bilingual dictionaries are allowed.   \n","\n","                                    target  \n","24128       Perché Tom vorrebbe cambiarlo?  \n","24129  Lui è il più basso dei due ragazzi.  \n","24130   Lo sa il nome della morosa di Tom?  \n","24131  I dizionari bilingue sono permessi.   \n","\n","-----------TRAIN SET--------------\n","['Do you think something will happen?', 'I wish I had bought a smaller house.', 'That island is American territory.', 'You seem to be waiting for somebody.']\n","['Pensate che succederà qualcosa?', 'Vorrei avere comprato una casa più piccola.', \"Quell'isola è territorio americano.\", 'Sembra che lei stia aspettando qualcuno.']\n","-----------VALIDATION SET---------------\n","[\"You should've done that yesterday.\", \"Let's try to answer these questions.\", 'Have you ever heard of Tom Jackson?', 'Tom plays with his friends online.']\n","['Lo avresti dovuto fare ieri.', 'Proviamo a dare qualche risposta a queste domande.', 'Ha mai sentito parlare di Tom Jackson?', 'Tom gioca con le sue amiche online.']\n","-----------TEST SET---------------\n","['Why would Tom want to change that?', 'He is the shorter of the two boys.', \"Do you know Tom's girlfriend's name?\", 'Bilingual dictionaries are allowed.']\n","['Perché Tom vorrebbe cambiarlo?', 'Lui è il più basso dei due ragazzi.', 'Lo sa il nome della morosa di Tom?', 'I dizionari bilingue sono permessi.']\n"]}]},{"cell_type":"markdown","source":["### Analisi Dati"],"metadata":{"id":"sM2ObgDNJfGj"}},{"cell_type":"code","source":["print(f'Esempi nel Dataset di Train                            : {len(train_input_data)}')\n","print(f'Frase più corta in inglese nel Dataset di Train        : {min(train_input_data, key = len)}')\n","print(f'Frase più corta in italiano nel Dataset di Train       : {min(train_target_data, key = len)}')\n","print(f'Frase più lunga in inglese nel Dataset di Train        : {max(train_input_data, key = len)}')\n","print(f'Frase più lunga in italiano nel Dataset di Train       : {max(train_target_data, key = len)}')\n","print('---------------------------------------------------------------------------------------')\n","print(f'Esempi nel Dataset di Validation                       : {len(validation_input_data)}')\n","print(f'Frase più corta in inglese nel Dataset di Validation   : {min(validation_input_data, key = len)}')\n","print(f'Frase più corta in italiano nel Dataset di Validation  : {min(validation_target_data, key = len)}')\n","print(f'Frase più lunga in inglese nel Dataset di Validation   : {max(validation_input_data, key = len)}')\n","print(f'Frase più lunga in italiano nel Dataset di Validation  : {max(validation_target_data, key = len)}')\n","print('---------------------------------------------------------------------------------------')\n","print(f'Esempi nel Dataset di Test                             : {len(test_input_data)}')\n","print(f'Frase più corta in inglese nel Dataset di Test         : {min(test_input_data, key = len)}')\n","print(f'Frase più corta in italiano nel Dataset di Test        : {min(test_target_data, key = len)}')\n","print(f'Frase più lunga in inglese nel Dataset di Test         : {max(test_input_data, key = len)}')\n","print(f'Frase più lunga in italiano nel Dataset di Test        : {max(test_target_data, key = len)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nAQe4pdQJg2t","executionInfo":{"status":"ok","timestamp":1670883368108,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"d7e67c45-2183-46da-b13d-01050cccbdbd"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Esempi nel Dataset di Train                            : 18016\n","Frase più corta in inglese nel Dataset di Train        : What time is it now? \"It's 2:30.\"\n","Frase più corta in italiano nel Dataset di Train       : Vi aspetterò.\n","Frase più lunga in inglese nel Dataset di Train        : Am I pronouncing your name correctly?\n","Frase più lunga in italiano nel Dataset di Train       : Per favore, non fare bollire le uova fino a farle diventare così dure.\n","---------------------------------------------------------------------------------------\n","Esempi nel Dataset di Validation                       : 6016\n","Frase più corta in inglese nel Dataset di Validation   : Are you in a hurry? \"Not really.\"\n","Frase più corta in italiano nel Dataset di Validation  : Andrà tutto bene.\n","Frase più lunga in inglese nel Dataset di Validation   : Both the boy and the girl are clever.\n","Frase più lunga in italiano nel Dataset di Validation  : È passato un po' di tempo dall'ultima volta che ci siamo incontrati.\n","---------------------------------------------------------------------------------------\n","Esempi nel Dataset di Test                             : 100\n","Frase più corta in inglese nel Dataset di Test         : This toy is almost indestructible.\n","Frase più corta in italiano nel Dataset di Test        : Tom era l'unico in casa.\n","Frase più lunga in inglese nel Dataset di Test         : Are the cherries ripe enough to pick?\n","Frase più lunga in italiano nel Dataset di Test        : Le ciliege sono sufficientemente mature per essere raccolte?\n"]}]},{"cell_type":"markdown","source":["## Tokenizer\n","\n","Carico il modello di tokenizer creato utilizzzando il set di dati a disposizione"],"metadata":{"id":"njyY9RWlFMWu"}},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n","dataset = dataset.shuffle(len(input_data)).batch(BATCH_SIZE, drop_remainder=True)\n","\n","train_en = dataset.map(lambda en, it: en)\n","train_it = dataset.map(lambda en, it: it)\n","\n","bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","bert_vocab_args = dict(\n","    # The target vocabulary size\n","    vocab_size = MAX_VOCAB_SIZE,\n","    # Reserved tokens that must be included in the vocabulary\n","    reserved_tokens=reserved_tokens,\n","    # Arguments for `text.BertTokenizer`\n","    bert_tokenizer_params=bert_tokenizer_params,\n","    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","    learn_params={},\n",")"],"metadata":{"id":"-4B-HWWcmsmz","executionInfo":{"status":"ok","timestamp":1670883369971,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"275a8886-ca4e-498e-db61-c2bee7fb344b"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)"],"metadata":{"id":"kTAPOPfHVTAa","executionInfo":{"status":"ok","timestamp":1670883370529,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","exist_vocab = Path(en_vocab_filenamepath)\n","\n","if not exist_vocab.exists():\n","  en_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_en.batch(MAX_VOCAB_SIZE).prefetch(tf.data.AUTOTUNE),\n","      **bert_vocab_args\n","  )\n","\n","  write_vocab_file(en_vocab_filenamepath, en_vocab)"],"metadata":{"id":"fRHSdOWWVVRD","executionInfo":{"status":"ok","timestamp":1670883380521,"user_tz":-60,"elapsed":9370,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["exist_vocab = Path(it_vocab_filenamepath)\n","\n","if not exist_vocab.exists():\n","  it_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_it.batch(MAX_VOCAB_SIZE).prefetch(tf.data.AUTOTUNE),\n","      **bert_vocab_args\n","  )\n","\n","  write_vocab_file(it_vocab_filenamepath, it_vocab)"],"metadata":{"id":"3KhpeO-HVV91","executionInfo":{"status":"ok","timestamp":1670883397658,"user_tz":-60,"elapsed":17169,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["en_tokenizer = text.BertTokenizer(en_vocab_filenamepath, **bert_tokenizer_params)\n","it_tokenizer = text.BertTokenizer(it_vocab_filenamepath, **bert_tokenizer_params)"],"metadata":{"id":"LG4tb0BHVXzg","executionInfo":{"status":"ok","timestamp":1670883397659,"user_tz":-60,"elapsed":44,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape()[0]\n","  starts = tf.fill([count,1], START)\n","  ends = tf.fill([count,1], END)\n","  x = tf.concat([starts, ragged, ends], axis=1)\n","  # x = keras.preprocessing.sequence.pad_sequences(x.numpy(), maxlen=MAX_SEQ_LENGTH, padding='post')\n","  return x\n","\n","def cleanup_text(reserved_tokens, token_txt):\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result"],"metadata":{"id":"aUOIr0wXYQM_","executionInfo":{"status":"ok","timestamp":1670883397660,"user_tz":-60,"elapsed":43,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["### Creo la Classe Custom Tokenizer"],"metadata":{"id":"pDnwtYN2YRih"}},{"cell_type":"code","source":["class CustomTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n","    self._reserved_tokens = reserved_tokens\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","    \n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","    \n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens)"],"metadata":{"id":"O4n7BoE1YVvp","executionInfo":{"status":"ok","timestamp":1670883397660,"user_tz":-60,"elapsed":42,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["tokenizers = tf.Module()\n","tokenizers.en = CustomTokenizer(reserved_tokens, en_vocab_filenamepath)\n","tokenizers.it = CustomTokenizer(reserved_tokens, it_vocab_filenamepath)"],"metadata":{"id":"dcw-OBW8YZnL","executionInfo":{"status":"ok","timestamp":1670883400319,"user_tz":-60,"elapsed":2695,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":["### Analisi Dati Tokenizzati"],"metadata":{"id":"6CqSOFvvJo_S"}},{"cell_type":"code","source":["print(f'Vocabolario Inglese  : {tokenizers.en.get_vocab_size()}')\n","print(f'Vocabolario Italiano : {tokenizers.it.get_vocab_size()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c8F_P5oCYvUi","executionInfo":{"status":"ok","timestamp":1670883402045,"user_tz":-60,"elapsed":6,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"c0514b79-dc98-40c8-c51f-5187b8b8bafd"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabolario Inglese  : 1729\n","Vocabolario Italiano : 2179\n"]}]},{"cell_type":"code","source":["print(input_data[-2:])\n","print(tokenizers.en.tokenize(input_data[-2:]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize(input_data[-2:]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.en.detokenize(tokenizers.en.tokenize(input_data[-2:])))\n","print('------------------------------------------------------------------')\n","print(target_data[-2:])\n","print(tokenizers.it.tokenize(target_data[-2:]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.it.tokenize(target_data[-2:]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.it.detokenize(tokenizers.it.tokenize(target_data[-2:])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rKEFeDdGFIGS","executionInfo":{"status":"ok","timestamp":1670883410469,"user_tz":-60,"elapsed":484,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"0ad467b1-a493-4b5d-c3fc-070b55ec89e1"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["[\"Do you know Tom's girlfriend's name?\", 'Bilingual dictionaries are allowed.']\n","<tf.RaggedTensor [[2, 57, 52, 76, 53, 8, 42, 338, 8, 42, 302, 23, 3],\n"," [2, 25, 1404, 1623, 27, 428, 1248, 1300, 73, 897, 11, 3]]>\n","[[   2   57   52   76   53    8   42  338    8   42  302   23    3    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]\n"," [   2   25 1404 1623   27  428 1248 1300   73  897   11    3    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]]\n","tf.Tensor(\n","[b\"do you know tom ' s girlfriend ' s name ?\"\n"," b'bilingual dictionaries are allowed .'], shape=(2,), dtype=string)\n","------------------------------------------------------------------\n","['Lo sa il nome della morosa di Tom?', 'I dizionari bilingue sono permessi.']\n","<tf.RaggedTensor [[2, 78, 154, 56, 357, 120, 936, 54, 52, 23, 3],\n"," [2, 32, 54, 860, 272, 198, 25, 139, 290, 916, 1142, 361, 64, 61, 591, 469,\n","  11, 3]                                                                   ]>\n","[[   2   78  154   56  357  120  936   54   52   23    3    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]\n"," [   2   32   54  860  272  198   25  139  290  916 1142  361   64   61\n","   591  469   11    3    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]]\n","tf.Tensor(\n","[b'lo sa il nome della morosa di tom ?'\n"," b'i dizionari bilingue sono permessi .'], shape=(2,), dtype=string)\n"]}]},{"cell_type":"code","source":["print([min(train_input_data, key = len)])\n","print(tokenizers.en.tokenize([min(train_input_data, key = len)]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize([min(train_input_data, key = len)]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.en.detokenize(tokenizers.en.tokenize([min(train_input_data, key = len)])))\n","print('------------------------------------------------------------------')\n","print([min(train_target_data, key = len)])\n","print(tokenizers.en.tokenize([min(train_target_data, key = len)]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize([min(train_target_data, key = len)]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.en.detokenize(tokenizers.en.tokenize([min(train_target_data, key = len)])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lC9y11sVJvUD","executionInfo":{"status":"ok","timestamp":1670883414023,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"103e05bb-5a93-456c-a7db-22159fc25e91"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["['What time is it now? \"It\\'s 2:30.\"']\n","<tf.RaggedTensor [[2, 71, 104, 55, 68, 145, 23, 5, 68, 8, 42, 14, 22, 470, 11, 5, 3]]>\n","[[  2  71 104  55  68 145  23   5  68   8  42  14  22 470  11   5   3   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]]\n","tf.Tensor([b'what time is it now ? \" it \\' s 2 : 30 . \"'], shape=(1,), dtype=string)\n","------------------------------------------------------------------\n","['Vi aspetterò.']\n","<tf.RaggedTensor [[2, 45, 537, 114, 748, 614, 581, 381, 11, 3]]>\n","[[  2  45 537 114 748 614 581 381  11   3   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","    0   0   0   0   0   0   0   0   0   0]]\n","tf.Tensor([b'vi aspettero .'], shape=(1,), dtype=string)\n"]}]},{"cell_type":"code","source":["print([max(train_input_data, key = len)])\n","print(tokenizers.en.tokenize([max(train_input_data, key = len)]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize([max(train_input_data, key = len)]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.en.detokenize(tokenizers.en.tokenize([max(train_input_data, key = len)])))\n","print('------------------------------------------------------------------')\n","print([max(train_target_data, key = len)])\n","print(tokenizers.en.tokenize([max(train_target_data, key = len)]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize([max(train_target_data, key = len)]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post'))\n","print(tokenizers.en.detokenize(tokenizers.en.tokenize([max(train_target_data, key = len)])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9N9ER7WJxUs","executionInfo":{"status":"ok","timestamp":1670883415597,"user_tz":-60,"elapsed":509,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"800aa83e-8f27-426e-92e5-49d2f3d6247b"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["['Am I pronouncing your name correctly?']\n","<tf.RaggedTensor [[2, 206, 32, 39, 241, 543, 381, 1625, 1397, 70, 302, 1540, 195, 23, 3]]>\n","[[   2  206   32   39  241  543  381 1625 1397   70  302 1540  195   23\n","     3    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]]\n","tf.Tensor([b'am i pronouncing your name correctly ?'], shape=(1,), dtype=string)\n","------------------------------------------------------------------\n","['Per favore, non fare bollire le uova fino a farle diventare così dure.']\n","<tf.RaggedTensor [[2, 39, 166, 870, 172, 9, 149, 353, 628, 172, 25, 1025, 812, 1023, 35,\n","  172, 44, 381, 1726, 294, 29, 623, 381, 24, 628, 370, 27, 475, 493, 526,\n","  26, 1509, 537, 27, 571, 11, 3]]>\n","[[   2   39  166  870  172    9  149  353  628  172   25 1025  812 1023\n","    35  172   44  381 1726  294   29  623  381   24  628  370   27  475\n","   493  526   26 1509  537   27  571   11    3    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0]]\n","tf.Tensor([b'per favore , non fare bollire le uova fino a farle diventare cosi dure .'], shape=(1,), dtype=string)\n"]}]},{"cell_type":"markdown","source":["## Creazione Dataset\n","Utilizzo della libreria tf.data per la gestione del dataset da utilizzare.\n","Verranno creati batch di esempi che verranno utilizzati durante l'addestramento."],"metadata":{"id":"5QIDajkEsVU1"}},{"cell_type":"code","source":["def prepare_batch(en, it):\n","  zero = tf.zeros([BATCH_SIZE, MAX_SEQ_LENGTH], tf.int64)\n","  en = tokenizers.en.tokenize(en) # Output is ragged.\n","  en = tf.concat([en, zero], 1)\n","  en = en[:, :MAX_SEQ_LENGTH]     # Trim to MAX_TOKENS.\n","  en = en.to_tensor()             # Convert to 0-padded dense Tensor\n","\n","  it = tokenizers.it.tokenize(it)\n","  it_inputs = it[:, :-1].to_tensor()  # Drop the [END] tokens\n","  it_labels = it[:, 1:].to_tensor()   # Drop the [START] tokens\n","  \n","  it_inputs = tf.concat([it_inputs, zero], 1)\n","  it_inputs = it_inputs[:, :(MAX_SEQ_LENGTH)]\n","\n","  it_labels = tf.concat([it_labels, zero], 1)\n","  it_labels = it_labels[:, :(MAX_SEQ_LENGTH)]\n","\n","  return (en, it_inputs), it_labels"],"metadata":{"id":"ccH3jHoABPzV","executionInfo":{"status":"ok","timestamp":1670883416824,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["def make_batches(ds):\n","  return (\n","      ds\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .map(prepare_batch, tf.data.AUTOTUNE)\n","      .prefetch(buffer_size=tf.data.AUTOTUNE))"],"metadata":{"id":"l_dswlCiBTdR","executionInfo":{"status":"ok","timestamp":1670883416825,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["# Definizione del dataset\n","# [from_tensor_slices] permette di recuperare batch\n","# di esempi dai dataset di riferimento\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_input_data, train_target_data))\n","validation_dataset = tf.data.Dataset.from_tensor_slices((validation_input_data, validation_target_data))\n","\n","# impostazione del recupero di esempi presi in maniera\n","# casuale in gruppi di [BATCH_SIZE] tra quelli disponibili\n","train_dataset = make_batches(train_dataset)\n","validation_dataset = make_batches(validation_dataset)"],"metadata":{"id":"tktJ5YuIsYe3","executionInfo":{"status":"ok","timestamp":1670883419739,"user_tz":-60,"elapsed":1953,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["# Recupero un batch di esempi per la verifica delle classi custom che andrò a creare\n","for (en_input, it_input), it_target in train_dataset.take(1):\n","  print(f'Shape en input           : {en_input.shape}')\n","  print(f'Example en input         : {en_input[0]}')  \n","  print('-------------------------------------------------------')\n","  print(f'Shape it input           : {it_input.shape}')\n","  print(f'Example it input         : {it_input[0]}')  \n","  print(f'Shape it input           : {it_target.shape}')\n","  print(f'Example it target        : {it_target[0]}')  "],"metadata":{"id":"VH_aKPlV_AWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670883420269,"user_tz":-60,"elapsed":538,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"670e3921-4466-45f7-d32f-4798d72b1bec"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape en input           : (32, 64)\n","Example en input         : [  2  79  62   8  43  52 668  59  54 169 148  23   3   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0]\n","-------------------------------------------------------\n","Shape it input           : (32, 64)\n","Example it input         : [   2   69   53  135  105 1953   62   29 1246  136   23    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0]\n","Shape it input           : (32, 64)\n","Example it target        : [  69   53  135  105 1953   62   29 1246  136   23    3    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0]\n"]}]},{"cell_type":"markdown","source":["## Token and Position Embedding\n","\n","Implementazione del blocco Embedding per l'utilizzo di vettori posizionali insieme ai vettori di token di parole tramite estensione della classe Layer di Keras"],"metadata":{"id":"gAu1IXlRZzlq"}},{"cell_type":"code","source":["class TokenAndPositionEmbedding(layers.Layer):\n","  def __init__(self, maxlen, vocab_size, embed_dim):\n","    super(TokenAndPositionEmbedding, self).__init__()\n","    self.maxlen = maxlen\n","    self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","    self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","  def call(self, x, debug=False):\n","    x = keras.preprocessing.sequence.pad_sequences(x, maxlen=self.maxlen, padding='post')\n","    maxlen = tf.shape(x)[-1]\n","\n","    if debug:\n","      print('********** DEBUG TOKEN AND POSITION EMBEDDING ***********')\n","      print(f'Sequence Max len                          : {maxlen}')\n","      print(f'Sequence Shape                            : {tf.shape(x)}')\n","\n","    positions = tf.range(start=0, limit=maxlen, delta=1)\n","    positions = self.pos_emb(positions)\n","    x = self.token_emb(x)\n","    output = x + positions\n","\n","    if debug:\n","      print(f'Shape TokenAndPositionEmbedding           : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"o9-RSKTqsmUC","executionInfo":{"status":"ok","timestamp":1670883420270,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["token_position_en = TokenAndPositionEmbedding(MAX_SEQ_LENGTH, tokenizers.en.get_vocab_size(), EMBEDDING_DIM)\n","token_position_it = TokenAndPositionEmbedding(MAX_SEQ_LENGTH, tokenizers.it.get_vocab_size(), EMBEDDING_DIM)\n","\n","inputs_encoder = token_position_en(en_input, debug)\n","inputs_decoder = token_position_it(it_input, debug)"],"metadata":{"id":"rr_EWQUX8EWP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670883422901,"user_tz":-60,"elapsed":586,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"cf6fb470-3c81-41a2-b41a-e1ed655829b2"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 64\n","Sequence Shape                            : [32 64]\n","Shape TokenAndPositionEmbedding           : (32, 64, 64)\n","*********************************************************\n","********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 64\n","Sequence Shape                            : [32 64]\n","Shape TokenAndPositionEmbedding           : (32, 64, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["## Encoder\n","\n","Implmentazione di un blocco di EncoderTransformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"_iq7Y-d4eRd8"}},{"cell_type":"code","source":["class Encoder(layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='ENC'):\n","    super(Encoder, self).__init__()\n","    self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = keras.Sequential(\n","      [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = layers.LayerNormalization()\n","    self.layernorm2 = layers.LayerNormalization()\n","    self.dropout1 = layers.Dropout(rate)\n","    self.dropout2 = layers.Dropout(rate)\n","    self._name = name\n","\n","  def call(self, inputs, training=False, debug=False):\n","    attn_output = self.att(query=inputs,\n","                           value=inputs, \n","                           key=inputs)\n","    \n","    attn_output = self.dropout1(attn_output)\n","    out1 = self.layernorm1(inputs + attn_output)\n","\n","    ffn_output = self.ffn(out1)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","\n","    output = self.layernorm2(out1 + ffn_output)\n","\n","    if debug:\n","      print('********************* DEBUG ENCODER *********************')\n","      print(f'Shape Input Layer Encoder       : {inputs.shape}')\n","      print(f'Shape Output Layer Encoder      : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"joTBTlWF8ETD","executionInfo":{"status":"ok","timestamp":1670883425437,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":["encoder = Encoder(MAX_SEQ_LENGTH, \n","                  EMBEDDING_DIM, \n","                  NUM_HEADS, \n","                  FF_DIM, \n","                  DROPUOT)\n","\n","outputs_encoder = encoder(inputs=inputs_encoder,\n","                          training=training, \n","                          debug=debug)"],"metadata":{"id":"JaIzBxFCfKe9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670883431245,"user_tz":-60,"elapsed":4295,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"2b135779-0e01-43b0-d771-2a3a2ac836b7"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["********************* DEBUG ENCODER *********************\n","Shape Input Layer Encoder       : (32, 64, 64)\n","Shape Output Layer Encoder      : (32, 64, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["## Decoder\n","\n","Implementazione di un blocco di DecoderTransformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"grNE3Ww9e6Av"}},{"cell_type":"code","source":["class Decoder(layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='DEC'):\n","    super(Decoder, self).__init__()\n","    self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.att2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = keras.Sequential(\n","      [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = layers.LayerNormalization()\n","    self.layernorm2 = layers.LayerNormalization()\n","    self.layernorm3 = layers.LayerNormalization()\n","    self.dropout1 = layers.Dropout(rate)\n","    self.dropout2 = layers.Dropout(rate)\n","    self.dropout3 = layers.Dropout(rate)\n","    self._name = name\n","\n","  def call(self, inputs, encoder_output, training=False, debug=False):\n","    attn_output1 = self.att1(query=inputs,\n","                             value=inputs, \n","                             key=inputs, \n","                             use_causal_mask=True)\n","    \n","    attn_output1 = self.dropout1(attn_output1)\n","    out1 = self.layernorm1(inputs + attn_output1)\n","\n","    attn_output2 = self.att2(key=encoder_output, \n","                             value=encoder_output, \n","                             query=out1)\n","    \n","    attn_output2 = self.dropout2(attn_output2, training=training)\n","    out2 = self.layernorm2(out1 + attn_output2)\n","\n","    ffn_output = self.ffn(out2)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","\n","    output = self.layernorm3(out2 + ffn_output)\n","\n","    if debug:\n","      print('******************* DEBUG DECODER ***********************')\n","      print(f'Input Shape                       : {inputs.shape}')\n","      print(f'Shape Outputs Decoder             : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"SO5rYsFpfFS_","executionInfo":{"status":"ok","timestamp":1670883431246,"user_tz":-60,"elapsed":36,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(MAX_SEQ_LENGTH, \n","                  EMBEDDING_DIM, \n","                  NUM_HEADS, \n","                  FF_DIM, \n","                  DROPUOT)\n","\n","outputs_decoder = decoder(inputs=inputs_decoder, \n","                          encoder_output=outputs_encoder,  \n","                          training=training,\n","                          debug=debug)"],"metadata":{"id":"yysVdkHH8EPH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670883435848,"user_tz":-60,"elapsed":469,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"69669801-5c1d-438a-8dcd-7b01670d9453"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 64, 64)\n","Shape Outputs Decoder             : (32, 64, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["## Transformer\n","\n","Implementazione del blocco Transformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"ne4zTOG_NKfV"}},{"cell_type":"code","execution_count":62,"metadata":{"pycharm":{"name":"#%%\n"},"id":"lw2xMCAMC_4M","executionInfo":{"status":"ok","timestamp":1670883442387,"user_tz":-60,"elapsed":1035,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"outputs":[],"source":["class TransformerBlock(keras.Model):\n","  def __init__(self, \n","               num_layers, \n","               embed_dim, \n","               num_heads, \n","               ff_dim, \n","               max_len,\n","               input_vocab_size,\n","               target_vocab_size,\n","               rate=0.5):\n","    \n","    super(TransformerBlock, self).__init__()\n","\n","    self.num_layers = num_layers\n","\n","    self.token_pos_enc = TokenAndPositionEmbedding(max_len, input_vocab_size, embed_dim)\n","    self.token_pos_dec = TokenAndPositionEmbedding(max_len, target_vocab_size, embed_dim)\n","\n","    self.encoder = [Encoder(max_len, embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n","    self.decoder = [Decoder(max_len, embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n","\n","    self.dropout = layers.Dropout(rate)\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","  def call(self, inputs, training=False, debug=False):\n","    inputs_encoder, inputs_decoder  = inputs\n","\n","    inputs_encoder = self.token_pos_enc(inputs_encoder, debug)\n","    inputs_decoder = self.token_pos_dec(inputs_decoder, debug)\n","\n","    if debug:\n","      print(f'---------------- DEBUG TRANSFORMER BLOCK ----------------')\n","      print(f'inputs_encoder       : {inputs_encoder.shape}')\n","      print(f'inputs_decoder       : {inputs_decoder.shape}')      \n","\n","    encoder_output = inputs_encoder\n","    transformer_output = inputs_decoder\n","\n","    for i in range(self.num_layers):\n","      encoder_output = self.encoder[i](inputs=encoder_output, \n","                                       training=training, \n","                                       debug=debug) \n","      \n","    for i in range(self.num_layers):\n","      transformer_output = self.decoder[i](inputs=transformer_output, \n","                                           encoder_output=encoder_output, \n","                                           training=training,\n","                                           debug=debug)\n","\n","    transformer_output = self.dropout(transformer_output)\n","    logits = self.final_layer(transformer_output)\n","\n","    if debug:\n","      print(f'Output Shape       : {logits.shape}')\n","      print(f'Output Transformer : {logits[0, :1, :12]}')    \n","      print(f'---------------------------------------------------------')\n","\n","    return logits"]},{"cell_type":"code","source":["transformer = TransformerBlock(NUM_LAYERS, \n","                               EMBEDDING_DIM, \n","                               NUM_HEADS, \n","                               FF_DIM,\n","                               MAX_SEQ_LENGTH,\n","                               tokenizers.en.get_vocab_size(),\n","                               tokenizers.it.get_vocab_size(),\n","                               DROPUOT)\n","\n","transformer_output = transformer((en_input, it_input), \n","                                 training=training,\n","                                 debug=debug)"],"metadata":{"id":"pr--G0ZZVAMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670883445511,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"8946cf87-b913-49eb-bb01-7314c063cfe6"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 64\n","Sequence Shape                            : [32 64]\n","Shape TokenAndPositionEmbedding           : (32, 64, 64)\n","*********************************************************\n","********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 64\n","Sequence Shape                            : [32 64]\n","Shape TokenAndPositionEmbedding           : (32, 64, 64)\n","*********************************************************\n","---------------- DEBUG TRANSFORMER BLOCK ----------------\n","inputs_encoder       : (32, 64, 64)\n","inputs_decoder       : (32, 64, 64)\n","********************* DEBUG ENCODER *********************\n","Shape Input Layer Encoder       : (32, 64, 64)\n","Shape Output Layer Encoder      : (32, 64, 64)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 64, 64)\n","Shape Outputs Decoder             : (32, 64, 64)\n","*********************************************************\n","Output Shape       : (32, 64, 2179)\n","Output Transformer : [[ 0.35788104  0.17997235  0.45243785  0.19010302  0.13346826  0.34313866\n","  -0.6479264  -0.15634535  0.02137411  0.22074032  0.24745895 -0.07987563]]\n","---------------------------------------------------------\n"]}]},{"cell_type":"code","source":["transformer.summary()"],"metadata":{"id":"_kwqvJSu8liP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670883449974,"user_tz":-60,"elapsed":699,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"b07e6063-9b27-43b4-d2c3-f334cfd7c477"},"execution_count":64,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer_block\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," token_and_position_embeddin  multiple                 114752    \n"," g_2 (TokenAndPositionEmbedd                                     \n"," ing)                                                            \n","                                                                 \n"," token_and_position_embeddin  multiple                 143552    \n"," g_3 (TokenAndPositionEmbedd                                     \n"," ing)                                                            \n","                                                                 \n"," ENC (Encoder)               multiple                  135056    \n","                                                                 \n"," DEC (Decoder)               multiple                  267856    \n","                                                                 \n"," dropout_13 (Dropout)        multiple                  0         \n","                                                                 \n"," dense_8 (Dense)             multiple                  141635    \n","                                                                 \n","=================================================================\n","Total params: 802,851\n","Trainable params: 802,851\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["## Addestramento modello conotttimizzatore ADAM"],"metadata":{"id":"IFmcHTSDTvYk"}},{"cell_type":"markdown","source":["### Compilazione"],"metadata":{"id":"MH1nB3ZrKhH8"}},{"cell_type":"code","source":["transformer.compile(\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_ADAM, \n","                                       beta_1=BETA_1, \n","                                       beta_2=BETA_2),\n","    metrics=[keras.metrics.SparseCategoricalAccuracy()])"],"metadata":{"id":"bOyqCyjIr-L2","executionInfo":{"status":"ok","timestamp":1670883453669,"user_tz":-60,"elapsed":4,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["# Create a callback that saves the model's weights\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 save_best_only=True)\n","\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"],"metadata":{"id":"3hurmpSjJ_dT","executionInfo":{"status":"ok","timestamp":1670883455297,"user_tz":-60,"elapsed":9,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}}},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":["### Addestramento"],"metadata":{"id":"MQKaImtuKyun"}},{"cell_type":"code","source":["start = datetime.datetime.now()\n","history = transformer.fit(train_dataset,\n","                          epochs=EPOCHS_ADAM,\n","                          shuffle=True,\n","                          validation_data=validation_dataset,\n","                          callbacks=[tensorboard_callback, \n","                                     cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"etOGtBcer9yi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cc0bdffc-9b04-4aae-f9ae-90ea940c59cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","563/563 [==============================] - 190s 337ms/step - loss: 3.6880 - sparse_categorical_accuracy: 0.8039 - val_loss: 1.2701 - val_sparse_categorical_accuracy: 0.8443\n","Epoch 2/50\n","563/563 [==============================] - 185s 329ms/step - loss: 1.1092 - sparse_categorical_accuracy: 0.8585 - val_loss: 0.9067 - val_sparse_categorical_accuracy: 0.8693\n","Epoch 3/50\n","563/563 [==============================] - 181s 322ms/step - loss: 0.9134 - sparse_categorical_accuracy: 0.8695 - val_loss: 0.8314 - val_sparse_categorical_accuracy: 0.8735\n","Epoch 4/50\n","563/563 [==============================] - 185s 328ms/step - loss: 0.8427 - sparse_categorical_accuracy: 0.8735 - val_loss: 0.7860 - val_sparse_categorical_accuracy: 0.8762\n","Epoch 5/50\n","563/563 [==============================] - 205s 364ms/step - loss: 0.7963 - sparse_categorical_accuracy: 0.8778 - val_loss: 0.7449 - val_sparse_categorical_accuracy: 0.8825\n","Epoch 6/50\n","563/563 [==============================] - 184s 327ms/step - loss: 0.7573 - sparse_categorical_accuracy: 0.8819 - val_loss: 0.7108 - val_sparse_categorical_accuracy: 0.8858\n","Epoch 7/50\n","563/563 [==============================] - 178s 316ms/step - loss: 0.7256 - sparse_categorical_accuracy: 0.8846 - val_loss: 0.6815 - val_sparse_categorical_accuracy: 0.8883\n","Epoch 8/50\n","196/563 [=========>....................] - ETA: 1:49 - loss: 0.7054 - sparse_categorical_accuracy: 0.8859"]}]},{"cell_type":"markdown","source":["### Valutazione dell'addestramento\n","Avendo in output il log ed i risultati dell'addestramento, possiamo visualizzare\n","queste informazioni relativamente alle metriche di interesse."],"metadata":{"id":"L0w4wF79UhAp"}},{"cell_type":"code","source":["# visualizzazione andamento addestramento\n","# su un grafico composto da due sub-plot\n","# uno per il loss, l'altro per l'accuracy\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n","\n","# Errore durante l'addestramento\n","ax1.plot(history.history['loss'], label='Loss')\n","ax1.plot(history.history['val_loss'], label='Validation Loss')\n","ax1.set_title('Training Loss')\n","ax1.legend()\n","\n","# Accuratezza durante l'addestramento\n","ax2.plot(history.history['sparse_categorical_accuracy'], label='Accuracy')\n","ax2.plot(history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n","ax2.set_title('Training Accuracy')\n","ax2.legend()\n","\n","plt.show()"],"metadata":{"id":"RpXR2p5VAdoG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Addestramento 2"],"metadata":{"id":"nckiEBHR8sG-"}},{"cell_type":"code","source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vULBPa4O-sKw","executionInfo":{"status":"ok","timestamp":1670322782828,"user_tz":-60,"elapsed":5028,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"495db5ce-1054-4069-b6e7-7180370c11c9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fe77c0b0250>"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","history = transformer.fit(train_dataset,\n","                          epochs=EPOCHS_ADAM+EPOCHS_ADAM,\n","                          initial_epoch=EPOCHS_ADAM,\n","                          shuffle=True,\n","                          validation_data=validation_dataset,\n","                          callbacks=[tensorboard_callback, \n","                                     cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"WcnDtuSJ8vLA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670331718931,"user_tz":-60,"elapsed":5024692,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"f40edf58-ca0f-49da-e9e1-3c879870c913"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 51/100\n","500/500 [==============================] - 166s 331ms/step - loss: 2.1910 - sparse_categorical_accuracy: 0.6185 - val_loss: 2.6461 - val_sparse_categorical_accuracy: 0.6264\n","Epoch 52/100\n","500/500 [==============================] - 164s 329ms/step - loss: 2.1958 - sparse_categorical_accuracy: 0.6177 - val_loss: 2.6386 - val_sparse_categorical_accuracy: 0.6255\n","Epoch 53/100\n","500/500 [==============================] - 162s 325ms/step - loss: 2.1880 - sparse_categorical_accuracy: 0.6199 - val_loss: 2.6518 - val_sparse_categorical_accuracy: 0.6286\n","Epoch 54/100\n","500/500 [==============================] - 161s 322ms/step - loss: 2.1845 - sparse_categorical_accuracy: 0.6189 - val_loss: 2.6390 - val_sparse_categorical_accuracy: 0.6279\n","Epoch 55/100\n","500/500 [==============================] - 163s 326ms/step - loss: 2.1789 - sparse_categorical_accuracy: 0.6193 - val_loss: 2.6250 - val_sparse_categorical_accuracy: 0.6302\n","Epoch 56/100\n","500/500 [==============================] - 163s 326ms/step - loss: 2.1768 - sparse_categorical_accuracy: 0.6198 - val_loss: 2.6350 - val_sparse_categorical_accuracy: 0.6298\n","Epoch 57/100\n","500/500 [==============================] - 160s 321ms/step - loss: 2.1765 - sparse_categorical_accuracy: 0.6197 - val_loss: 2.6339 - val_sparse_categorical_accuracy: 0.6304\n","Epoch 58/100\n","500/500 [==============================] - 163s 325ms/step - loss: 2.1697 - sparse_categorical_accuracy: 0.6200 - val_loss: 2.6270 - val_sparse_categorical_accuracy: 0.6304\n","Epoch 59/100\n","500/500 [==============================] - 162s 325ms/step - loss: 2.1710 - sparse_categorical_accuracy: 0.6204 - val_loss: 2.6344 - val_sparse_categorical_accuracy: 0.6289\n","Epoch 60/100\n","500/500 [==============================] - 163s 327ms/step - loss: 2.1684 - sparse_categorical_accuracy: 0.6200 - val_loss: 2.6308 - val_sparse_categorical_accuracy: 0.6308\n","Epoch 61/100\n","500/500 [==============================] - 161s 321ms/step - loss: 2.1672 - sparse_categorical_accuracy: 0.6197 - val_loss: 2.6270 - val_sparse_categorical_accuracy: 0.6299\n","Epoch 62/100\n","500/500 [==============================] - 159s 317ms/step - loss: 2.1663 - sparse_categorical_accuracy: 0.6211 - val_loss: 2.6277 - val_sparse_categorical_accuracy: 0.6337\n","Epoch 63/100\n","500/500 [==============================] - 161s 323ms/step - loss: 2.1651 - sparse_categorical_accuracy: 0.6212 - val_loss: 2.6281 - val_sparse_categorical_accuracy: 0.6287\n","Epoch 64/100\n","500/500 [==============================] - 158s 317ms/step - loss: 2.1614 - sparse_categorical_accuracy: 0.6203 - val_loss: 2.6193 - val_sparse_categorical_accuracy: 0.6321\n","Epoch 65/100\n","500/500 [==============================] - 158s 317ms/step - loss: 2.1630 - sparse_categorical_accuracy: 0.6218 - val_loss: 2.6262 - val_sparse_categorical_accuracy: 0.6348\n","Epoch 66/100\n","500/500 [==============================] - 158s 316ms/step - loss: 2.1588 - sparse_categorical_accuracy: 0.6219 - val_loss: 2.6269 - val_sparse_categorical_accuracy: 0.6320\n","Epoch 67/100\n","500/500 [==============================] - 157s 314ms/step - loss: 2.1546 - sparse_categorical_accuracy: 0.6223 - val_loss: 2.6260 - val_sparse_categorical_accuracy: 0.6332\n","Epoch 68/100\n","500/500 [==============================] - 157s 314ms/step - loss: 2.1560 - sparse_categorical_accuracy: 0.6221 - val_loss: 2.6232 - val_sparse_categorical_accuracy: 0.6330\n","Epoch 69/100\n","500/500 [==============================] - 157s 315ms/step - loss: 2.1511 - sparse_categorical_accuracy: 0.6229 - val_loss: 2.6358 - val_sparse_categorical_accuracy: 0.6298\n","Epoch 70/100\n","500/500 [==============================] - 159s 318ms/step - loss: 2.1462 - sparse_categorical_accuracy: 0.6241 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.6338\n","Epoch 71/100\n","500/500 [==============================] - 159s 319ms/step - loss: 2.1504 - sparse_categorical_accuracy: 0.6240 - val_loss: 2.6299 - val_sparse_categorical_accuracy: 0.6345\n","Epoch 72/100\n","500/500 [==============================] - 154s 309ms/step - loss: 2.1483 - sparse_categorical_accuracy: 0.6234 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.6323\n","Epoch 73/100\n","500/500 [==============================] - 153s 306ms/step - loss: 2.1526 - sparse_categorical_accuracy: 0.6225 - val_loss: 2.6174 - val_sparse_categorical_accuracy: 0.6315\n","Epoch 74/100\n","500/500 [==============================] - 156s 313ms/step - loss: 2.1442 - sparse_categorical_accuracy: 0.6237 - val_loss: 2.6327 - val_sparse_categorical_accuracy: 0.6329\n","Epoch 75/100\n","500/500 [==============================] - 155s 310ms/step - loss: 2.1405 - sparse_categorical_accuracy: 0.6253 - val_loss: 2.6295 - val_sparse_categorical_accuracy: 0.6362\n","Epoch 76/100\n","500/500 [==============================] - 157s 314ms/step - loss: 2.1385 - sparse_categorical_accuracy: 0.6249 - val_loss: 2.6309 - val_sparse_categorical_accuracy: 0.6345\n","Epoch 77/100\n","500/500 [==============================] - 156s 313ms/step - loss: 2.1400 - sparse_categorical_accuracy: 0.6232 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.6347\n","Epoch 78/100\n","500/500 [==============================] - 158s 316ms/step - loss: 2.1380 - sparse_categorical_accuracy: 0.6247 - val_loss: 2.6265 - val_sparse_categorical_accuracy: 0.6317\n","Epoch 79/100\n","500/500 [==============================] - 155s 310ms/step - loss: 2.1350 - sparse_categorical_accuracy: 0.6248 - val_loss: 2.6205 - val_sparse_categorical_accuracy: 0.6322\n","Epoch 80/100\n","500/500 [==============================] - 155s 309ms/step - loss: 2.1389 - sparse_categorical_accuracy: 0.6238 - val_loss: 2.6265 - val_sparse_categorical_accuracy: 0.6317\n","Epoch 81/100\n","500/500 [==============================] - 156s 312ms/step - loss: 2.1322 - sparse_categorical_accuracy: 0.6256 - val_loss: 2.6251 - val_sparse_categorical_accuracy: 0.6338\n","Epoch 82/100\n","500/500 [==============================] - 158s 316ms/step - loss: 2.1331 - sparse_categorical_accuracy: 0.6249 - val_loss: 2.6176 - val_sparse_categorical_accuracy: 0.6343\n","Epoch 83/100\n","500/500 [==============================] - 155s 309ms/step - loss: 2.1304 - sparse_categorical_accuracy: 0.6259 - val_loss: 2.6313 - val_sparse_categorical_accuracy: 0.6301\n","Epoch 84/100\n","500/500 [==============================] - 158s 316ms/step - loss: 2.1271 - sparse_categorical_accuracy: 0.6256 - val_loss: 2.6194 - val_sparse_categorical_accuracy: 0.6325\n","Epoch 85/100\n","500/500 [==============================] - 153s 305ms/step - loss: 2.1253 - sparse_categorical_accuracy: 0.6259 - val_loss: 2.6210 - val_sparse_categorical_accuracy: 0.6372\n","Epoch 86/100\n","500/500 [==============================] - 156s 312ms/step - loss: 2.1260 - sparse_categorical_accuracy: 0.6248 - val_loss: 2.6295 - val_sparse_categorical_accuracy: 0.6328\n","Epoch 87/100\n","500/500 [==============================] - 158s 317ms/step - loss: 2.1200 - sparse_categorical_accuracy: 0.6266 - val_loss: 2.6337 - val_sparse_categorical_accuracy: 0.6320\n","Epoch 88/100\n","500/500 [==============================] - 157s 313ms/step - loss: 2.1267 - sparse_categorical_accuracy: 0.6256 - val_loss: 2.6232 - val_sparse_categorical_accuracy: 0.6364\n","Epoch 89/100\n","500/500 [==============================] - 155s 311ms/step - loss: 2.1246 - sparse_categorical_accuracy: 0.6246 - val_loss: 2.6252 - val_sparse_categorical_accuracy: 0.6347\n","Epoch 90/100\n","500/500 [==============================] - 154s 309ms/step - loss: 2.1218 - sparse_categorical_accuracy: 0.6258 - val_loss: 2.6338 - val_sparse_categorical_accuracy: 0.6363\n","Epoch 91/100\n","500/500 [==============================] - 155s 309ms/step - loss: 2.1223 - sparse_categorical_accuracy: 0.6270 - val_loss: 2.6323 - val_sparse_categorical_accuracy: 0.6318\n","Epoch 92/100\n","500/500 [==============================] - 156s 312ms/step - loss: 2.1215 - sparse_categorical_accuracy: 0.6254 - val_loss: 2.6286 - val_sparse_categorical_accuracy: 0.6369\n","Epoch 93/100\n","500/500 [==============================] - 152s 305ms/step - loss: 2.1218 - sparse_categorical_accuracy: 0.6273 - val_loss: 2.6377 - val_sparse_categorical_accuracy: 0.6306\n","Epoch 94/100\n","500/500 [==============================] - 158s 316ms/step - loss: 2.1194 - sparse_categorical_accuracy: 0.6265 - val_loss: 2.6325 - val_sparse_categorical_accuracy: 0.6351\n","Epoch 95/100\n","500/500 [==============================] - 157s 315ms/step - loss: 2.1183 - sparse_categorical_accuracy: 0.6260 - val_loss: 2.6386 - val_sparse_categorical_accuracy: 0.6304\n","Epoch 96/100\n","500/500 [==============================] - 152s 304ms/step - loss: 2.1129 - sparse_categorical_accuracy: 0.6269 - val_loss: 2.6286 - val_sparse_categorical_accuracy: 0.6315\n","Epoch 97/100\n","500/500 [==============================] - 156s 311ms/step - loss: 2.1165 - sparse_categorical_accuracy: 0.6259 - val_loss: 2.6236 - val_sparse_categorical_accuracy: 0.6341\n","Epoch 98/100\n","500/500 [==============================] - 158s 316ms/step - loss: 2.1123 - sparse_categorical_accuracy: 0.6261 - val_loss: 2.6316 - val_sparse_categorical_accuracy: 0.6332\n","Epoch 99/100\n","500/500 [==============================] - 155s 310ms/step - loss: 2.1126 - sparse_categorical_accuracy: 0.6273 - val_loss: 2.6316 - val_sparse_categorical_accuracy: 0.6337\n","Epoch 100/100\n","500/500 [==============================] - 154s 307ms/step - loss: 2.1090 - sparse_categorical_accuracy: 0.6265 - val_loss: 2.6337 - val_sparse_categorical_accuracy: 0.6369\n","Tempo necessario per l'addestramento: 2:28:55.675260\n"]}]},{"cell_type":"markdown","source":["### Test del modello\n","La seguente cella permette di caricare l'ultimo checkpoint dell'addestramento\n","precedentemente salvato."],"metadata":{"id":"ReOkcBp2WHWW"}},{"cell_type":"code","source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"],"metadata":{"id":"5PIf_6-RSBb1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Translate:\n","  def __init__(self, transformer_block, tokenizers):\n","    self.transformer = transformer_block\n","    self.tokenizers = tokenizers\n","\n","  def predict(self, input_text, max_length):\n","    if input_text is None:\n","      input_text = input_data[np.random.choice(len(input_data))]\n","      print(input_text)\n","\n","    # print(input_text)\n","    inputs_encoder = self.tokenizers.en.tokenize(input_text).to_tensor()\n","    inputs_encoder = keras.preprocessing.sequence.pad_sequences(inputs_encoder, maxlen=max_length, padding='post')\n","\n","    # print(inputs_encoder)\n","    \n","    start_end = self.tokenizers.it.tokenize([''])[0]\n","    start = start_end[0][tf.newaxis]\n","    end = (start_end[1][tf.newaxis]).numpy()[0]\n","\n","    output_array = tf.TensorArray(dtype=tf.int64, size=max_length, dynamic_size=True)\n","    output_array = output_array.write(0, start)     \n","\n","    out_words = []\n","\n","    for i in tf.range(max_length):\n","      # decodifica e recupero probabilità di output\n","      output = tf.transpose(output_array.stack())\n","      \n","      transformer_output = transformer([inputs_encoder, output], \n","                                        training=False,\n","                                        debug=False)\n","\n","      predictions = transformer_output[:, -1:, :]\n","\n","      # selezione della parola più probabile\n","      predict = tf.argmax(predictions, -1)\n","      pred_values = (K.argmax(transformer_output, axis=-1)).numpy()\n","    \n","      # inserimento della parola nella sequenza di output\n","      output_array = output_array.write(i+1, [pred_values[0][i]])\n","\n","      if pred_values[0][i] == end:\n","        break\n","\n","    output = tf.transpose(output_array.stack())\n","    text = tokenizers.it.detokenize(output)[0]  \n","\n","    tokens = tokenizers.it.lookup(output)[0]\n","\n","    return text, tokens"],"metadata":{"id":"L2PEoJVb1V8x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_sequences = [test_input_data[41], test_input_data[30], test_input_data[10], \n","                  test_input_data[57], test_input_data[82], test_input_data[15], \n","                  test_input_data[4], test_input_data[42]]\n","\n","translate = Translate(transformer_block=transformer,\n","                      tokenizers=tokenizers)\n","\n","for test_sequence in test_sequences:\n","  text, token = translate.predict(tf.constant([test_sequence]), MAX_SEQ_LENGTH)\n","\n","  print(f'{\"Input:\":15s}: {test_sequence}')\n","  print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')  \n","  print('---------------------------------------------')\n","\n","print(test_target_data[41])\n","print(test_target_data[30])\n","print(test_target_data[10])\n","print(test_target_data[57])\n","print(test_target_data[82])\n","print(test_target_data[15])\n","print(test_target_data[4])\n","print(test_target_data[42])"],"metadata":{"id":"udIjI2jZWR6g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Tensorboard"],"metadata":{"id":"YJf4hjv4PMAJ"}},{"cell_type":"code","source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"],"metadata":{"id":"vcwHe7VJWt-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["log_dir"],"metadata":{"id":"7AB28JmGPQgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir drive/MyDrive/BERT/logs/fit/20221026-134720"],"metadata":{"id":"2ZkkDKVwPT2O"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"collapsed_sections":["YJf4hjv4PMAJ"]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}