{"cells":[{"cell_type":"code","source":["!pip install -q -U 'tensorflow-text==2.8.*'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NE4enZGpvMRX","executionInfo":{"status":"ok","timestamp":1670968394302,"user_tz":-60,"elapsed":71739,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"39c358c7-9a02-445e-fae2-8c0cd033addf"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.9 MB 27.3 MB/s \n","\u001b[K     |████████████████████████████████| 498.0 MB 12 kB/s \n","\u001b[K     |████████████████████████████████| 1.4 MB 65.7 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 72.1 MB/s \n","\u001b[K     |████████████████████████████████| 462 kB 83.5 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["!pip install -q tf-models-official"],"metadata":{"id":"FPtWz_qHuofc","executionInfo":{"status":"ok","timestamp":1670968469865,"user_tz":-60,"elapsed":75570,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5f167dfb-56f5-48bc-d045-4d1f2dbfb47b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 2.4 MB 36.4 MB/s \n","\u001b[K     |████████████████████████████████| 118 kB 69.1 MB/s \n","\u001b[K     |████████████████████████████████| 238 kB 70.6 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 2.3 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 59.2 MB/s \n","\u001b[K     |████████████████████████████████| 2.3 MB 26.1 MB/s \n","\u001b[K     |████████████████████████████████| 38.2 MB 1.9 MB/s \n","\u001b[K     |████████████████████████████████| 5.8 MB 54.7 MB/s \n","\u001b[K     |████████████████████████████████| 588.3 MB 19 kB/s \n","\u001b[K     |████████████████████████████████| 662 kB 71.0 MB/s \n","\u001b[K     |████████████████████████████████| 1.3 MB 58.2 MB/s \n","\u001b[K     |████████████████████████████████| 352 kB 74.0 MB/s \n","\u001b[K     |████████████████████████████████| 439 kB 80.9 MB/s \n","\u001b[K     |████████████████████████████████| 6.0 MB 91.3 MB/s \n","\u001b[K     |████████████████████████████████| 1.7 MB 66.5 MB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"hJy-juNOpUOY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670968491683,"user_tz":-60,"elapsed":21842,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"0b923e6e-f8ba-4b82-c06f-0371f8cb2752"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":true,"id":"UaAiWsEuC_4K","executionInfo":{"status":"ok","timestamp":1670968502860,"user_tz":-60,"elapsed":11184,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"outputs":[],"source":["import os\n","import re\n","import time\n","import unicodedata\n","import datetime\n","import pathlib\n","from pathlib import Path\n","\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras import backend as K\n","from keras import layers\n","\n","import tensorflow_hub as hub\n","import tensorflow_models as tfm\n","\n","import tensorflow_text as text\n","from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"]},{"cell_type":"code","source":["tf.get_logger().setLevel('ERROR')\n","tf.config.run_functions_eagerly(True)"],"metadata":{"id":"uKEqRlKowOQS","executionInfo":{"status":"ok","timestamp":1670968502861,"user_tz":-60,"elapsed":8,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## Parametri BERT"],"metadata":{"id":"BehZY4rETECN"}},{"cell_type":"code","source":["bert_model_name = 'bert_en_uncased_L-12_H-768_A-12'  \n","tfhub_handle_encoder =  'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3'\n","tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'\n","gs_folder_bert = 'gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12'\n","bert_vocab = os.path.join(gs_folder_bert, 'vocab.txt')\n","\n","print('BERT model selected                : ', tfhub_handle_encoder)\n","print('Preprocessing model auto-selected  : ', tfhub_handle_preprocess)\n","print('BERT vocab                         : ', bert_vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fodDcY6sm392","executionInfo":{"status":"ok","timestamp":1670968502863,"user_tz":-60,"elapsed":9,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"4d7204de-8c2b-4a51-ead6-83d1f04dc3ed"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["BERT model selected                :  https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3\n","Preprocessing model auto-selected  :  https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n","BERT vocab                         :  gs://cloud-tpu-checkpoints/bert/v3/uncased_L-12_H-768_A-12/vocab.txt\n"]}]},{"cell_type":"markdown","source":["### Variabili Globali"],"metadata":{"id":"HRe16D-rUBLA"}},{"cell_type":"code","source":["# PARAMETRI GLOBALI\n","root_folder = 'drive/MyDrive/BERT/'\n","\n","# DATI\n","data_folder_name = 'data'\n","train_filename = 'ita.txt'\n","\n","DATA_PATH = os.path.abspath(os.path.join(root_folder, data_folder_name))\n","train_filenamepath = os.path.abspath(os.path.join(DATA_PATH, train_filename))\n","\n","# PATH LOG Tensorboard\n","PATH_LOG = 'logs/fit/transformer_bert'\n","PATH_LOG = os.path.abspath(os.path.join(root_folder, PATH_LOG))\n","log_dir =  os.path.abspath(os.path.join(PATH_LOG, datetime.datetime.now().strftime('%Y%m%d-%H%M%S'))) \n","\n","# PATH WEIGHTS Tensorboard\n","PATH_WEIGHTS = 'weights/transformer_bert'\n","PATH_WEIGHTS = os.path.abspath(os.path.join(root_folder, PATH_WEIGHTS))\n","checkpoint_path = os.path.abspath(os.path.join(PATH_WEIGHTS, 'cp.ckpt'))\n","\n","# VOCABOLARIO\n","vocab_folder = 'vocab'\n","en_vocab_finalname = 'en_vocab_custom.txt'\n","it_vocab_finalname = 'it_vocab_custom.txt'\n","\n","VOCAB_PATH = os.path.abspath(os.path.join(root_folder, vocab_folder))\n","en_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, en_vocab_finalname))\n","it_vocab_filenamepath = os.path.abspath(os.path.join(VOCAB_PATH, it_vocab_finalname))"],"metadata":{"id":"ewLgCIuEpczO","executionInfo":{"status":"ok","timestamp":1670968502864,"user_tz":-60,"elapsed":7,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# parametri per il modello\n","INPUT_COLUMN = 'input'\n","TARGET_COLUMN = 'target'\n","\n","# parametri per il modello\n","NUM_SAMPLES = 300000 \n","TRAIN = 18016\n","VALIDATION = 6016\n","TEST = 100\n","\n","MAX_VOCAB_SIZE = 20000 \n","EMBEDDING_DIM = 64 \n","HIDDEN_DIM = 1024 # numero di celle nei layer ricorrenti nascosti\n","\n","BATCH_SIZE = 32\n","BUFFER_SIZE = 2000\n","MAX_SEQ_LENGTH = 64\n","\n","NUM_LAYERS = 1 # Numero di layer di Encoder e Decoder del Transformer\n","NUM_HEADS = 8 # Numero di meccanismi di multi-head attention\n","FF_DIM = 16 # Numero di celle dei Layer Feed Forward\n","DROPUOT = 0.5\n","\n","# Ottimizzatore Adam\n","LEARNING_RATE_ADAM = 1e-4\n","BETA_1 = 0.66\n","BETA_2 = 0.999\n","EPOCHS_ADAM = 50\n","\n","# IMPOSTO IL DEBUG A TRUE \n","debug = True\n","training = True"],"metadata":{"id":"8CN-4Uzoqbjl","executionInfo":{"status":"ok","timestamp":1670968502864,"user_tz":-60,"elapsed":7,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## DATASET"],"metadata":{"id":"5DPeN9Vanbvv"}},{"cell_type":"markdown","source":["### Caricamento Dati"],"metadata":{"id":"LU7AorKXT8K7"}},{"cell_type":"code","source":["# Caricamento dataset: frasi in inglese, frasi in italiano\n","df = pd.read_csv(\n","    train_filenamepath,\n","    sep=\"\\t\",\n","    header=None,\n","    names=[INPUT_COLUMN, TARGET_COLUMN],\n","    usecols=[0,1],\n","    nrows=NUM_SAMPLES\n",")\n","\n","df = df[-(TRAIN+VALIDATION+TEST):].reset_index(drop=True)\n","\n","# Mischio il dataset in modo che sia più uniforme tra train e test\n","df = df.iloc[np.random.permutation(df.index)].reset_index(drop=True)\n","\n","print(df.iloc[-4:], '\\n')\n","\n","# Preprocessing dei dati di Input\n","input_data = df[INPUT_COLUMN].tolist()\n","\n","# Preprocessing dei dati Target con aggiunta del token di fine frase\n","target_data = df[TARGET_COLUMN].tolist()\n","\n","train_input_data = input_data[:TRAIN]\n","train_target_data = target_data[:TRAIN]\n","\n","validation_input_data = input_data[TRAIN:TRAIN+VALIDATION]\n","validation_target_data = target_data[TRAIN:TRAIN+VALIDATION]\n","\n","test_input_data = input_data[TRAIN+VALIDATION:]\n","test_target_data = target_data[TRAIN+VALIDATION:]\n","\n","print('-----------TRAIN SET--------------')\n","print(train_input_data[-4:])\n","print(train_target_data[-4:])\n","print('-----------VALIDATION SET---------------')\n","print(validation_input_data[-4:])\n","print(validation_target_data[-4:])\n","print('-----------TEST SET---------------')\n","print(test_input_data[-4:])\n","print(test_target_data[-4:])"],"metadata":{"id":"-K_qU8ouq5lO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670968506092,"user_tz":-60,"elapsed":3234,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"41ff0cd9-f07c-4864-d1fe-58348c3af6df"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["                                      input  \\\n","24128    I wish Tom would smile more often.   \n","24129  These insects are different species.   \n","24130    I think we should all go together.   \n","24131   You won't be able to see Tom today.   \n","\n","                                         target  \n","24128  Io vorrei che Tom sorridesse più spesso.  \n","24129    Questi insetti sono di specie diverse.  \n","24130  Penso che dovremmo andare tutti assieme.  \n","24131    Non sarai in grado di vedere Tom oggi.   \n","\n","-----------TRAIN SET--------------\n","['I wrote a lot in my diary yesterday.', 'It was July. The heat was intense.', 'Did you enjoy yourself at the party?', \"We don't need help from anyone else.\"]\n","['Ho scritto molto nel mio diario ieri.', 'Era luglio. Il calore era intenso.', 'Lei si è divertita alla festa?', \"Non abbiamo bisogno dell'aiuto di nessun altro.\"]\n","-----------VALIDATION SET---------------\n","['I want to know what this is called.', 'I want to know where Tom comes from.', 'Tom went out and left the door open.', \"Tom is very spontaneous, isn't he?\"]\n","['Io voglio sapere come si chiama questa.', 'Io voglio sapere da dove arriva Tom.', 'Tom uscì e lasciò la porta aperta.', 'Tom è molto spontaneo, vero?']\n","-----------TEST SET---------------\n","['I wish Tom would smile more often.', 'These insects are different species.', 'I think we should all go together.', \"You won't be able to see Tom today.\"]\n","['Io vorrei che Tom sorridesse più spesso.', 'Questi insetti sono di specie diverse.', 'Penso che dovremmo andare tutti assieme.', 'Non sarai in grado di vedere Tom oggi.']\n"]}]},{"cell_type":"markdown","source":["### Analisi Dati"],"metadata":{"id":"q1u_rcHxUaqV"}},{"cell_type":"code","source":["print(f'Esempi nel Dataset di Train                            : {len(train_input_data)}')\n","print(f'Frase più corta in inglese nel Dataset di Train        : {min(train_input_data, key = len)}')\n","print(f'Frase più corta in italiano nel Dataset di Train       : {min(train_target_data, key = len)}')\n","print(f'Frase più lunga in inglese nel Dataset di Train        : {max(train_input_data, key = len)}')\n","print(f'Frase più lunga in italiano nel Dataset di Train       : {max(train_target_data, key = len)}')\n","print('---------------------------------------------------------------------------------------')\n","print(f'Esempi nel Dataset di Validation                       : {len(validation_input_data)}')\n","print(f'Frase più corta in inglese nel Dataset di Validation   : {min(validation_input_data, key = len)}')\n","print(f'Frase più corta in italiano nel Dataset di Validation  : {min(validation_target_data, key = len)}')\n","print(f'Frase più lunga in inglese nel Dataset di Validation   : {max(validation_input_data, key = len)}')\n","print(f'Frase più lunga in italiano nel Dataset di Validation  : {max(validation_target_data, key = len)}')\n","print('---------------------------------------------------------------------------------------')\n","print(f'Esempi nel Dataset di Test                             : {len(test_input_data)}')\n","print(f'Frase più corta in inglese nel Dataset di Test         : {min(test_input_data, key = len)}')\n","print(f'Frase più corta in italiano nel Dataset di Test        : {min(test_target_data, key = len)}')\n","print(f'Frase più lunga in inglese nel Dataset di Test         : {max(test_input_data, key = len)}')\n","print(f'Frase più lunga in italiano nel Dataset di Test        : {max(test_target_data, key = len)}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6rssHK6CUcRL","executionInfo":{"status":"ok","timestamp":1670968506092,"user_tz":-60,"elapsed":22,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"19b65533-5997-418f-edb7-d9bf49e5fad2"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Esempi nel Dataset di Train                            : 18016\n","Frase più corta in inglese nel Dataset di Train        : Are you in a hurry? \"Not really.\"\n","Frase più corta in italiano nel Dataset di Train       : Ti aspetterò.\n","Frase più lunga in inglese nel Dataset di Train        : Are you brushing your teeth properly?\n","Frase più lunga in italiano nel Dataset di Train       : Per favore, non fare bollire le uova fino a farle diventare così dure.\n","---------------------------------------------------------------------------------------\n","Esempi nel Dataset di Validation                       : 6016\n","Frase più corta in inglese nel Dataset di Validation   : She’s the prettiest in the group.\n","Frase più corta in italiano nel Dataset di Validation  : Vi aspetterò.\n","Frase più lunga in inglese nel Dataset di Validation   : A group of gangsters stole the money.\n","Frase più lunga in italiano nel Dataset di Validation  : È passato un po' di tempo dall'ultima volta che ci siamo incontrati.\n","---------------------------------------------------------------------------------------\n","Esempi nel Dataset di Test                             : 100\n","Frase più corta in inglese nel Dataset di Test         : My students are learning to drive.\n","Frase più corta in italiano nel Dataset di Test        : Lo conobbi in Francia.\n","Frase più lunga in inglese nel Dataset di Test         : Am I pronouncing your name correctly?\n","Frase più lunga in italiano nel Dataset di Test        : Io voglio scambiare due parole con voi nel mio ufficio.\n"]}]},{"cell_type":"markdown","source":["## Tokenizer\n","\n","Carico il modello di tokenizer di BERT e creo un Tokenizer per il set di dati a disposizione"],"metadata":{"id":"njyY9RWlFMWu"}},{"cell_type":"markdown","source":["### Tokenizer Bert"],"metadata":{"id":"0KUcCnjXVjt3"}},{"cell_type":"code","source":["# Tokenizer BERT\n","tokenizer_encoder = hub.KerasLayer(tfhub_handle_preprocess, name='Bert_Preprocessing')"],"metadata":{"id":"-4B-HWWcmsmz","executionInfo":{"status":"ok","timestamp":1670968514278,"user_tz":-60,"elapsed":8203,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Tokenizer Custom"],"metadata":{"id":"mICEGEzJVnvx"}},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices((input_data, target_data))\n","dataset = dataset.shuffle(len(input_data)).batch(BATCH_SIZE, drop_remainder=True)\n","\n","train_en = dataset.map(lambda en, it: en)\n","train_it = dataset.map(lambda en, it: it)\n","\n","bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n","\n","bert_vocab_args = dict(\n","    # The target vocabulary size\n","    vocab_size = MAX_VOCAB_SIZE,\n","    # Reserved tokens that must be included in the vocabulary\n","    reserved_tokens=reserved_tokens,\n","    # Arguments for `text.BertTokenizer`\n","    bert_tokenizer_params=bert_tokenizer_params,\n","    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","    learn_params={},\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"abBEnJJGV0AD","executionInfo":{"status":"ok","timestamp":1670968514679,"user_tz":-60,"elapsed":426,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"11a98f3b-a2a9-431a-c2fd-c49b4effe807"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["def write_vocab_file(filepath, vocab):\n","  with open(filepath, 'w') as f:\n","    for token in vocab:\n","      print(token, file=f)"],"metadata":{"id":"fPqihfIGVz9o","executionInfo":{"status":"ok","timestamp":1670968514681,"user_tz":-60,"elapsed":9,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["exist_vocab = Path(en_vocab_filenamepath)\n","\n","if not exist_vocab.exists():\n","  en_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_en.batch(MAX_VOCAB_SIZE).prefetch(tf.data.AUTOTUNE),\n","      **bert_vocab_args\n","  )\n","\n","  write_vocab_file(en_vocab_filenamepath, en_vocab)"],"metadata":{"id":"dGsP1V4nVz6S","executionInfo":{"status":"ok","timestamp":1670968514682,"user_tz":-60,"elapsed":9,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["exist_vocab = Path(it_vocab_filenamepath)\n","\n","if not exist_vocab.exists():\n","  it_vocab = bert_vocab.bert_vocab_from_dataset(\n","      train_it.batch(MAX_VOCAB_SIZE).prefetch(tf.data.AUTOTUNE),\n","      **bert_vocab_args\n","  )\n","\n","  write_vocab_file(it_vocab_filenamepath, it_vocab)"],"metadata":{"id":"GTMPQWlmVz1W","executionInfo":{"status":"ok","timestamp":1670968514682,"user_tz":-60,"elapsed":8,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["en_tokenizer = text.BertTokenizer(en_vocab_filenamepath, **bert_tokenizer_params)\n","it_tokenizer = text.BertTokenizer(it_vocab_filenamepath, **bert_tokenizer_params)"],"metadata":{"id":"l4rnN0BsVzxq","executionInfo":{"status":"ok","timestamp":1670968515438,"user_tz":-60,"elapsed":764,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["START = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\n","END = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n","\n","def add_start_end(ragged):\n","  count = ragged.bounding_shape()[0]\n","  starts = tf.fill([count,1], START)\n","  ends = tf.fill([count,1], END)\n","  x = tf.concat([starts, ragged, ends], axis=1)\n","  # x = keras.preprocessing.sequence.pad_sequences(x.numpy(), maxlen=MAX_SEQ_LENGTH, padding='post')\n","  return x\n","\n","def cleanup_text(reserved_tokens, token_txt):\n","  # Drop the reserved tokens, except for \"[UNK]\".\n","  bad_tokens = [re.escape(tok) for tok in reserved_tokens if tok != \"[UNK]\"]\n","  bad_token_re = \"|\".join(bad_tokens)\n","\n","  bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n","  result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n","\n","  # Join them into strings.\n","  result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n","\n","  return result"],"metadata":{"id":"BeaD2-uLWT50","executionInfo":{"status":"ok","timestamp":1670968515439,"user_tz":-60,"elapsed":6,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["#### Classe Tokenizer Custom"],"metadata":{"id":"jgPVS9ZEWWVI"}},{"cell_type":"code","source":["class CustomTokenizer(tf.Module):\n","  def __init__(self, reserved_tokens, vocab_path):\n","    self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n","    self._reserved_tokens = reserved_tokens\n","    self._vocab_path = tf.saved_model.Asset(vocab_path)\n","\n","    vocab = pathlib.Path(vocab_path).read_text().splitlines()\n","    self.vocab = tf.Variable(vocab)\n","\n","    ## Create the signatures for export:   \n","\n","    # Include a tokenize signature for a batch of strings. \n","    self.tokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None], dtype=tf.string))\n","    \n","    # Include `detokenize` and `lookup` signatures for:\n","    #   * `Tensors` with shapes [tokens] and [batch, tokens]\n","    #   * `RaggedTensors` with shape [batch, tokens]\n","    self.detokenize.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.detokenize.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    self.lookup.get_concrete_function(\n","        tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n","    self.lookup.get_concrete_function(\n","          tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n","\n","    # These `get_*` methods take no arguments\n","    self.get_vocab_size.get_concrete_function()\n","    self.get_vocab_path.get_concrete_function()\n","    self.get_reserved_tokens.get_concrete_function()\n","    \n","  @tf.function\n","  def tokenize(self, strings):\n","    enc = self.tokenizer.tokenize(strings)\n","    # Merge the `word` and `word-piece` axes.\n","    enc = enc.merge_dims(-2,-1)\n","    enc = add_start_end(enc)\n","    return enc\n","\n","  @tf.function\n","  def detokenize(self, tokenized):\n","    words = self.tokenizer.detokenize(tokenized)\n","    return cleanup_text(self._reserved_tokens, words)\n","\n","  @tf.function\n","  def lookup(self, token_ids):\n","    return tf.gather(self.vocab, token_ids)\n","\n","  @tf.function\n","  def get_vocab_size(self):\n","    return tf.shape(self.vocab)[0]\n","\n","  @tf.function\n","  def get_vocab_path(self):\n","    return self._vocab_path\n","\n","  @tf.function\n","  def get_reserved_tokens(self):\n","    return tf.constant(self._reserved_tokens)"],"metadata":{"id":"iaAW-xm5WT1_","executionInfo":{"status":"ok","timestamp":1670968515440,"user_tz":-60,"elapsed":5,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["tokenizers = tf.Module()\n","tokenizers.en = CustomTokenizer(reserved_tokens, en_vocab_filenamepath)\n","tokenizers.it = CustomTokenizer(reserved_tokens, it_vocab_filenamepath)"],"metadata":{"id":"svlLobM4WTzC","executionInfo":{"status":"ok","timestamp":1670968520710,"user_tz":-60,"elapsed":5275,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["### Analisi Dati Tokenizzati"],"metadata":{"id":"pKZxiQ5_Whmw"}},{"cell_type":"code","source":["print(f'Vocabolario Italiano : {tokenizers.it.get_vocab_size()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jrg6TwQzW5LN","executionInfo":{"status":"ok","timestamp":1670968520711,"user_tz":-60,"elapsed":31,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"c12d2392-df91-4da5-cb9a-0ff8b0831d65"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabolario Italiano : 2179\n"]}]},{"cell_type":"code","source":["print(input_data[-2:])\n","print(tokenizer_encoder(input_data[-2:])['input_word_ids'][:, :16])\n","print('------------------------------------------------------------------')\n","print(target_data[-2:])\n","print(tokenizers.it.tokenize(target_data[-2:]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.it.tokenize(target_data[-2:]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post')[:, :16])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rKEFeDdGFIGS","executionInfo":{"status":"ok","timestamp":1670968521105,"user_tz":-60,"elapsed":419,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"9dd572c0-f0e2-4952-b99a-3d90af89a46a"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["['I think we should all go together.', \"You won't be able to see Tom today.\"]\n","tf.Tensor(\n","[[ 101 1045 2228 2057 2323 2035 2175 2362 1012  102    0    0    0    0\n","     0    0]\n"," [ 101 2017 2180 1005 1056 2022 2583 2000 2156 3419 2651 1012  102    0\n","     0    0]], shape=(2, 16), dtype=int32)\n","------------------------------------------------------------------\n","['Penso che dovremmo andare tutti assieme.', 'Non sarai in grado di vedere Tom oggi.']\n","<tf.RaggedTensor [[2, 98, 55, 433, 108, 148, 430, 11, 3],\n"," [2, 53, 883, 62, 271, 54, 350, 52, 136, 11, 3]]>\n","[[  2  98  55 433 108 148 430  11   3   0   0   0   0   0   0   0]\n"," [  2  53 883  62 271  54 350  52 136  11   3   0   0   0   0   0]]\n"]}]},{"cell_type":"code","source":["print([min(train_input_data, key = len)])\n","print(tokenizer_encoder([min(train_input_data, key = len)])['input_word_ids'][:, :16])\n","print('------------------------------------------------------------------')\n","print([min(train_target_data, key = len)])\n","print(tokenizers.en.tokenize([min(train_target_data, key = len)]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize([min(train_target_data, key = len)]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post')[:, :16])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O85BUq2INK5j","executionInfo":{"status":"ok","timestamp":1670968521383,"user_tz":-60,"elapsed":286,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"44ece729-04f9-477f-9be8-76b4cc91e495"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["['Are you in a hurry? \"Not really.\"']\n","tf.Tensor(\n","[[ 101 2024 2017 1999 1037 9241 1029 1000 2025 2428 1012 1000  102    0\n","     0    0]], shape=(1, 16), dtype=int32)\n","------------------------------------------------------------------\n","['Ti aspetterò.']\n","<tf.RaggedTensor [[2, 43, 537, 114, 748, 614, 581, 381, 11, 3]]>\n","[[  2  43 537 114 748 614 581 381  11   3   0   0   0   0   0   0]]\n"]}]},{"cell_type":"code","source":["print([min(train_input_data, key = len)])\n","print(tokenizer_encoder([max(train_input_data, key = len)])['input_word_ids'][:, :16])\n","print('------------------------------------------------------------------')\n","print([max(train_target_data, key = len)])\n","print(tokenizers.en.tokenize([max(train_target_data, key = len)]))\n","print(keras.preprocessing.sequence.pad_sequences(tokenizers.en.tokenize([max(train_target_data, key = len)]).numpy(), maxlen=MAX_SEQ_LENGTH, padding='post')[:, :40])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JzJl4YGBPR3Z","executionInfo":{"status":"ok","timestamp":1670968521675,"user_tz":-60,"elapsed":299,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"1b80c2f1-1f32-4fa9-b7f6-ae956e1b1bef"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["['Are you in a hurry? \"Not really.\"']\n","tf.Tensor(\n","[[  101  2024  2017 12766  2115  4091  7919  1029   102     0     0     0\n","      0     0     0     0]], shape=(1, 16), dtype=int32)\n","------------------------------------------------------------------\n","['Per favore, non fare bollire le uova fino a farle diventare così dure.']\n","<tf.RaggedTensor [[2, 39, 166, 870, 173, 9, 150, 353, 628, 173, 25, 1025, 812, 1023, 35,\n","  173, 44, 381, 1726, 294, 29, 624, 381, 24, 628, 370, 27, 475, 484, 525,\n","  26, 1509, 537, 27, 571, 11, 3]]>\n","[[   2   39  166  870  173    9  150  353  628  173   25 1025  812 1023\n","    35  173   44  381 1726  294   29  624  381   24  628  370   27  475\n","   484  525   26 1509  537   27  571   11    3    0    0    0]]\n"]}]},{"cell_type":"markdown","source":["## Creazione dataset\n","Utilizzo della libreria tf.data per la gestione del dataset da utilizzare.\n","Verranno creati batch di esempi che verranno utilizzati durante l'addestramento."],"metadata":{"id":"5QIDajkEsVU1"}},{"cell_type":"code","source":["def prepare_batch(en, it):\n","  zero = tf.zeros([BATCH_SIZE, MAX_SEQ_LENGTH], tf.int64)\n","\n","  # Tokenizzo l'input per l'Encoder\n","  encoder = tokenizer_encoder(en)          \n","\n","  # Tokenizzo l'input per il Decder e creo la variabile Target\n","  it = tokenizers.it.tokenize(it)\n","  decoder = it[:, :-1].to_tensor()  # Drop the [END] tokens\n","  target = it[:, 1:].to_tensor()   # Drop the [START] tokens\n","  \n","  decoder = tf.concat([decoder, zero], 1)\n","  decoder = decoder[:, :(MAX_SEQ_LENGTH)]\n","\n","  target = tf.concat([target, zero], 1)\n","  target = target[:, :(MAX_SEQ_LENGTH)]\n","\n","  return (encoder, decoder), target"],"metadata":{"id":"ccH3jHoABPzV","executionInfo":{"status":"ok","timestamp":1670968521676,"user_tz":-60,"elapsed":6,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def make_batches(ds):\n","  return (\n","      ds\n","      .shuffle(BUFFER_SIZE)\n","      .batch(BATCH_SIZE)\n","      .map(prepare_batch, tf.data.AUTOTUNE)\n","      .prefetch(buffer_size=tf.data.AUTOTUNE))"],"metadata":{"id":"l_dswlCiBTdR","executionInfo":{"status":"ok","timestamp":1670968521677,"user_tz":-60,"elapsed":7,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# Definizione del dataset\n","# [from_tensor_slices] permette di recuperare batch\n","# di esempi dai dataset di riferimento\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_input_data, train_target_data))\n","validation_dataset = tf.data.Dataset.from_tensor_slices((validation_input_data, validation_target_data))\n","\n","# impostazione del recupero di esempi presi in maniera\n","# casuale in gruppi di [BATCH_SIZE] tra quelli disponibili\n","train_dataset = make_batches(train_dataset)\n","validation_dataset = make_batches(validation_dataset)"],"metadata":{"id":"tktJ5YuIsYe3","executionInfo":{"status":"ok","timestamp":1670968523076,"user_tz":-60,"elapsed":1405,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# Recupero un batch di esempi per la verifica delle classi custom che andrò a creare\n","for (enc_input, dec_input), target in train_dataset.take(1):\n","  print('----------------------- ENCODER  -------------------------------')\n","  print(f'Shape                    : {enc_input[\"input_word_ids\"].shape}')\n","  print(f'Word Ids                 : {enc_input[\"input_word_ids\"][0, :MAX_SEQ_LENGTH]}')\n","  print(f'Input Mask               : {enc_input[\"input_mask\"][0, :MAX_SEQ_LENGTH]}')\n","  print(f'Type Ids                 : {enc_input[\"input_type_ids\"][0, :MAX_SEQ_LENGTH]}')  \n","  print('--------------------- DECODER ----------------------------------')\n","  print(f'Shape it input           : {dec_input.shape}')\n","  print(f'Example it input         : {dec_input[0]}')  \n","  print('--------------------- TARGET -----------------------------------')\n","  print(f'Shape it input           : {target.shape}')\n","  print(f'Example it target        : {target[0]}')  "],"metadata":{"id":"VH_aKPlV_AWA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670968523429,"user_tz":-60,"elapsed":358,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"b0b2508d-afec-433e-d657-9bf74f28401a"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------- ENCODER  -------------------------------\n","Shape                    : (32, 128)\n","Word Ids                 : [ 101 2017 2323 2031 3970 2010 6040 1012  102    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0]\n","Input Mask               : [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","Type Ids                 : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","--------------------- DECODER ----------------------------------\n","Shape it input           : (32, 64)\n","Example it input         : [   2   99 1846  234   24 2167  771  304  296   56   86  569   11    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0]\n","--------------------- TARGET -----------------------------------\n","Shape it input           : (32, 64)\n","Example it target        : [  99 1846  234   24 2167  771  304  296   56   86  569   11    3    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0]\n"]}]},{"cell_type":"markdown","source":["## Encoder BERT\n","\n","Predispondo la classe necessaria per la costruzione di BERT\n"],"metadata":{"id":"8dtVuZGJpvXl"}},{"cell_type":"code","source":["class EncoderBert(layers.Layer):\n","  def __init__(self, bert_encoder, embedding_dim, max_len):\n","    super(EncoderBert, self).__init__()\n","    self.encoder = hub.KerasLayer(bert_encoder, name='BERT_encoder')\n","    self.conv_1 = tf.keras.layers.Conv1D(embedding_dim * 4, 1, activation='relu') \n","    self.conv_2 = tf.keras.layers.Conv1D(embedding_dim, 1, activation='relu') \n","    self.lambda_layer = tf.keras.layers.Lambda(lambda x: x[:,:max_len])\n","    self.max_len = max_len\n","\n","  def call(self, x, debug=False):\n","    # x = self.preprocess(input_sequnces)\n","\n","    if debug:\n","      print(f'****************** DEBUG ENCODER BERT ******************')\n","      print(f\"First example\")\n","      print(f'Keys                         : {list(x.keys())}')\n","      print(f'Shape                        : {x[\"input_word_ids\"].shape}')\n","      print(f'Word Ids                     : {x[\"input_word_ids\"][0, :self.max_len]}')\n","      print(f'Input Mask                   : {x[\"input_mask\"][0, :self.max_len]}')\n","      print(f'Type Ids                     : {x[\"input_type_ids\"][0, :self.max_len]}')\n","      \n","    # x = self.encoder(x)['sequence_output'] \n","    # encoder_outputs stato intermedio di BERT prima che esegua la traduzione recuperare la metà della lunghezza\n","    x = self.encoder(x)['encoder_outputs'] \n","    x = x[int(len(x) / 2) - 1]\n","\n","    if debug:\n","      print()\n","      print(f'Encoder Outputs BERT Shape   : {x.shape}')\n","      print(f'Encoder Outputs BERT Values  : {x[0, :1, :self.max_len]}')\n","\n","    x = self.conv_1(x)\n","    if debug:\n","      print()\n","      print(f'Sequence Conv1 Shape         : {x.shape}')\n","\n","    x = self.conv_2(x)\n","    if debug:\n","      print(f'Sequence Conv2 Shape         : {x.shape}')\n","\n","    x = self.lambda_layer(x)\n","    if debug:\n","      print(f'Sequence Lambda Layer        : {x.shape}')\n","      print()\n","      print(f'Sequence Outputs Values      : {x[0, 0, :self.max_len]}')      \n","      print('*********************************************************') \n","\n","    return x"],"metadata":{"id":"m7v9Y-Lep4CD","executionInfo":{"status":"ok","timestamp":1670968523736,"user_tz":-60,"elapsed":309,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["encoder_bert = EncoderBert(tfhub_handle_encoder, \n","                           EMBEDDING_DIM, \n","                           MAX_SEQ_LENGTH)\n","\n","bert_outputs = encoder_bert(enc_input, debug) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"morr6J7rp-SP","executionInfo":{"status":"ok","timestamp":1670968543003,"user_tz":-60,"elapsed":19272,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"df96600d-046e-4deb-a613-17735393bd07"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["****************** DEBUG ENCODER BERT ******************\n","First example\n","Keys                         : ['input_type_ids', 'input_mask', 'input_word_ids']\n","Shape                        : (32, 128)\n","Word Ids                     : [ 101 2017 2323 2031 3970 2010 6040 1012  102    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0]\n","Input Mask                   : [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","Type Ids                     : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","\n","Encoder Outputs BERT Shape   : (32, 128, 768)\n","Encoder Outputs BERT Values  : [[ 1.8346179e-03 -8.8925809e-01 -6.4584208e-01 -8.0253422e-01\n","   7.6997831e-02  8.3424360e-02 -1.3799310e-01  2.1194197e-02\n","  -1.0264742e-01 -1.0135630e+00 -5.1351207e-01  1.0444273e+00\n","   1.8602656e-01  7.4282944e-02 -4.7794941e-01 -1.4830519e-01\n","  -1.4694713e-01  2.5556862e-01 -4.3863800e-01 -3.0928925e-03\n","  -6.7853582e-01 -1.1789495e-01 -4.7720522e-03  3.9824650e-02\n","   4.4479057e-01 -3.1355999e-02 -5.2738947e-01 -1.9697002e-01\n","  -1.9103611e-01  1.3642883e-01 -5.3236282e-01  3.3120245e-01\n","  -6.0285211e-01 -3.9877465e-01  2.1990746e-01  3.9255941e-01\n","   1.6060317e-01 -1.8654266e-01  2.0663151e-01 -1.5083165e-01\n","   4.5953494e-02  2.6864871e-01  3.0716348e-02 -7.6888613e-02\n","   1.7449136e-01 -1.9393146e-01 -5.7863717e+00  7.3618722e-01\n","   9.2270160e-01 -4.5254663e-01  7.8959095e-01 -1.0556595e+00\n","  -3.7689525e-01  5.7406038e-01  1.0972959e+00 -1.0050968e-01\n","  -3.2842088e-01  6.1498821e-02  3.1950539e-01 -2.2569743e-01\n","   1.1817815e-02  2.6292446e-01 -1.4733136e-02  1.9802120e-01]]\n","\n","Sequence Conv1 Shape         : (32, 128, 256)\n","Sequence Conv2 Shape         : (32, 128, 64)\n","Sequence Lambda Layer        : (32, 64, 64)\n","\n","Sequence Outputs Values      : [0.9574595  0.05105043 1.7571464  1.0746868  0.         0.6318099\n"," 0.44149914 0.3954906  1.8804557  0.26886016 0.0572056  0.\n"," 0.         0.         0.         0.34302646 0.2107454  1.593862\n"," 0.         0.85071236 0.         1.3347751  0.         0.06995505\n"," 0.         0.14152984 0.         0.         1.0547178  0.\n"," 0.         0.51395226 1.4302729  0.         0.         0.369759\n"," 0.24146771 0.42997378 0.         0.         0.         0.36889693\n"," 0.         0.         1.5019133  0.87281036 0.         0.71937656\n"," 0.         0.         0.17928538 0.36888108 0.24327376 1.3383045\n"," 0.         0.15192783 0.         0.23027813 0.         1.0918413\n"," 1.5350559  0.         0.37822908 0.5285893 ]\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["## Decoder\n","\n","Predispondo la classe necessaria per la costruzione di un Layer di Decoder"],"metadata":{"id":"ReEQ5rX7aGtl"}},{"cell_type":"markdown","source":["### TOKEN AND POSITION EMBEDDING\n","\n","Implementazione del blocco Embedding per l'utilizzo di vettori posizionali insieme ai vettori di token di parole tramite estensione della classe Layer di Keras. "],"metadata":{"id":"gAu1IXlRZzlq"}},{"cell_type":"code","source":["class TokenAndPositionEmbedding(layers.Layer):\n","  def __init__(self, maxlen, vocab_size, embed_dim):\n","    super(TokenAndPositionEmbedding, self).__init__()\n","    self.maxlen = maxlen\n","    self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","    self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","  def call(self, x, debug=False):\n","    x = keras.preprocessing.sequence.pad_sequences(x, maxlen=self.maxlen, padding='post')\n","    maxlen = tf.shape(x)[-1]\n","\n","    if debug:\n","      print('********** DEBUG TOKEN AND POSITION EMBEDDING ***********')\n","      print(f'Sequence Max len                          : {maxlen}')\n","      print(f'Sequence Shape                            : {tf.shape(x)}')\n","\n","    positions = tf.range(start=0, limit=maxlen, delta=1)\n","    positions = self.pos_emb(positions)\n","    x = self.token_emb(x)\n","    output = x + positions\n","\n","    if debug:\n","      print(f'Shape TokenAndPositionEmbedding           : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"o9-RSKTqsmUC","executionInfo":{"status":"ok","timestamp":1670968543004,"user_tz":-60,"elapsed":24,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["token_position_it = TokenAndPositionEmbedding(MAX_SEQ_LENGTH, tokenizers.it.get_vocab_size(), EMBEDDING_DIM)\n","\n","inputs_decoder = token_position_it(dec_input, debug)"],"metadata":{"id":"rr_EWQUX8EWP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670968543382,"user_tz":-60,"elapsed":400,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"256113d1-4796-494c-ce09-2ae94cd8eccc"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 64\n","Sequence Shape                            : [32 64]\n","Shape TokenAndPositionEmbedding           : (32, 64, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["### LAYER DECODER\n","\n","Implementazione di un blocco di DecoderTransformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"XdLv-6nidKGK"}},{"cell_type":"markdown","source":["#### DecodeBert\n","\n","Implmentazione di un blocco di  decodifica custom per decodificare l'output dal layer EncoderBert prima di passarlo al Decoder del Transformer tramite estensione della classe Layer di Keras"],"metadata":{"id":"_iq7Y-d4eRd8"}},{"cell_type":"code","source":["class DecodeBert(layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='DecodeBert'):\n","    super(DecodeBert, self).__init__()\n","    self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = keras.Sequential(\n","      [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = layers.LayerNormalization()\n","    self.layernorm2 = layers.LayerNormalization()\n","    self.dropout1 = layers.Dropout(rate)\n","    self.dropout2 = layers.Dropout(rate)\n","    self._name = name\n","\n","  def call(self, bert_outputs, training=False, debug=False):\n","    attn_output = self.att(query=bert_outputs,\n","                           value=bert_outputs, \n","                           key=bert_outputs)\n","    \n","    attn_output = self.dropout1(attn_output)\n","    out1 = self.layernorm1(bert_outputs + attn_output)\n","\n","    ffn_output = self.ffn(out1)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","\n","    output = self.layernorm2(out1 + ffn_output)\n","\n","    if debug:\n","      print('********************* DEBUG DECODE-BERT *********************')\n","      print(f'Shape Input Layer Decode-Bert       : {bert_outputs.shape}')\n","      print(f'Shape Output Layer Decode-Bert      : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"joTBTlWF8ETD","executionInfo":{"status":"ok","timestamp":1670968543383,"user_tz":-60,"elapsed":6,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["encoder = DecodeBert(MAX_SEQ_LENGTH, \n","                  EMBEDDING_DIM, \n","                  NUM_HEADS, \n","                  FF_DIM, \n","                  DROPUOT)\n","\n","outputs_encoder = encoder(bert_outputs=bert_outputs,\n","                          training=training, \n","                          debug=debug)"],"metadata":{"id":"JaIzBxFCfKe9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670968545323,"user_tz":-60,"elapsed":1945,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"f3e92945-12b7-4931-eec8-1551252ef25f"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["********************* DEBUG DECODE-BERT *********************\n","Shape Input Layer Decode-Bert       : (32, 64, 64)\n","Shape Output Layer Decode-Bert      : (32, 64, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["#### Layer Decoder"],"metadata":{"id":"dMTKLwd3dRw5"}},{"cell_type":"code","source":["class Decoder(layers.Layer):\n","  def __init__(self, max_len, embed_dim, num_heads, ff_dim, rate=0.5, name='DEC'):\n","    super(Decoder, self).__init__()\n","    self.decode_bert = DecodeBert(max_len=max_len, embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim, rate=rate)\n","    self.att1 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.att2 = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","    self.ffn = keras.Sequential(\n","      [layers.Dense(ff_dim, activation='relu'), layers.Dense(embed_dim),]\n","    )\n","    self.layernorm1 = layers.LayerNormalization()\n","    self.layernorm2 = layers.LayerNormalization()\n","    self.layernorm3 = layers.LayerNormalization()\n","    self.dropout1 = layers.Dropout(rate)\n","    self.dropout2 = layers.Dropout(rate)\n","    self.dropout3 = layers.Dropout(rate)\n","    self._name = name\n","\n","  def call(self, inputs, bert_outputs, training=False, debug=False):\n","    attn_output1 = self.att1(query=inputs,\n","                             value=inputs, \n","                             key=inputs, \n","                             use_causal_mask=True)\n","    \n","    attn_output1 = self.dropout1(attn_output1)\n","    out1 = self.layernorm1(inputs + attn_output1)\n","\n","    dec_bert = self.decode_bert(bert_outputs=bert_outputs, training=training, debug=debug)\n","\n","    attn_output2 = self.att2(key=dec_bert, \n","                             value=dec_bert, \n","                             query=out1)\n","    \n","    attn_output2 = self.dropout2(attn_output2, training=training)\n","    out2 = self.layernorm2(out1 + attn_output2)\n","\n","    ffn_output = self.ffn(out2)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","\n","    output = self.layernorm3(out2 + ffn_output)\n","\n","    if debug:\n","      print('******************* DEBUG DECODER ***********************')\n","      print(f'Input Shape                       : {inputs.shape}')\n","      print(f'Shape Outputs Decoder             : {output.shape}')\n","      print('*********************************************************')\n","\n","    return output"],"metadata":{"id":"SO5rYsFpfFS_","executionInfo":{"status":"ok","timestamp":1670968545324,"user_tz":-60,"elapsed":7,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(MAX_SEQ_LENGTH, \n","                  EMBEDDING_DIM, \n","                  NUM_HEADS, \n","                  FF_DIM, \n","                  DROPUOT)\n","\n","outputs_decoder = decoder(inputs=inputs_decoder, \n","                          bert_outputs=outputs_encoder,  \n","                          training=training,\n","                          debug=debug)"],"metadata":{"id":"yysVdkHH8EPH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670968545325,"user_tz":-60,"elapsed":7,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"edbe53e5-c2e6-43cc-aa12-75e40441e988"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["********************* DEBUG DECODE-BERT *********************\n","Shape Input Layer Decode-Bert       : (32, 64, 64)\n","Shape Output Layer Decode-Bert      : (32, 64, 64)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 64, 64)\n","Shape Outputs Decoder             : (32, 64, 64)\n","*********************************************************\n"]}]},{"cell_type":"markdown","source":["## TRANSFORMER\n","\n","Implementazione del blocco Transformer tramite estensione della classe Layer di Keras."],"metadata":{"id":"ne4zTOG_NKfV"}},{"cell_type":"code","execution_count":36,"metadata":{"pycharm":{"name":"#%%\n"},"id":"lw2xMCAMC_4M","executionInfo":{"status":"ok","timestamp":1670968545326,"user_tz":-60,"elapsed":6,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"outputs":[],"source":["class TransformerBlock(keras.Model):\n","  def __init__(self, \n","               num_layers, \n","               embed_dim, \n","               num_heads, \n","               ff_dim, \n","               max_len,\n","               vocab_size,\n","               tfhub_handle_encoder,\n","               rate=0.5):\n","    \n","    super(TransformerBlock, self).__init__()\n","\n","    self.num_layers = num_layers\n","\n","    self.token_pos_dec = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim)\n","\n","    self.encoder = EncoderBert(tfhub_handle_encoder, embed_dim, max_len)\n","    self.decoder = [Decoder(max_len, embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)]\n","\n","    self.dropout = layers.Dropout(rate)\n","    self.final_layer = tf.keras.layers.Dense(vocab_size)\n","\n","  def call(self, inputs, training=False, debug=False):\n","    inputs_encoder, inputs_decoder  = inputs\n","\n","    encoder_output = self.encoder(inputs_encoder, debug) \n","\n","    inputs_decoder = self.token_pos_dec(inputs_decoder, debug)\n","\n","    if debug:\n","      print(f'---------------- DEBUG TRANSFORMER BLOCK ----------------')\n","      print(f'inputs_encoder       : {inputs_encoder[\"input_word_ids\"].shape}')\n","      print(f'inputs_decoder       : {inputs_decoder.shape}')      \n","\n","    transformer_output = inputs_decoder\n","      \n","    for i in range(self.num_layers):\n","      transformer_output = self.decoder[i](inputs=transformer_output, \n","                                           bert_outputs=encoder_output, \n","                                           training=training,\n","                                           debug=debug)\n","\n","    transformer_output = self.dropout(transformer_output)\n","    logits = self.final_layer(transformer_output)\n","\n","    if debug:\n","      print(f'Output Shape       : {logits.shape}')\n","      print(f'Output Transformer : {logits[0, :1, :12]}')    \n","      print(f'---------------------------------------------------------')\n","\n","    return logits"]},{"cell_type":"code","source":["transformer = TransformerBlock(NUM_LAYERS, \n","                               EMBEDDING_DIM, \n","                               NUM_HEADS, \n","                               FF_DIM,\n","                               MAX_SEQ_LENGTH,\n","                               tokenizers.it.get_vocab_size(),\n","                               tfhub_handle_encoder,\n","                               DROPUOT)\n","\n","transformer_output = transformer((enc_input, dec_input), \n","                                 training=training,\n","                                 debug=debug)"],"metadata":{"id":"pr--G0ZZVAMi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670968559630,"user_tz":-60,"elapsed":14310,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"2c7db56a-6c3a-4904-fac2-7acb025fd193"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["****************** DEBUG ENCODER BERT ******************\n","First example\n","Keys                         : ['input_type_ids', 'input_mask', 'input_word_ids']\n","Shape                        : (32, 128)\n","Word Ids                     : [ 101 2017 2323 2031 3970 2010 6040 1012  102    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0]\n","Input Mask                   : [1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","Type Ids                     : [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n","\n","Encoder Outputs BERT Shape   : (32, 128, 768)\n","Encoder Outputs BERT Values  : [[ 1.8346179e-03 -8.8925809e-01 -6.4584208e-01 -8.0253422e-01\n","   7.6997831e-02  8.3424360e-02 -1.3799310e-01  2.1194197e-02\n","  -1.0264742e-01 -1.0135630e+00 -5.1351207e-01  1.0444273e+00\n","   1.8602656e-01  7.4282944e-02 -4.7794941e-01 -1.4830519e-01\n","  -1.4694713e-01  2.5556862e-01 -4.3863800e-01 -3.0928925e-03\n","  -6.7853582e-01 -1.1789495e-01 -4.7720522e-03  3.9824650e-02\n","   4.4479057e-01 -3.1355999e-02 -5.2738947e-01 -1.9697002e-01\n","  -1.9103611e-01  1.3642883e-01 -5.3236282e-01  3.3120245e-01\n","  -6.0285211e-01 -3.9877465e-01  2.1990746e-01  3.9255941e-01\n","   1.6060317e-01 -1.8654266e-01  2.0663151e-01 -1.5083165e-01\n","   4.5953494e-02  2.6864871e-01  3.0716348e-02 -7.6888613e-02\n","   1.7449136e-01 -1.9393146e-01 -5.7863717e+00  7.3618722e-01\n","   9.2270160e-01 -4.5254663e-01  7.8959095e-01 -1.0556595e+00\n","  -3.7689525e-01  5.7406038e-01  1.0972959e+00 -1.0050968e-01\n","  -3.2842088e-01  6.1498821e-02  3.1950539e-01 -2.2569743e-01\n","   1.1817815e-02  2.6292446e-01 -1.4733136e-02  1.9802120e-01]]\n","\n","Sequence Conv1 Shape         : (32, 128, 256)\n","Sequence Conv2 Shape         : (32, 128, 64)\n","Sequence Lambda Layer        : (32, 64, 64)\n","\n","Sequence Outputs Values      : [0.         0.         0.         0.         0.5662148  0.\n"," 0.08497036 0.28320053 0.         0.         0.         0.\n"," 0.03501832 0.         0.         0.         0.         1.0134387\n"," 0.         0.         0.4990031  0.5880209  0.         0.57682616\n"," 1.4206696  0.         0.72841114 0.         0.         0.2605423\n"," 0.         1.0951375  0.         0.49421185 0.         0.\n"," 0.32112727 0.38339907 0.         0.         0.         0.\n"," 0.         0.         0.         0.         0.         0.28559777\n"," 0.         0.8075973  0.         1.462271   0.         0.\n"," 0.         0.         0.02741159 0.         0.18958804 0.\n"," 0.         0.2781565  0.513266   0.        ]\n","*********************************************************\n","********** DEBUG TOKEN AND POSITION EMBEDDING ***********\n","Sequence Max len                          : 64\n","Sequence Shape                            : [32 64]\n","Shape TokenAndPositionEmbedding           : (32, 64, 64)\n","*********************************************************\n","---------------- DEBUG TRANSFORMER BLOCK ----------------\n","inputs_encoder       : (32, 128)\n","inputs_decoder       : (32, 64, 64)\n","********************* DEBUG DECODE-BERT *********************\n","Shape Input Layer Decode-Bert       : (32, 64, 64)\n","Shape Output Layer Decode-Bert      : (32, 64, 64)\n","*********************************************************\n","******************* DEBUG DECODER ***********************\n","Input Shape                       : (32, 64, 64)\n","Shape Outputs Decoder             : (32, 64, 64)\n","*********************************************************\n","Output Shape       : (32, 64, 2179)\n","Output Transformer : [[ 0.40120378  0.09499563 -0.14909141 -0.5371725  -0.32882264 -0.45211855\n","  -0.83796597  0.17230652 -0.38706917  0.35631555  0.50101334  0.2501232 ]]\n","---------------------------------------------------------\n"]}]},{"cell_type":"code","source":["transformer.summary()"],"metadata":{"id":"_kwqvJSu8liP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670968559631,"user_tz":-60,"elapsed":12,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"d474029a-0254-433c-de27-5ad1effcaaa2"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer_block\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," token_and_position_embeddin  multiple                 143552    \n"," g_1 (TokenAndPositionEmbedd                                     \n"," ing)                                                            \n","                                                                 \n"," encoder_bert_1 (EncoderBert  multiple                 109695553 \n"," )                                                               \n","                                                                 \n"," DEC (Decoder)               multiple                  402912    \n","                                                                 \n"," dropout_16 (Dropout)        multiple                  0         \n","                                                                 \n"," dense_10 (Dense)            multiple                  141635    \n","                                                                 \n","=================================================================\n","Total params: 110,383,652\n","Trainable params: 901,411\n","Non-trainable params: 109,482,241\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["### Addestramento"],"metadata":{"id":"IFmcHTSDTvYk"}},{"cell_type":"code","source":["transformer.compile(\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_ADAM, \n","                                       beta_1=BETA_1, \n","                                       beta_2=BETA_2),\n","    metrics=[keras.metrics.SparseCategoricalAccuracy()])"],"metadata":{"id":"bOyqCyjIr-L2","executionInfo":{"status":"ok","timestamp":1670956977631,"user_tz":-60,"elapsed":11,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["# Create a callback that saves the model's weights\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 save_best_only=True)\n","\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"],"metadata":{"id":"3hurmpSjJ_dT","executionInfo":{"status":"ok","timestamp":1670956977631,"user_tz":-60,"elapsed":10,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","history = transformer.fit(train_dataset,\n","                          epochs=20,\n","                          shuffle=True,\n","                          validation_data=validation_dataset,\n","                          callbacks=[tensorboard_callback, \n","                                     cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"id":"etOGtBcer9yi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2995b489-3235-40b8-d0a8-1beed56a26d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","563/563 [==============================] - 671s 1s/step - loss: 3.6674 - sparse_categorical_accuracy: 0.7959 - val_loss: 1.2734 - val_sparse_categorical_accuracy: 0.8354\n","Epoch 2/20\n","563/563 [==============================] - 659s 1s/step - loss: 1.1192 - sparse_categorical_accuracy: 0.8542 - val_loss: 0.9151 - val_sparse_categorical_accuracy: 0.8674\n","Epoch 3/20\n","563/563 [==============================] - 658s 1s/step - loss: 0.9175 - sparse_categorical_accuracy: 0.8683 - val_loss: 0.8323 - val_sparse_categorical_accuracy: 0.8737\n","Epoch 4/20\n","563/563 [==============================] - 643s 1s/step - loss: 0.8438 - sparse_categorical_accuracy: 0.8736 - val_loss: 0.7809 - val_sparse_categorical_accuracy: 0.8777\n","Epoch 5/20\n","563/563 [==============================] - 642s 1s/step - loss: 0.7914 - sparse_categorical_accuracy: 0.8789 - val_loss: 0.7340 - val_sparse_categorical_accuracy: 0.8837\n","Epoch 6/20\n","563/563 [==============================] - 654s 1s/step - loss: 0.7496 - sparse_categorical_accuracy: 0.8832 - val_loss: 0.6974 - val_sparse_categorical_accuracy: 0.8873\n","Epoch 7/20\n","563/563 [==============================] - 652s 1s/step - loss: 0.7165 - sparse_categorical_accuracy: 0.8858 - val_loss: 0.6685 - val_sparse_categorical_accuracy: 0.8895\n","Epoch 8/20\n","563/563 [==============================] - 653s 1s/step - loss: 0.6900 - sparse_categorical_accuracy: 0.8876 - val_loss: 0.6445 - val_sparse_categorical_accuracy: 0.8917\n","Epoch 9/20\n","563/563 [==============================] - 641s 1s/step - loss: 0.6666 - sparse_categorical_accuracy: 0.8895 - val_loss: 0.6238 - val_sparse_categorical_accuracy: 0.8935\n","Epoch 10/20\n","563/563 [==============================] - 652s 1s/step - loss: 0.6470 - sparse_categorical_accuracy: 0.8912 - val_loss: 0.6050 - val_sparse_categorical_accuracy: 0.8954\n","Epoch 11/20\n","563/563 [==============================] - 652s 1s/step - loss: 0.6289 - sparse_categorical_accuracy: 0.8928 - val_loss: 0.5864 - val_sparse_categorical_accuracy: 0.8969\n","Epoch 12/20\n","563/563 [==============================] - 655s 1s/step - loss: 0.6129 - sparse_categorical_accuracy: 0.8941 - val_loss: 0.5709 - val_sparse_categorical_accuracy: 0.8982\n","Epoch 13/20\n","563/563 [==============================] - 655s 1s/step - loss: 0.5979 - sparse_categorical_accuracy: 0.8951 - val_loss: 0.5561 - val_sparse_categorical_accuracy: 0.8998\n","Epoch 14/20\n","563/563 [==============================] - 654s 1s/step - loss: 0.5844 - sparse_categorical_accuracy: 0.8964 - val_loss: 0.5423 - val_sparse_categorical_accuracy: 0.9011\n","Epoch 15/20\n","563/563 [==============================] - 654s 1s/step - loss: 0.5716 - sparse_categorical_accuracy: 0.8974 - val_loss: 0.5306 - val_sparse_categorical_accuracy: 0.9026\n","Epoch 16/20\n"," 63/563 [==>...........................] - ETA: 8:27 - loss: 0.5562 - sparse_categorical_accuracy: 0.9002"]}]},{"cell_type":"markdown","source":["### Valutazione dell'addestramento\n","Avendo in output il log ed i risultati dell'addestramento, possiamo visualizzare\n","queste informazioni relativamente alle metriche di interesse."],"metadata":{"id":"L0w4wF79UhAp"}},{"cell_type":"code","source":["# visualizzazione andamento addestramento\n","# su un grafico composto da due sub-plot\n","# uno per il loss, l'altro per l'accuracy\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5))\n","\n","# Errore durante l'addestramento\n","ax1.plot(history.history['loss'], label='Loss')\n","ax1.plot(history.history['val_loss'], label='Validation Loss')\n","ax1.set_title('Training Loss')\n","ax1.legend()\n","\n","# Accuratezza durante l'addestramento\n","ax2.plot(history.history['accuracy'], label='Accuracy')\n","ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","ax2.set_title('Training Accuracy')\n","ax2.legend()\n","\n","plt.show()"],"metadata":{"id":"RpXR2p5VAdoG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Addestramento 2"],"metadata":{"id":"FOXy7yFkxt4q"}},{"cell_type":"code","source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"],"metadata":{"id":"bxKgmkMrxsZr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670956994276,"user_tz":-60,"elapsed":8948,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"6471a7fa-c477-4f88-fed1-bcd0bb475539"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f055008a070>"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["start = datetime.datetime.now()\n","history = transformer.fit(train_dataset,\n","                          initial_epoch=15,\n","                          epochs=30,\n","                          shuffle=True,\n","                          validation_data=validation_dataset,\n","                          callbacks=[tensorboard_callback, \n","                                     cp_callback])\n","\n","end = datetime.datetime.now()\n","print(f'Tempo necessario per l\\'addestramento: {end - start}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KHTgL6iyyBU2","outputId":"421f006f-d189-415a-a561-f96cd1d44f6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 16/30\n","563/563 [==============================] - 622s 1s/step - loss: 0.5769 - sparse_categorical_accuracy: 0.8970 - val_loss: 0.5124 - val_sparse_categorical_accuracy: 0.9042\n","Epoch 17/30\n","563/563 [==============================] - 630s 1s/step - loss: 0.5633 - sparse_categorical_accuracy: 0.8985 - val_loss: 0.5023 - val_sparse_categorical_accuracy: 0.9053\n","Epoch 18/30\n","563/563 [==============================] - 628s 1s/step - loss: 0.5522 - sparse_categorical_accuracy: 0.8997 - val_loss: 0.4915 - val_sparse_categorical_accuracy: 0.9067\n","Epoch 19/30\n","563/563 [==============================] - 609s 1s/step - loss: 0.5408 - sparse_categorical_accuracy: 0.9008 - val_loss: 0.4834 - val_sparse_categorical_accuracy: 0.9077\n","Epoch 20/30\n","563/563 [==============================] - 606s 1s/step - loss: 0.5304 - sparse_categorical_accuracy: 0.9018 - val_loss: 0.4739 - val_sparse_categorical_accuracy: 0.9090\n","Epoch 21/30\n","563/563 [==============================] - 606s 1s/step - loss: 0.5207 - sparse_categorical_accuracy: 0.9030 - val_loss: 0.4658 - val_sparse_categorical_accuracy: 0.9103\n","Epoch 22/30\n","563/563 [==============================] - 605s 1s/step - loss: 0.5119 - sparse_categorical_accuracy: 0.9039 - val_loss: 0.4584 - val_sparse_categorical_accuracy: 0.9110\n","Epoch 23/30\n","563/563 [==============================] - 624s 1s/step - loss: 0.5034 - sparse_categorical_accuracy: 0.9047 - val_loss: 0.4513 - val_sparse_categorical_accuracy: 0.9121\n","Epoch 24/30\n","563/563 [==============================] - 624s 1s/step - loss: 0.4949 - sparse_categorical_accuracy: 0.9059 - val_loss: 0.4447 - val_sparse_categorical_accuracy: 0.9130\n","Epoch 25/30\n","563/563 [==============================] - 625s 1s/step - loss: 0.4867 - sparse_categorical_accuracy: 0.9070 - val_loss: 0.4377 - val_sparse_categorical_accuracy: 0.9146\n","Epoch 26/30\n","563/563 [==============================] - 624s 1s/step - loss: 0.4805 - sparse_categorical_accuracy: 0.9074 - val_loss: 0.4332 - val_sparse_categorical_accuracy: 0.9146\n","Epoch 27/30\n","563/563 [==============================] - 608s 1s/step - loss: 0.4734 - sparse_categorical_accuracy: 0.9084 - val_loss: 0.4266 - val_sparse_categorical_accuracy: 0.9157\n","Epoch 28/30\n"," 34/563 [>.............................] - ETA: 8:24 - loss: 0.4634 - sparse_categorical_accuracy: 0.9096"]}]},{"cell_type":"markdown","source":["### Test del modello\n","La seguente cella permette di caricare l'ultimo checkpoint dell'addestramento\n","precedentemente salvato."],"metadata":{"id":"ReOkcBp2WHWW"}},{"cell_type":"code","source":["# Carico i pesi modello\n","latest = tf.train.latest_checkpoint(PATH_WEIGHTS)\n","transformer.load_weights(latest)"],"metadata":{"id":"5PIf_6-RSBb1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670968577733,"user_tz":-60,"elapsed":7698,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"82d79ff5-7a5d-4efe-b13c-6ff35c806b16"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f9dc5bf7070>"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["class Translate:\n","  def __init__(self, transformer_block, tokenizers, tokenizer_bert):\n","    self.transformer = transformer_block\n","    self.tokenizers = tokenizers\n","    self.tokenizer_bert = tokenizer_bert\n","\n","  def predict(self, input_text, max_length):\n","    if input_text is None:\n","      input_text = input_data[np.random.choice(len(input_data))]\n","      print(input_text)\n","\n","    inputs_bert = self.tokenizer_bert(input_text)\n","\n","    start_end = self.tokenizers.it.tokenize([''])[0]\n","    start = start_end[0][tf.newaxis]\n","    end = (start_end[1][tf.newaxis]).numpy()[0]\n","\n","    output_array = tf.TensorArray(dtype=tf.int64, size=max_length, dynamic_size=True)\n","    output_array = output_array.write(0, start)     \n","\n","    out_words = []\n","\n","    for i in tf.range(max_length):\n","      # decodifica e recupero probabilità di output\n","      output = tf.transpose(output_array.stack())\n","      # print('Output', output)\n","      transformer_output = transformer([inputs_bert, output], \n","                                        training=False,\n","                                        debug=False)\n","\n","      predictions = transformer_output[:, -1:, :]\n","\n","      # selezione della parola più probabile\n","      predict = tf.argmax(predictions, -1)\n","      pred_values = (K.argmax(transformer_output, axis=-1)).numpy()\n","\n","      if pred_values[0][i] == end:\n","        break\n","\n","    output = tf.transpose(output_array.stack())\n","    text = tokenizers.it.detokenize(output)[0]  \n","\n","    tokens = tokenizers.it.lookup(output)[0]\n","\n","    return text, tokens"],"metadata":{"id":"L2PEoJVb1V8x","executionInfo":{"status":"ok","timestamp":1670969265508,"user_tz":-60,"elapsed":274,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["test_sequences = [test_input_data[41], test_input_data[30], test_input_data[10], \n","                  test_input_data[57], test_input_data[82], test_input_data[15], \n","                  test_input_data[4], test_input_data[42]]\n","\n","translate = Translate(transformer_block=transformer,\n","                      tokenizers=tokenizers,\n","                      tokenizer_bert=tokenizer_encoder)\n","\n","for test_sequence in test_sequences:\n","  text, token = translate.predict(tf.constant([test_sequence]), MAX_SEQ_LENGTH)\n","\n","  print(f'{\"Input:\":15s}: {test_sequence}')\n","  print(f'{\"Prediction\":15s}: {text.numpy().decode(\"utf-8\")}')  \n","  # print(f'tokens : {token}')\n","  # print(target_data[41])\n","  # break\n","  print('---------------------------------------------')\n","\n","print(test_target_data[41])\n","print(test_target_data[30])\n","print(test_target_data[10])\n","print(test_target_data[57])\n","print(test_target_data[82])\n","print(test_target_data[15])\n","print(test_target_data[4])\n","print(test_target_data[42])"],"metadata":{"id":"udIjI2jZWR6g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670969356341,"user_tz":-60,"elapsed":31433,"user":{"displayName":"Dan Bad","userId":"09439284819205921448"}},"outputId":"6af34faa-ea6c-4176-e85f-af55501e4d8c"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Input:         : You must be very proud of your son.\n","Prediction     : \n","---------------------------------------------\n","Input:         : I don't think that Tom will change.\n","Prediction     : \n","---------------------------------------------\n","Input:         : I just want to be a good neighbor.\n","Prediction     : \n","---------------------------------------------\n","Input:         : I'm sorry, but I don't understand.\n","Prediction     : \n","---------------------------------------------\n","Input:         : We plan to invite both Tom and Mary.\n","Prediction     : \n","---------------------------------------------\n","Input:         : I didn't know Tom was so confused.\n","Prediction     : \n","---------------------------------------------\n","Input:         : The universe is full of mysteries.\n","Prediction     : \n","---------------------------------------------\n","Input:         : Tom took the job Mary offered him.\n","Prediction     : \n","---------------------------------------------\n","Lei deve essere molto orgogliosa di suo figlio.\n","Non penso che Tom cambierà.\n","Voglio solo essere una brava vicina.\n","Mi dispiace, però non capisco.\n","Noi contiamo di invitare sia Tom che Mary.\n","Io non pensavo che Tom fosse così confuso.\n","L'universo è pieno di misteri.\n","Tom ha accettato il lavoro che gli ha offerto Mary.\n"]}]},{"cell_type":"markdown","source":["### Tensorboard"],"metadata":{"id":"YJf4hjv4PMAJ"}},{"cell_type":"code","source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"],"metadata":{"id":"vcwHe7VJWt-O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["log_dir"],"metadata":{"id":"7AB28JmGPQgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%tensorboard --logdir drive/MyDrive/BERT/logs/fit/20221026-134720"],"metadata":{"id":"2ZkkDKVwPT2O"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[],"collapsed_sections":["YJf4hjv4PMAJ"]},"gpuClass":"standard","accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}