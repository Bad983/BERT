{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SaA0nfIpgjG9","executionInfo":{"status":"ok","timestamp":1662278340362,"user_tz":-120,"elapsed":11274,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"24e5f409-bc0b-4890-fc63-8e0097221470"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n","\u001b[K     |████████████████████████████████| 4.7 MB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 38.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n","\u001b[K     |████████████████████████████████| 120 kB 54.2 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"U6J4rHzmdkRm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **BERT**\n","\n","Il modello BERT è un trasformatore bidirezionale preaddestrato su un ampio set di dati in lingua inglese su testi grezzi, senza cioè che i dati venissero etichettati in alcun modo con un processo automatico per generare input ed etichette da quei testi. Più precisamente, è stato preaddestrato con due obiettivi:\n","\n","*   \"modelazione del linguaggio mascherato\" (**Masked Language Modeling MLM**): prendendo una frase, il modello maschera casualmente il 15% delle parole nell'input, quindi legge l'intera frase mascherata attraverso il modello e deve prevedere le parole mascherate (**Token [MASK]**). Consente al modello di apprendere una rappresentazione bidirezionale della frase.\n","*   \"previsione della frase sucessiva\" (**Next Sentence Prediction (NSP)**): il modello concatena due frasi mascherate come input durante il l'addestramento. A volte corrispondono a frasi che erano una accanto all'altra nel testo originale, a volte no. Il modello deve quindi prevedere se le due frasi si susseguivano o meno.\n","\n","In questo modo, il modello apprende una rappresentazione interna della lingua inglese che può quindi essere utilizzata per estrarre funzionalità utili per le attività a valle\n","\n","Paper: https://arxiv.org/pdf/1810.04805.pdf\n","\n","\n","#### Usi previsti e limitazioni\n","Pensato principalmente per essere perfezionato su una determinata attività che utilizza l'intera frase (potenzialmente anche mascherata) come ad esempio:\n","\n","*   Sequence Classification\n","*   Question Answering\n","\n","Non è ottimizzato per la traduzione e generazione del testo\n","Per l'utilizzo di BERT per la traduzione si rimanda al paper\n","https://aclanthology.org/D19-5611.pdf\n","\n","\n","\n","\n"],"metadata":{"id":"fP_UyPcObDRT"}},{"cell_type":"markdown","source":["E' possibile utilizzare questo modello direttamente tramite il package **transformers** utilizzando la funzione **pipeline**)"],"metadata":{"id":"30nG0Ztk9TQf"}},{"cell_type":"code","source":["from transformers import pipeline"],"metadata":{"id":"1ZgkFQuA-52J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["unmasker = pipeline('fill-mask', model='bert-base-cased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n5Fl0f7b_p5B","executionInfo":{"status":"ok","timestamp":1662286480261,"user_tz":-120,"elapsed":4076,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"79f248d8-0ac6-496e-9d16-2f833b4fbe5b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["unmasker(\"Hello! My [MASK] is Daniele.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4jb9H6j5vPU","executionInfo":{"status":"ok","timestamp":1662286482172,"user_tz":-120,"elapsed":463,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"1a8dd7a5-f686-4925-c1c8-1ef622f7bee2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'score': 0.9982873797416687,\n","  'token': 1271,\n","  'token_str': 'name',\n","  'sequence': 'Hello! My name is Daniele.'},\n"," {'score': 0.0005912640481255949,\n","  'token': 10208,\n","  'token_str': 'Name',\n","  'sequence': 'Hello! My Name is Daniele.'},\n"," {'score': 0.0002137003612006083,\n","  'token': 4134,\n","  'token_str': 'address',\n","  'sequence': 'Hello! My address is Daniele.'},\n"," {'score': 0.00011752943100873381,\n","  'token': 12239,\n","  'token_str': 'surname',\n","  'sequence': 'Hello! My surname is Daniele.'},\n"," {'score': 0.00010872080019908026,\n","  'token': 2666,\n","  'token_str': 'names',\n","  'sequence': 'Hello! My names is Daniele.'}]"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["classification_task = pipeline(\"sentiment-analysis\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_RNfMGX_lHA","executionInfo":{"status":"ok","timestamp":1662286490085,"user_tz":-120,"elapsed":3051,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"e645402d-98f2-4066-e8ce-eef3eedcc18e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n","Using a pipeline without specifying a model name and revision in production is not recommended.\n"]}]},{"cell_type":"code","source":["result = classification_task(\"I love ypu.\")\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Vcu-oXt-4zW","executionInfo":{"status":"ok","timestamp":1662286490985,"user_tz":-120,"elapsed":18,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"3a039b8c-7f0b-4606-887e-ffbb0c6f1f81"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': 'POSITIVE', 'score': 0.9998257756233215}]"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["result = classification_task(\"I hate you\")\n","result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1hIz6k_W_hhH","executionInfo":{"status":"ok","timestamp":1662286492364,"user_tz":-120,"elapsed":676,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"11c7354c-af09-4e60-ad65-90af6e0a61ce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'label': 'NEGATIVE', 'score': 0.9991129040718079}]"]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","source":["## **TOKEN**\n","\n","Per utilizzare un modello BERT pre-addestrato, dobbiamo convertire i dati di input in un formato appropriato in modo che ogni frase possa essere inviata al modello"],"metadata":{"id":"TbGi43EY5qbN"}},{"cell_type":"markdown","source":["### **1. Token [CLS]**\n","\n","Per l'attività di classificazione è necessario inviare un singolo vettore che rappresenti l'intera frase. E' necessario, quindi, aggiungere manualmente un tokek aggiuntivo alla frase di input **[CLS]**\n","\n"],"metadata":{"id":"Qv95Ucsfd69Q"}},{"cell_type":"code","source":["from transformers import BertTokenizer"],"metadata":{"id":"zxKl3VgAjhYs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tWRq_HgdaW14","executionInfo":{"status":"ok","timestamp":1662282871461,"user_tz":-120,"elapsed":1132,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"ce4e565c-93af-42c6-e8b8-8737dab83bf9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]', 'Let', \"'\", 's', 'learn', 'deep', 'learning', '!']"]},"metadata":{},"execution_count":39}],"source":["tz = BertTokenizer.from_pretrained(\"bert-base-cased\")\n","\n","token_cls = \"[CLS]\"\n","sent = \"Let's learn deep learning!\"\n","\n","sentence = token_cls + sent\n","tz.tokenize(sentence)"]},{"cell_type":"markdown","source":["Dopo aver suddiviso la frase in **token** la devo convertire in un formato appropriato per il modello **ID**"],"metadata":{"id":"fWoEunY7hS66"}},{"cell_type":"code","source":["tz.convert_tokens_to_ids(tz.tokenize(sentence))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-_KciEhgWsg","executionInfo":{"status":"ok","timestamp":1662282874415,"user_tz":-120,"elapsed":354,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"e1f45678-5764-4e21-aa80-7ed2e40ce8f8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[101, 2421, 112, 188, 3858, 1996, 3776, 106]"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["### **2. Token [SEP]**\n","\n","Per l'attività di \"previsione della frase sucessiva\" si ha bisogno di informare il modello dove finisce la prima frase (**Sentence A**) e dove inizia la seconda frase (**Sentence B**).\n","Viene, quindi, introdotto un nuovo token **[SEP]**. \n","\n","N.B. Nei task di classificazione ogni frase di input conterrà il token **[SEP]** che verrà aggiunto alla fine della frase.\n"],"metadata":{"id":"aOvaw8hCd9eO"}},{"cell_type":"code","source":["token_sep = \"[SEP]\"\n","\n","sentence = token_cls + sent + token_sep\n","tz.tokenize(sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cDw-0BxsiNQE","executionInfo":{"status":"ok","timestamp":1662282877713,"user_tz":-120,"elapsed":237,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"103d5e0d-7b38-47ea-c006-935100c4fed9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]', 'Let', \"'\", 's', 'learn', 'deep', 'learning', '!', '[SEP]']"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["ids = tz.convert_tokens_to_ids(tz.tokenize(sentence))\n","ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdtSDnFtl5if","executionInfo":{"status":"ok","timestamp":1662282879122,"user_tz":-120,"elapsed":339,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"bb0b2387-fca1-4e68-c87c-7ad784c30a37"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[101, 2421, 112, 188, 3858, 1996, 3776, 106, 102]"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["### **3. Token [PAD]**\n","\n","Il modello BERT riceve come input una lunghezza fissa della frase. La lunghezza massima di una frase dipende dai dati su cui stiamo lavorando. Per le frasi che sono più brevi di questa lunghezza massima, dovremo aggiungere padding (token vuoti) alle frasi per comporre la lunghezza. Viene, quindi, aggiunto il token **[PAD]** (in coda alla frase) in modo da avere le frasi tutte della stessa lunghezza. Nel Modello originario è stato impostato a 512, per il nostro esempio imposteremo la lunghezza a 64"],"metadata":{"id":"VHaThE6FmcV6"}},{"cell_type":"code","source":["max_length = 10\n","\n","for token_pad in range(len(tz.tokenize(sentence)), max_length):\n","  sentence = sentence + \"[PAD]\"\n","\n","sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"yXpOa2kQq-G2","executionInfo":{"status":"ok","timestamp":1662282890516,"user_tz":-120,"elapsed":334,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"f90612e0-18f9-4da8-b5ae-30598d70e946"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"[CLS]Let's learn deep learning![SEP][PAD]\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["ids = tz.convert_tokens_to_ids(tz.tokenize(sentence))\n","ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4FJ_e-HPs4Fg","executionInfo":{"status":"ok","timestamp":1662282895012,"user_tz":-120,"elapsed":369,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"4eaf8fe6-531b-4080-b322-a6e2a5b134b6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[101, 2421, 112, 188, 3858, 1996, 3776, 106, 102, 0]"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["### **4. Token [UNK]**\n","\n","Quando il modello BERT è stato addestrato, a ogni token è stato assegnato un ID univoco . Quindi, quando vogliamo utilizzare un modello BERT pre-addestrato, dovremo prima convertire ogni token nella frase di input nei suoi ID univoci corrispondenti.\n","\n","C'è un punto importante da notare quando utilizziamo un modello pre-addestrato.Il vocabolario utilizzato per l'addestramento del modello BERT è di 30.000 token. In altre parole, quando applichiamo un modello pre-addestrato ad altri dati, è possibile che alcuni token nei nuovi dati potrebbero non apparire nel vocabolario originario del modello pre-addestrato. Questo è comunemente noto come **out-of-vocabulary (OOV)**.\n","\n","Per i token che non compaiono nel vocabolario originale, il sistema è stato progettato per sostituire le parole mancanti con un token speciale [UNK], che sta per token sconosciuto.\n","\n","Tuttavia, la conversione potrebbe far perdere numerose informazioni d ai dati di inpunt. Quindi, BERT utilizza un algoritmo **WordPiece** che suddivide una parola in più sottoparole, in modo tale che anche le sottoparole comunemente viste possano essere rappresentate dal modello.\n","\n","Ad esempio, la parola **characteristically** non compare nel vocabolario originale. "],"metadata":{"id":"bbmt0zu5UhUS"}},{"cell_type":"code","source":["tz.convert_tokens_to_ids([\"characteristically\"]) # TOKEN [UKN] == 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wtpMBVU4X2L-","executionInfo":{"status":"ok","timestamp":1662292843794,"user_tz":-120,"elapsed":320,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"79785ab4-8ac0-49d6-bc5c-ba52d8439c58"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[100]"]},"metadata":{},"execution_count":61}]},{"cell_type":"markdown","source":["Se però utilizzo la funzione **tokenize** la parola verrà automaticamente suddivisa in 2 sottoparole \n","\n","*   **characteristic**: prefisso\n","*   **##ally**: suffisso, il doppio hash **##** indica che è suffisso di altre parole\n","\n"],"metadata":{"id":"51_Ad5SLYN4D"}},{"cell_type":"code","source":["tz.tokenize(\"characteristically\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JO3vyFplYrZS","executionInfo":{"status":"ok","timestamp":1662293063379,"user_tz":-120,"elapsed":409,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"1ef8b838-4774-4a28-d3da-912d8ddc170f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['characteristic', '##ally']"]},"metadata":{},"execution_count":62}]},{"cell_type":"markdown","source":["Questo aiuta a non perdere delle informazioni da passare al modello"],"metadata":{"id":"CrbJ8HdFY4Oe"}},{"cell_type":"code","source":["sent = \"He remains characteristically confident and optimistic.\"\n","tz.tokenize(sent)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5PcAqCnZBTd","executionInfo":{"status":"ok","timestamp":1662293161656,"user_tz":-120,"elapsed":527,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"9c631568-f3ad-478f-aea8-592e8d6b4f6b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['He',\n"," 'remains',\n"," 'characteristic',\n"," '##ally',\n"," 'confident',\n"," 'and',\n"," 'optimistic',\n"," '.']"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["tz.convert_tokens_to_ids(tz.tokenize(sent))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"065x4x1VZGIX","executionInfo":{"status":"ok","timestamp":1662293164184,"user_tz":-120,"elapsed":318,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"1d163a6e-54e9-4540-8f6c-4467da37bd8e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1124, 2606, 7987, 2716, 9588, 1105, 24876, 119]"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","source":["### Riepilogo\n","\n","Per un'attività di classificazione la frase di input dovrà eseguire i seguenti passaggi prima di essere inserita nel modello BERT:\n","\n","\n","1.   **Tokenization** suddivisione della frase in token\n","2.   **Adding [CLS]** aggiunta del token di inizio frase\n","3.   **Adding [SEP]** aggiunta del token di suddivisione/chiusura della frase \n","4.   **Adding [PAD]** aggiunta del token in modo che le frasi siano della stessa lunghezza\n","5.   **Convert ID** conversione di ogni token negli ID corrispondenti\n","\n","\n","Example:\n","\n","**Original Sentence**\n","\n","Let's learn deep learning!\n","\n","**Tokenized Sentence**\n","\n","['Let', \"'\", 's', 'learn', 'deep', 'learning', '!']\n","\n","**Adding [CLS] and [SEP] Tokens**\n","\n","['[CLS]', 'Let', \"'\", 's', 'learn', 'deep', 'learning', '!', '[SEP]']\n","\n","**Padding**\n","\n","['[CLS]', 'Let', \"'\", 's', 'learn', 'deep', 'learning', '!', '[SEP]', '[PAD]']\n","\n","**Converting to IDs**\n","\n","[101, 2421, 112, 188, 3858, 1996, 3776, 106, 102, 0]\n","\n","\n","\n","\n"],"metadata":{"id":"QLZYKg-JtkyK"}},{"cell_type":"markdown","source":["### Tokenization --> Transformers Package\n","\n","Possiamo utilizzare le funzioni fornite dal package **transformers** per aiutarci ad eseguire tutti questi passaggi tramite la funzione **encode_plus**"],"metadata":{"id":"z_5HgELHwjH6"}},{"cell_type":"code","source":["encoded = tz.encode_plus(\n","    text=sent,  # the sentence to be encoded\n","    add_special_tokens=True,  # Add [CLS] and [SEP]\n","    max_length = 64,  # maximum length of a sentence\n","    padding='max_length',  # Add [PAD]s\n","    return_attention_mask = True,  # Generate the attention mask\n","    return_tensors = 'tf',  # ask the function to return Tensorflow tensors\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ObsOE2sfl827","executionInfo":{"status":"ok","timestamp":1662282917623,"user_tz":-120,"elapsed":330,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"e9397f0e-cc8d-4c99-d275-faa8cb5ecc8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["# Get the input IDs in tensor format\n","input_ids = encoded['input_ids']\n","input_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3jm9IssoTSn","executionInfo":{"status":"ok","timestamp":1662282929295,"user_tz":-120,"elapsed":342,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"1e29149c-7f3f-4704-d88f-dfcbdc92651d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 64), dtype=int32, numpy=\n","array([[ 101, 2421,  112,  188, 3858, 1996, 3776,  106,  102,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","           0,    0,    0,    0,    0,    0,    0,    0,    0]],\n","      dtype=int32)>"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["# Get the attention mask in tensor format\n","attn_mask = encoded['attention_mask']\n","attn_mask"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A24uCqzcqUPg","executionInfo":{"status":"ok","timestamp":1662282933632,"user_tz":-120,"elapsed":290,"user":{"displayName":"Daniele Badiali","userId":"10202881816518872932"}},"outputId":"3079b2a4-550f-4c21-e4c1-321e3dde5c60"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 64), dtype=int32, numpy=\n","array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","      dtype=int32)>"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":[],"metadata":{"id":"Urp0Nmwcqqjq"},"execution_count":null,"outputs":[]}]}